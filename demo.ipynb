{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MicroGrad demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP, AutoEncoder, VariationalAutoEncoder\n",
    "from micrograd.loss_funcs import mean_squared_error, vae_loss\n",
    "from micrograd.optimizer import SGD\n",
    "from micrograd.batch_iterator import BatchIterator\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[Value(random.randint(0, 1)) for _ in range(5)] for _ in range(20)]\n",
    "target = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = AutoEncoder(\n",
    "    in_embeds=25,# Input dimension   # Number of hidden layers\n",
    "    latent_dim=5,\n",
    "    act_func=Value.sigmoid, # Latent space dimension,       # Activation function (None uses default leaky_relu)\n",
    "    tied=True       # Whether to use tied weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, AutoEncoder Loss: 6.256687214258\n",
      "Iteration: 10, AutoEncoder Loss: 3.021441369853\n",
      "Iteration: 20, AutoEncoder Loss: 2.378851304624\n",
      "Iteration: 30, AutoEncoder Loss: 2.031525398569\n",
      "Iteration: 40, AutoEncoder Loss: 1.769468417888\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(auto.parameters(), learning_rate=0.5)\n",
    "\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reconstruction = [auto(i) for i in x]\n",
    "    \n",
    "    loss = sum([mean_squared_error(recon, targ) for recon, targ in zip(reconstruction, target)])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration: {i}, AutoEncoder Loss: {loss.data:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1]\n",
      "\n",
      "[0.97381359 0.85307712 0.9897     0.57469108 0.26877156 0.37199923\n",
      " 0.34930796 0.06376394 0.9956249  0.78792912 0.63948578 0.02492209\n",
      " 0.74980356 0.33467329 0.56673871 0.10448293 0.00606407 0.09418646\n",
      " 0.60444269 0.11290939 0.97749671 0.80515552 0.2014048  0.82550021\n",
      " 0.79164191]\n"
     ]
    }
   ],
   "source": [
    "original_data = np.array([i.data for i in x[0]])\n",
    "\n",
    "reconstructed = np.array([j.data for j in reconstruction[0]])\n",
    "\n",
    "print(original_data)\n",
    "print()\n",
    "print(reconstructed)\n",
    "\n",
    "\n",
    "encoder = auto.encode\n",
    "\n",
    "test = [Value(random.uniform(0, 1)) for _ in range(25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1701073  0.37303355 0.51598203 0.32412059 0.25192021 0.14925865\n",
      " 0.46177295 0.51839559 0.36265632 0.71896608 0.92644477 0.86578428\n",
      " 0.06961999 0.94279002 0.1770544  0.40709859 0.23278027 0.79388905\n",
      " 0.12750324 0.04178973 0.96025833 0.15691848 0.36567517 0.92996087\n",
      " 0.80917674]\n",
      "[0.79694723 0.64358232 0.68676757 0.29544522 0.26878396 0.60277587\n",
      " 0.36787543 0.65969419 0.71589039 0.13510937 0.61907216 0.29359141\n",
      " 0.34221491 0.45083033 0.07235615 0.3020035  0.24966053 0.55860424\n",
      " 0.53821378 0.48280322 0.87954599 0.44818257 0.17946126 0.34670628\n",
      " 0.30976285]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([i.data for i in test]))\n",
    "\n",
    "# print(auto.encode(noise[0]))\n",
    "\n",
    "print(np.array([val.data for val in auto(test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
