{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MicroGrad demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import AutoEncoder, VariationalAutoEncoder\n",
    "from micrograd.loss_funcs import mean_squared_error, vae_loss\n",
    "from micrograd.optimizer import SGD\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "]\n",
    "target = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = AutoEncoder(\n",
    "    in_embeds=5,# input dimension   , # len = number of layers, i = size of layer\n",
    "    hidden_layers=[3],\n",
    "    latent_dim=2, # compressed layer dimensions\n",
    "    act_func=Value.sigmoid, # activation function for final decoder layer\n",
    ")\n",
    "\n",
    "optimizer = optimizer = SGD(auto.parameters(), learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, AutoEncoder Loss: 1.371534565316\n",
      "Iteration: 10, AutoEncoder Loss: 0.828867493523\n",
      "Iteration: 20, AutoEncoder Loss: 0.800121316840\n",
      "Iteration: 30, AutoEncoder Loss: 0.796200928183\n",
      "Iteration: 40, AutoEncoder Loss: 0.793153087901\n",
      "Iteration: 50, AutoEncoder Loss: 0.789298019836\n",
      "Iteration: 60, AutoEncoder Loss: 0.783577204890\n",
      "Iteration: 70, AutoEncoder Loss: 0.774073506406\n",
      "Iteration: 80, AutoEncoder Loss: 0.757584390492\n",
      "Iteration: 90, AutoEncoder Loss: 0.720449213257\n",
      "Iteration: 100, AutoEncoder Loss: 0.650519424161\n",
      "Iteration: 110, AutoEncoder Loss: 0.601960645456\n",
      "Iteration: 120, AutoEncoder Loss: 0.574013927675\n",
      "Iteration: 130, AutoEncoder Loss: 0.556683287104\n",
      "Iteration: 140, AutoEncoder Loss: 0.545111582130\n",
      "Iteration: 150, AutoEncoder Loss: 0.537140917181\n",
      "Iteration: 160, AutoEncoder Loss: 0.530531654328\n",
      "Iteration: 170, AutoEncoder Loss: 0.525172208415\n",
      "Iteration: 180, AutoEncoder Loss: 0.520056642697\n",
      "Iteration: 190, AutoEncoder Loss: 0.518232457409\n",
      "Iteration: 200, AutoEncoder Loss: 0.514859873832\n",
      "Iteration: 210, AutoEncoder Loss: 0.509371774312\n",
      "Iteration: 220, AutoEncoder Loss: 0.502157755093\n",
      "Iteration: 230, AutoEncoder Loss: 0.500940787279\n",
      "Iteration: 240, AutoEncoder Loss: 0.481982982089\n",
      "Iteration: 250, AutoEncoder Loss: 0.471499547646\n",
      "Iteration: 260, AutoEncoder Loss: 0.472600351834\n",
      "Iteration: 270, AutoEncoder Loss: 0.453565798362\n",
      "Iteration: 280, AutoEncoder Loss: 0.440536171797\n",
      "Iteration: 290, AutoEncoder Loss: 0.414488451150\n",
      "Iteration: 300, AutoEncoder Loss: 0.403819535461\n",
      "Iteration: 310, AutoEncoder Loss: 0.399981932162\n",
      "Iteration: 320, AutoEncoder Loss: 0.386682465377\n",
      "Iteration: 330, AutoEncoder Loss: 0.351657711034\n",
      "Iteration: 340, AutoEncoder Loss: 0.356176344601\n",
      "Iteration: 350, AutoEncoder Loss: 0.338073033662\n",
      "Iteration: 360, AutoEncoder Loss: 0.382161777637\n",
      "Iteration: 370, AutoEncoder Loss: 0.309284537811\n",
      "Iteration: 380, AutoEncoder Loss: 0.413371767216\n",
      "Iteration: 390, AutoEncoder Loss: 0.381268308423\n",
      "Iteration: 400, AutoEncoder Loss: 0.349621329320\n",
      "Iteration: 410, AutoEncoder Loss: 0.287151832930\n",
      "Iteration: 420, AutoEncoder Loss: 0.276335243496\n",
      "Iteration: 430, AutoEncoder Loss: 0.386816539873\n",
      "Iteration: 440, AutoEncoder Loss: 0.256108998928\n",
      "Iteration: 450, AutoEncoder Loss: 0.232899152979\n",
      "Iteration: 460, AutoEncoder Loss: 0.236230360409\n",
      "Iteration: 470, AutoEncoder Loss: 0.216750923701\n",
      "Iteration: 480, AutoEncoder Loss: 0.199674002965\n",
      "Iteration: 490, AutoEncoder Loss: 0.192034351732\n",
      "Iteration: 500, AutoEncoder Loss: 0.185962252992\n",
      "Iteration: 510, AutoEncoder Loss: 0.180110886548\n",
      "Iteration: 520, AutoEncoder Loss: 0.174412363783\n",
      "Iteration: 530, AutoEncoder Loss: 0.169423749891\n",
      "Iteration: 540, AutoEncoder Loss: 0.163816233472\n",
      "Iteration: 550, AutoEncoder Loss: 0.157735920260\n",
      "Iteration: 560, AutoEncoder Loss: 0.145809931290\n",
      "Iteration: 570, AutoEncoder Loss: 0.166324855696\n",
      "Iteration: 580, AutoEncoder Loss: 0.366200065988\n",
      "Iteration: 590, AutoEncoder Loss: 0.045178402728\n",
      "Iteration: 600, AutoEncoder Loss: 0.036393255335\n",
      "Iteration: 610, AutoEncoder Loss: 0.030446316324\n",
      "Iteration: 620, AutoEncoder Loss: 0.025848461316\n",
      "Iteration: 630, AutoEncoder Loss: 0.022192510327\n",
      "Iteration: 640, AutoEncoder Loss: 0.019230416975\n",
      "Iteration: 650, AutoEncoder Loss: 0.016818314511\n",
      "Iteration: 660, AutoEncoder Loss: 0.014931080861\n",
      "Iteration: 670, AutoEncoder Loss: 0.013479417662\n",
      "Iteration: 680, AutoEncoder Loss: 0.012279262012\n",
      "Iteration: 690, AutoEncoder Loss: 0.011274494385\n",
      "Iteration: 700, AutoEncoder Loss: 0.010421494300\n",
      "Iteration: 710, AutoEncoder Loss: 0.009690702386\n",
      "Iteration: 720, AutoEncoder Loss: 0.009058425677\n",
      "Iteration: 730, AutoEncoder Loss: 0.008506381900\n",
      "Iteration: 740, AutoEncoder Loss: 0.008024109078\n",
      "Iteration: 750, AutoEncoder Loss: 0.007591343812\n",
      "Iteration: 760, AutoEncoder Loss: 0.007205373543\n",
      "Iteration: 770, AutoEncoder Loss: 0.006860898919\n",
      "Iteration: 780, AutoEncoder Loss: 0.006545953084\n",
      "Iteration: 790, AutoEncoder Loss: 0.006260320319\n",
      "Iteration: 800, AutoEncoder Loss: 0.006000889290\n",
      "Iteration: 810, AutoEncoder Loss: 0.005760891672\n",
      "Iteration: 820, AutoEncoder Loss: 0.005540961020\n",
      "Iteration: 830, AutoEncoder Loss: 0.005336357254\n",
      "Iteration: 840, AutoEncoder Loss: 0.005146892911\n",
      "Iteration: 850, AutoEncoder Loss: 0.004976931507\n",
      "Iteration: 860, AutoEncoder Loss: 0.004809160638\n",
      "Iteration: 870, AutoEncoder Loss: 0.004660551019\n",
      "Iteration: 880, AutoEncoder Loss: 0.004512466264\n",
      "Iteration: 890, AutoEncoder Loss: 0.004377609134\n",
      "Iteration: 900, AutoEncoder Loss: 0.004251090459\n",
      "Iteration: 910, AutoEncoder Loss: 0.004130616083\n",
      "Iteration: 920, AutoEncoder Loss: 0.004016388643\n",
      "Iteration: 930, AutoEncoder Loss: 0.003909504921\n",
      "Iteration: 940, AutoEncoder Loss: 0.003807009741\n",
      "Iteration: 950, AutoEncoder Loss: 0.003709617974\n",
      "Iteration: 960, AutoEncoder Loss: 0.003617960798\n",
      "Iteration: 970, AutoEncoder Loss: 0.003529088362\n",
      "Iteration: 980, AutoEncoder Loss: 0.003445901970\n",
      "Iteration: 990, AutoEncoder Loss: 0.003365341051\n",
      "Iteration: 1000, AutoEncoder Loss: 0.003288957743\n",
      "Iteration: 1010, AutoEncoder Loss: 0.003218046076\n",
      "Iteration: 1020, AutoEncoder Loss: 0.003145629618\n",
      "Iteration: 1030, AutoEncoder Loss: 0.003078897252\n",
      "Iteration: 1040, AutoEncoder Loss: 0.003016028044\n",
      "Iteration: 1050, AutoEncoder Loss: 0.002952539356\n",
      "Iteration: 1060, AutoEncoder Loss: 0.002893529407\n",
      "Iteration: 1070, AutoEncoder Loss: 0.002836233730\n",
      "Iteration: 1080, AutoEncoder Loss: 0.002781406268\n",
      "Iteration: 1090, AutoEncoder Loss: 0.002730412379\n",
      "Iteration: 1100, AutoEncoder Loss: 0.002677445513\n",
      "Iteration: 1110, AutoEncoder Loss: 0.002628729464\n",
      "Iteration: 1120, AutoEncoder Loss: 0.002580930667\n",
      "Iteration: 1130, AutoEncoder Loss: 0.002535596681\n",
      "Iteration: 1140, AutoEncoder Loss: 0.002492742682\n",
      "Iteration: 1150, AutoEncoder Loss: 0.002449098805\n",
      "Iteration: 1160, AutoEncoder Loss: 0.002409051286\n",
      "Iteration: 1170, AutoEncoder Loss: 0.002368114514\n",
      "Iteration: 1180, AutoEncoder Loss: 0.002329893944\n",
      "Iteration: 1190, AutoEncoder Loss: 0.002292347429\n",
      "Iteration: 1200, AutoEncoder Loss: 0.002256434119\n",
      "Iteration: 1210, AutoEncoder Loss: 0.002221092212\n",
      "Iteration: 1220, AutoEncoder Loss: 0.002188383745\n",
      "Iteration: 1230, AutoEncoder Loss: 0.002154161807\n",
      "Iteration: 1240, AutoEncoder Loss: 0.002122272561\n",
      "Iteration: 1250, AutoEncoder Loss: 0.002090983166\n",
      "Iteration: 1260, AutoEncoder Loss: 0.002060684543\n",
      "Iteration: 1270, AutoEncoder Loss: 0.002031407098\n",
      "Iteration: 1280, AutoEncoder Loss: 0.002002702101\n",
      "Iteration: 1290, AutoEncoder Loss: 0.001975089049\n",
      "Iteration: 1300, AutoEncoder Loss: 0.001947902083\n",
      "Iteration: 1310, AutoEncoder Loss: 0.001922372205\n",
      "Iteration: 1320, AutoEncoder Loss: 0.001895977247\n",
      "Iteration: 1330, AutoEncoder Loss: 0.001870888582\n",
      "Iteration: 1340, AutoEncoder Loss: 0.001846701630\n",
      "Iteration: 1350, AutoEncoder Loss: 0.001822802780\n",
      "Iteration: 1360, AutoEncoder Loss: 0.001800534095\n",
      "Iteration: 1370, AutoEncoder Loss: 0.001777137845\n",
      "Iteration: 1380, AutoEncoder Loss: 0.001755095380\n",
      "Iteration: 1390, AutoEncoder Loss: 0.001733691131\n",
      "Iteration: 1400, AutoEncoder Loss: 0.001712615523\n",
      "Iteration: 1410, AutoEncoder Loss: 0.001692803406\n",
      "Iteration: 1420, AutoEncoder Loss: 0.001672166022\n",
      "Iteration: 1430, AutoEncoder Loss: 0.001652531722\n",
      "Iteration: 1440, AutoEncoder Loss: 0.001634106656\n",
      "Iteration: 1450, AutoEncoder Loss: 0.001614788225\n",
      "Iteration: 1460, AutoEncoder Loss: 0.001596556157\n",
      "Iteration: 1470, AutoEncoder Loss: 0.001578703815\n",
      "Iteration: 1480, AutoEncoder Loss: 0.001561163451\n",
      "Iteration: 1490, AutoEncoder Loss: 0.001544540906\n",
      "Iteration: 1500, AutoEncoder Loss: 0.001527365176\n",
      "Iteration: 1510, AutoEncoder Loss: 0.001510911912\n",
      "Iteration: 1520, AutoEncoder Loss: 0.001495379114\n",
      "Iteration: 1530, AutoEncoder Loss: 0.001479210348\n",
      "Iteration: 1540, AutoEncoder Loss: 0.001463768469\n",
      "Iteration: 1550, AutoEncoder Loss: 0.001449146461\n",
      "Iteration: 1560, AutoEncoder Loss: 0.001433960664\n",
      "Iteration: 1570, AutoEncoder Loss: 0.001419428982\n",
      "Iteration: 1580, AutoEncoder Loss: 0.001405338989\n",
      "Iteration: 1590, AutoEncoder Loss: 0.001391352737\n",
      "Iteration: 1600, AutoEncoder Loss: 0.001377654694\n",
      "Iteration: 1610, AutoEncoder Loss: 0.001364829814\n",
      "Iteration: 1620, AutoEncoder Loss: 0.001351164792\n",
      "Iteration: 1630, AutoEncoder Loss: 0.001338225637\n",
      "Iteration: 1640, AutoEncoder Loss: 0.001325661969\n",
      "Iteration: 1650, AutoEncoder Loss: 0.001313187925\n",
      "Iteration: 1660, AutoEncoder Loss: 0.001301283188\n",
      "Iteration: 1670, AutoEncoder Loss: 0.001289086451\n",
      "Iteration: 1680, AutoEncoder Loss: 0.001277289047\n",
      "Iteration: 1690, AutoEncoder Loss: 0.001265721152\n",
      "Iteration: 1700, AutoEncoder Loss: 0.001254460785\n",
      "Iteration: 1710, AutoEncoder Loss: 0.001243255345\n",
      "Iteration: 1720, AutoEncoder Loss: 0.001232270608\n",
      "Iteration: 1730, AutoEncoder Loss: 0.001221706320\n",
      "Iteration: 1740, AutoEncoder Loss: 0.001210961770\n",
      "Iteration: 1750, AutoEncoder Loss: 0.001200652909\n",
      "Iteration: 1760, AutoEncoder Loss: 0.001190366359\n",
      "Iteration: 1770, AutoEncoder Loss: 0.001180267415\n",
      "Iteration: 1780, AutoEncoder Loss: 0.001170360502\n",
      "Iteration: 1790, AutoEncoder Loss: 0.001160912362\n",
      "Iteration: 1800, AutoEncoder Loss: 0.001151088424\n",
      "Iteration: 1810, AutoEncoder Loss: 0.001141642005\n",
      "Iteration: 1820, AutoEncoder Loss: 0.001132486012\n",
      "Iteration: 1830, AutoEncoder Loss: 0.001123284372\n",
      "Iteration: 1840, AutoEncoder Loss: 0.001114293837\n",
      "Iteration: 1850, AutoEncoder Loss: 0.001105437474\n",
      "Iteration: 1860, AutoEncoder Loss: 0.001097052836\n",
      "Iteration: 1870, AutoEncoder Loss: 0.001088199120\n",
      "Iteration: 1880, AutoEncoder Loss: 0.001079743911\n",
      "Iteration: 1890, AutoEncoder Loss: 0.001071566508\n",
      "Iteration: 1900, AutoEncoder Loss: 0.001063269713\n",
      "Iteration: 1910, AutoEncoder Loss: 0.001055205963\n",
      "Iteration: 1920, AutoEncoder Loss: 0.001047256673\n",
      "Iteration: 1930, AutoEncoder Loss: 0.001039460930\n",
      "Iteration: 1940, AutoEncoder Loss: 0.001031727278\n",
      "Iteration: 1950, AutoEncoder Loss: 0.001024132815\n",
      "Iteration: 1960, AutoEncoder Loss: 0.001016784371\n",
      "Iteration: 1970, AutoEncoder Loss: 0.001009275884\n",
      "Iteration: 1980, AutoEncoder Loss: 0.001001979585\n",
      "Iteration: 1990, AutoEncoder Loss: 0.000994782685\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    reconstruction = [auto(i) for i in x]\n",
    "    \n",
    "    loss = sum([mean_squared_error(recon, targ) for recon, targ in zip(reconstruction, target)])\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration: {i}, AutoEncoder Loss: {loss.data:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected 1, result, 0.9828378220266868\n",
      "expected 0, result, 0.012172861565287187\n",
      "expected 0, result, 3.901187256193221e-47\n",
      "expected 0, result, 0.005003588986116644\n",
      "expected 0, result, 6.857949047430323e-09\n",
      "--------------------------------\n",
      "expected 0, result, 0.005675473893101285\n",
      "expected 1, result, 0.9898491343551946\n",
      "expected 0, result, 6.2553952232554994e-77\n",
      "expected 0, result, 1.9639129351845353e-47\n",
      "expected 0, result, 3.83708905981204e-05\n",
      "--------------------------------\n",
      "expected 0, result, 0.005871274840454873\n",
      "expected 0, result, 0.001939028395184959\n",
      "expected 1, result, 0.9851282356482717\n",
      "expected 0, result, 0.014210616354544917\n",
      "expected 0, result, 0.037494859788822464\n",
      "--------------------------------\n",
      "expected 0, result, 0.039206799781306576\n",
      "expected 0, result, 0.011868570847641412\n",
      "expected 0, result, 0.009582516696300777\n",
      "expected 1, result, 0.9842390069729133\n",
      "expected 0, result, 0.0023686139937995885\n",
      "--------------------------------\n",
      "expected 0, result, 0.0005334597907096435\n",
      "expected 0, result, 0.00910955834983402\n",
      "expected 0, result, 0.005125694688581963\n",
      "expected 0, result, 5.906605465276877e-18\n",
      "expected 1, result, 0.981692733290781\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x[0])):\n",
    "    for j in range(len(x)):\n",
    "        print(f\"expected {x[j][i]}, result, {auto(x[j])[i].data}\")\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(\n",
    "    in_embeds=10,  \n",
    "    hidden_layers=[7],\n",
    "    latent_dim=4,     \n",
    "    act_func=Value.sigmoid\n",
    ")\n",
    "\n",
    "vae_x = [Value(random.uniform(0, 1)) for _ in range(10)]\n",
    "\n",
    "vae_target = vae_x\n",
    "\n",
    "vae_optimizer = SGD(vae.parameters(), learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, VAE Loss: 0.170626104863\n",
      "Iteration 10, VAE Loss: 0.131525882735\n",
      "Iteration 20, VAE Loss: 0.108139044673\n",
      "Iteration 30, VAE Loss: 0.096432586365\n",
      "Iteration 40, VAE Loss: 0.097865735823\n",
      "Iteration 50, VAE Loss: 0.074479838637\n",
      "Iteration 60, VAE Loss: 0.061534849368\n",
      "Iteration 70, VAE Loss: 0.094860216655\n",
      "Iteration 80, VAE Loss: 0.061495898405\n",
      "Iteration 90, VAE Loss: 0.085765781855\n",
      "Iteration 100, VAE Loss: 0.059986325212\n",
      "Iteration 110, VAE Loss: 0.077054973399\n",
      "Iteration 120, VAE Loss: 0.078699398175\n",
      "Iteration 130, VAE Loss: 0.069944847449\n",
      "Iteration 140, VAE Loss: 0.048835501760\n",
      "Iteration 150, VAE Loss: 0.060075495213\n",
      "Iteration 160, VAE Loss: 0.060551678856\n",
      "Iteration 170, VAE Loss: 0.046948542038\n",
      "Iteration 180, VAE Loss: 0.045171046028\n",
      "Iteration 190, VAE Loss: 0.049456701024\n",
      "Iteration 200, VAE Loss: 0.041387394008\n",
      "Iteration 210, VAE Loss: 0.056881396611\n",
      "Iteration 220, VAE Loss: 0.049644293772\n",
      "Iteration 230, VAE Loss: 0.030230020479\n",
      "Iteration 240, VAE Loss: 0.028289717526\n",
      "Iteration 250, VAE Loss: 0.023803624618\n",
      "Iteration 260, VAE Loss: 0.032132003378\n",
      "Iteration 270, VAE Loss: 0.033080873677\n",
      "Iteration 280, VAE Loss: 0.046725220416\n",
      "Iteration 290, VAE Loss: 0.015487247023\n",
      "Iteration 300, VAE Loss: 0.013601438645\n",
      "Iteration 310, VAE Loss: 0.022900371647\n",
      "Iteration 320, VAE Loss: 0.026169101632\n",
      "Iteration 330, VAE Loss: 0.027425702513\n",
      "Iteration 340, VAE Loss: 0.016684773073\n",
      "Iteration 350, VAE Loss: 0.013760191057\n",
      "Iteration 360, VAE Loss: 0.012436402134\n",
      "Iteration 370, VAE Loss: 0.020407388963\n",
      "Iteration 380, VAE Loss: 0.018986163560\n",
      "Iteration 390, VAE Loss: 0.009953897800\n",
      "Iteration 400, VAE Loss: 0.012870496804\n",
      "Iteration 410, VAE Loss: 0.025482563624\n",
      "Iteration 420, VAE Loss: 0.011224466103\n",
      "Iteration 430, VAE Loss: 0.007571031428\n",
      "Iteration 440, VAE Loss: 0.012603746683\n",
      "Iteration 450, VAE Loss: 0.012658939799\n",
      "Iteration 460, VAE Loss: 0.011502672241\n",
      "Iteration 470, VAE Loss: 0.019142171122\n",
      "Iteration 480, VAE Loss: 0.006896953810\n",
      "Iteration 490, VAE Loss: 0.008346324400\n",
      "Iteration 500, VAE Loss: 0.017808504259\n",
      "Iteration 510, VAE Loss: 0.009195026738\n",
      "Iteration 520, VAE Loss: 0.017544034566\n",
      "Iteration 530, VAE Loss: 0.008226393179\n",
      "Iteration 540, VAE Loss: 0.006617401761\n",
      "Iteration 550, VAE Loss: 0.008434225990\n",
      "Iteration 560, VAE Loss: 0.013780386171\n",
      "Iteration 570, VAE Loss: 0.008029818086\n",
      "Iteration 580, VAE Loss: 0.007838742058\n",
      "Iteration 590, VAE Loss: 0.006709324014\n",
      "Iteration 600, VAE Loss: 0.012551941445\n",
      "Iteration 610, VAE Loss: 0.009587256417\n",
      "Iteration 620, VAE Loss: 0.007727322014\n",
      "Iteration 630, VAE Loss: 0.010288589056\n",
      "Iteration 640, VAE Loss: 0.002357953156\n",
      "Iteration 650, VAE Loss: 0.008166288971\n",
      "Iteration 660, VAE Loss: 0.002748242711\n",
      "Iteration 670, VAE Loss: 0.007259792085\n",
      "Iteration 680, VAE Loss: 0.010722847742\n",
      "Iteration 690, VAE Loss: 0.008227571556\n",
      "Iteration 700, VAE Loss: 0.005095050207\n",
      "Iteration 710, VAE Loss: 0.005631459276\n",
      "Iteration 720, VAE Loss: 0.002917191123\n",
      "Iteration 730, VAE Loss: 0.003959966475\n",
      "Iteration 740, VAE Loss: 0.005954053854\n",
      "Iteration 750, VAE Loss: 0.004443120413\n",
      "Iteration 760, VAE Loss: 0.014904446988\n",
      "Iteration 770, VAE Loss: 0.007094751815\n",
      "Iteration 780, VAE Loss: 0.004321538165\n",
      "Iteration 790, VAE Loss: 0.005143897084\n",
      "Iteration 800, VAE Loss: 0.006298324818\n",
      "Iteration 810, VAE Loss: 0.004605789570\n",
      "Iteration 820, VAE Loss: 0.002451651390\n",
      "Iteration 830, VAE Loss: 0.002602039552\n",
      "Iteration 840, VAE Loss: 0.002109065602\n",
      "Iteration 850, VAE Loss: 0.003946287386\n",
      "Iteration 860, VAE Loss: 0.002225624585\n",
      "Iteration 870, VAE Loss: 0.003005188305\n",
      "Iteration 880, VAE Loss: 0.004431960971\n",
      "Iteration 890, VAE Loss: 0.002864506513\n",
      "Iteration 900, VAE Loss: 0.009861456293\n",
      "Iteration 910, VAE Loss: 0.003137857039\n",
      "Iteration 920, VAE Loss: 0.004837670631\n",
      "Iteration 930, VAE Loss: 0.003861863056\n",
      "Iteration 940, VAE Loss: 0.005271835358\n",
      "Iteration 950, VAE Loss: 0.003663313075\n",
      "Iteration 960, VAE Loss: 0.003451957346\n",
      "Iteration 970, VAE Loss: 0.003242078302\n",
      "Iteration 980, VAE Loss: 0.003826145842\n",
      "Iteration 990, VAE Loss: 0.006193365956\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    vae_optimizer.zero_grad()\n",
    "\n",
    "    reconstruction, mu, log_var = vae(vae_x)\n",
    "\n",
    "    if not isinstance(reconstruction, list):\n",
    "        reconstruction = [reconstruction]\n",
    "\n",
    "\n",
    "    loss = vae_loss(reconstruction, vae_target, mu, log_var)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward() \n",
    "\n",
    "    \n",
    "    vae_optimizer.step()\n",
    "\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}, VAE Loss: {loss.data:.12f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Value: 0.484368020718412, generated Value: 0.556659056499744\n",
      "Actual Value: 0.151270886744849, generated Value: 0.09377571083477061\n",
      "Actual Value: 0.10854135277607113, generated Value: 0.13579411652640058\n",
      "Actual Value: 0.2047944506633067, generated Value: 0.1931632424309115\n",
      "Actual Value: 0.8996515210360224, generated Value: 0.8766529052359195\n",
      "Actual Value: 0.7044760413408526, generated Value: 0.7176015852633344\n",
      "Actual Value: 0.1631162699452855, generated Value: 0.14863566238654993\n",
      "Actual Value: 0.043721800004570266, generated Value: 0.13885014252413172\n",
      "Actual Value: 0.0022668172091712124, generated Value: 0.06705478010374977\n",
      "Actual Value: 0.2372607572927734, generated Value: 0.14875811361140243\n"
     ]
    }
   ],
   "source": [
    "noise = np.random.normal(size=(10))\n",
    "\n",
    "generated_samples = vae.decode(noise)\n",
    "for i in range(len(noise)):\n",
    "    print(f\"Actual Value: {vae_x[i].data}, generated Value: {generated_samples[i].data}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
