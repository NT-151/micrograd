{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# import engine\n",
    "# import nn\n",
    "# importlib.reload(engine)\n",
    "# importlib.reload(nn)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.    , 0.    , 0.3125, ..., 0.    , 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , ..., 0.625 , 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , ..., 1.    , 0.5625, 0.    ],\n",
       "       ...,\n",
       "       [0.    , 0.    , 0.0625, ..., 0.375 , 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.125 , ..., 0.75  , 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.625 , ..., 0.75  , 0.0625, 0.    ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = MinMaxScaler()\n",
    "t.fit(X)\n",
    "X_train = t.transform(X)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def loss(batch_size):\n",
    "#     epsilon = 1e-8\n",
    "#     ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "#     Xb = X[ri]\n",
    "#     ri = np.random.permutation(y.shape[0])[:batch_size]\n",
    "#     yb = y[ri]\n",
    "    \n",
    "#     inputs = [list(map(engine.Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "#     print(\"is me\", inputs)\n",
    "    \n",
    "#     scores = [n(input) for input in inputs]\n",
    "    \n",
    "    \n",
    "#     scores = [i.sigmoid() for i in scores]\n",
    "    \n",
    "#     combination = zip(scores, yb)\n",
    "# # \n",
    "#     combination = [i for i in combination if not np.isnan(i[0].data)]\n",
    "    \n",
    "\n",
    "#     Binary_Cross_Entropy = [(truth * ((y + epsilon).log()) + (1 - truth) * ((1 - y + epsilon).log())) for y, truth in combination]\n",
    "    \n",
    "#     loss = sum(Binary_Cross_Entropy) * -(1/len(scores))\n",
    "    \n",
    "#     alpha = 1e-4\n",
    "#     reg_loss = alpha * sum((p*p for p in n.parameters()))\n",
    "    \n",
    "    \n",
    "#     total_loss = loss + reg_loss\n",
    "    \n",
    "#     accuracy = [(yi > 0) == (scorei.data > 0) for scorei, yi in combination]\n",
    "    \n",
    "#     return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "# print(loss(15))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optimization\n",
    "# for k in range(100):\n",
    "    \n",
    "#     # forward\n",
    "#     total_loss, acc = loss(100)\n",
    "    \n",
    "#     # backward\n",
    "#     n.zero_grad()\n",
    "#     total_loss.backward()\n",
    "    \n",
    "#     # update (sgd)\n",
    "#     learning_rate = 1.0 - 0.9*k/100\n",
    "#     for p in n.parameters():\n",
    "#         p.data -= learning_rate * p.grad\n",
    "    \n",
    "#     if k % 1 == 0:\n",
    "#         print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)\n",
    "                          ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # fix dead neuron problem\n",
    "    def leaky_relu(self):\n",
    "        out = Value(self.data * 0.01 if self.data <=\n",
    "                    0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            local_grad = 1.0 if self.data > 0 else 0.01\n",
    "            self.grad += local_grad * out.grad \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "\n",
    "        out = Value(math.log(self.data), (self, ), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        t = 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "        out = Value(t, (self, ), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data * (1 - out.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other.data\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.data > other.data\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=-2.70118143540359, grad=0), Value(data=2.4023136894932335, grad=0), Value(data=-3.54287180637959, grad=0), Value(data=2.238346393859305, grad=0), Value(data=3.4033814120977426, grad=0), Value(data=-0.5534481657779167, grad=0)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "import importlib\n",
    "\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def zero_weights(self):\n",
    "        for w in self.weights():\n",
    "            for j in w:\n",
    "                print(j)\n",
    "                j.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def weights(self):\n",
    "        return []\n",
    "\n",
    "    def layers(self):\n",
    "        return []\n",
    "\n",
    "    def summary(self):\n",
    "        return f\"{len(self.layers())} layers, {len(self.parameters)} parameters\"\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # I want to introduce weight sharing, which means I need to be able to\n",
    "    # initialise a neuron with pre defined weights, but leave the bias?\n",
    "\n",
    "    def __init__(self, nin, nonlin=True, **kwargs):\n",
    "        tied_weights = kwargs.get('tied_weights', None)\n",
    "        self.w = tied_weights if tied_weights is not None else [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        self.nonlin = nonlin\n",
    "        self.activate = kwargs.get('activate', None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # print(\"come on then\", self.w, \"come on son                   \",x)\n",
    "        if isinstance(x, Value):\n",
    "            # This is for a single input, likely at the start of a layer\n",
    "            act = self.w[0] * x + self.b\n",
    "        else:\n",
    "            act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        if self.activate and self.nonlin == False:\n",
    "            return self.activate(act)\n",
    "        else:\n",
    "            return act.leaky_relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b] if isinstance(self.w[0], Value) else [p for w_list in self.w for p in w_list] + [self.b]\n",
    "\n",
    "    def weights(self):\n",
    "        # return self.w without the brackets\n",
    "        return [self.w]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else '{self.activate}'}Neuron({len(self.w)})\"\n",
    "\n",
    "\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, tied_to_layer=None, **kwargs):\n",
    "        if tied_to_layer is None:\n",
    "            # Standard Layer initialization\n",
    "            self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "        else:\n",
    "            # Tied Layer initialization\n",
    "            # The weights for this layer are the transpose of the tied_to_layer's weights\n",
    "            # This requires careful construction.\n",
    "            # Number of inputs for this layer = number of outputs of the tied layer\n",
    "            # Number of outputs for this layer = number of inputs of the tied layer\n",
    "            self.tied_to_layer = tied_to_layer\n",
    "            self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            # The weights are conceptually transposed.\n",
    "            # So, the output of a neuron is sum(w_ji * x_i), which means summing over the neurons of the previous layer.\n",
    "            # This is hard to do cleanly with the current structure.\n",
    "            # The simpler approach is to loop manually.\n",
    "            out = []\n",
    "            for j in range(len(self.neurons)):\n",
    "                # The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\n",
    "                # For each output neuron (j), sum the weighted inputs.\n",
    "                # The weight connecting input `i` to output `j` is the same as the weight connecting\n",
    "                # input `j` of the encoder layer to output `i`.\n",
    "                act = sum(self.tied_to_layer.neurons[i].w[j] * x[i] for i in range(len(x))) + self.neurons[j].b if isinstance(x, list) else self.tied_to_layer.neurons[0].w[j] * x + self.neurons[j].b\n",
    "                \n",
    "                # Apply activation\n",
    "                if self.neurons[j].activate and self.neurons[j].nonlin is False:\n",
    "                    act = self.neurons[j].activate(act)\n",
    "                else:\n",
    "                    act = act.relu() if self.neurons[j].nonlin else act\n",
    "                out.append(act)\n",
    "            return out[0] if len(out) == 1 else out\n",
    "        else:\n",
    "            # Standard layer behavior\n",
    "            out = [n(x) for n in self.neurons]\n",
    "            return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        # In a tied layer, the weights are shared, but the biases are not.\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            return [n.b for n in self.neurons]\n",
    "        else:\n",
    "            return [p for n in self.neurons for p in n.parameters()]\n",
    "                \n",
    "\n",
    "    def weights(self):\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            # The weights belong to the encoder, so we don't need to report them here\n",
    "            # as they are included in the encoder's parameters list.\n",
    "            return []\n",
    "        else:\n",
    "            return [p for n in self.neurons for p in n.weights()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts, tied_weights_from=None, **kwargs):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = []\n",
    "        if tied_weights_from is None:\n",
    "            # Standard MLP initialization\n",
    "            self.layers = [Layer(sz[i], sz[i+1], nonlin=i != len(nouts)-1, **kwargs) for i in range(len(nouts))]\n",
    "        else:\n",
    "            # Tied-weight MLP initialization\n",
    "            tied_layers = list(reversed(tied_weights_from))\n",
    "            for i in range(len(nouts)):\n",
    "                # Pass the encoder's layer directly to the decoder's layer.\n",
    "                # The decoder layer will use the encoder's weights.\n",
    "                self.layers.append(Layer(sz[i], sz[i+1], tied_to_layer=tied_layers[i], nonlin=i != len(nouts)-1, **kwargs))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def weights(self):\n",
    "        # [[p for p in layer.weights()] for layer in self.layers] \n",
    "        return [p for layer in self.layers for p in layer.weights()]\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "\n",
    "class AutoEncoder(Module):\n",
    "    def __init__(self, in_embeds, n_hidden_layers, compressed, act_func=None, tied=False):\n",
    "        n_hidden_layers = [math.ceil(in_embeds / i)\n",
    "                           for i in range(2, n_hidden_layers + 2)]\n",
    "        \n",
    "        \n",
    "        self.act_func = act_func\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoder = MLP(in_embeds, n_hidden_layers + [compressed])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create decoder, passing encoder layers for tied weights\n",
    "        if tied:\n",
    "            self.decoder = MLP(compressed, list(reversed(n_hidden_layers)) + [in_embeds], tied_weights_from=self.encoder.layers, activate=act_func)\n",
    "        else:\n",
    "            self.decoder = MLP(compressed, list(reversed(n_hidden_layers)) + [in_embeds], activate=act_func)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        compressed = self.encoder(x)\n",
    "        out = self.decoder(compressed)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.encoder.parameters() + self.decoder.parameters()\n",
    "\n",
    "    def weights(self):\n",
    "        return self.encoder.weights()\n",
    "\n",
    "    def layers(self):\n",
    "        return self.encoder.layers + self.decoder.layers\n",
    "\n",
    "    def pretty(self):\n",
    "        if self.act_func != None:\n",
    "            hey = str(self.act_func)\n",
    "            return hey.split()[1][6:]\n",
    "        else:\n",
    "            return \"no function\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"encoder has {self.summary()}, decoder has {self.summary()} activated with {self.pretty()}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoEncoder(6, 1, 2, tied=True)\n",
    "\n",
    "hey = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "print(model(hey))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers\"\"\"\n",
    "\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Take a step of gradient descent\"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Update model parameters in the opposite direction of their gradient\"\"\"\n",
    "\n",
    "        for p in self.parameters:\n",
    "            p.data -= self.learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data utilities\n",
    "\n",
    "Heavily inspired by https://github.com/joelgrus/joelnet/blob/master/joelnet/data.py\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=too-few-public-methods\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# Batch = NamedTuple(\"Batch\", [(\"inputs\", List[Vector]), (\"targets\", Vector)])\n",
    "\n",
    "\n",
    "class BatchIterator:\n",
    "    \"\"\"Iterates on data by batches\"\"\"\n",
    "\n",
    "    def __init__(self,inputs,targets,batch_size=1, shuffle=True):\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __call__(self):\n",
    "        starts = list(range(0, len(self.inputs), self.batch_size))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(starts)\n",
    "\n",
    "        for start in starts:\n",
    "            end = start + self.batch_size\n",
    "            batch_inputs = self.inputs[start:end]\n",
    "            batch_targets = self.targets[start:end]\n",
    "            yield (batch_inputs, batch_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"MSE loss\"\"\"\n",
    "\n",
    "    total_squared_error = sum((y_true_i - y_pred_i)**2 for (y_true_i, y_pred_i) in zip(y_true, y_pred))\n",
    "    n_total = max(len(y_true), 1)\n",
    "    return total_squared_error / n_total\n",
    "\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    \"\"\"Binary accuracy\"\"\"\n",
    "\n",
    "    n_exact = sum(\n",
    "        (\n",
    "            y_true_i.data == round(y_pred_i.data)\n",
    "            for (y_true_i, y_pred_i) in zip(y_true, y_pred)\n",
    "        )\n",
    "    )\n",
    "    n_total: int = max(len(y_true), 1)\n",
    "    return n_exact / n_total\n",
    "\n",
    "def array_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"MSE loss\"\"\"\n",
    "\n",
    "    total_squared_error = sum((y_true_i - y_pred_i)**2 for y_true_i, y_pred_i in zip(y_true, y_pred))\n",
    "    n_total = max(len(y_true), 1)\n",
    "    thingy = total_squared_error / \n",
    "    return sum(total_squared_error) / len(y_true[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyfit.engine import Vector, Scalar\n",
    "# from pyfit.nn import Module\n",
    "# from pyfit.optim import Optimizer\n",
    "# from pyfit.data import BatchIterator\n",
    "# from pyfit.metrics import binary_accuracy\n",
    "\n",
    "# Used to record training history for metrics\n",
    "History = {}\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Encapsulates the model training loop\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, data_iterator, num_epochs=500, verbose=False):\n",
    "        \"\"\"Fits the model to the data\"\"\"\n",
    "\n",
    "        history = {\"loss\": [], \"acc\": []}\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        epoch_y_true = []\n",
    "        epoch_y_pred = []\n",
    "        for epoch in range(num_epochs):\n",
    "            # Reset the gradients of model parameters\n",
    "            self.optimizer.zero_grad()\n",
    "            # Reset epoch data\n",
    "            epoch_loss = 0\n",
    "            epoch_y_true = []\n",
    "            epoch_y_pred = []\n",
    "\n",
    "            for batch in data_iterator():\n",
    "                # Forward pass\n",
    "                outputs = list(map(self.model, batch[0]))\n",
    "                \n",
    "\n",
    "                # Loss computation\n",
    "                # [item for sublist in outputs[0] for item in sublist]\n",
    "                batch_y_pred = outputs\n",
    "                batch_loss = self.loss(batch[1], batch_y_pred)\n",
    "                print(batch_loss)\n",
    "                epoch_loss += batch_loss.data\n",
    "\n",
    "                # Store batch predictions and ground truth for computing epoch metrics\n",
    "                epoch_y_pred.extend(batch_y_pred)\n",
    "                epoch_y_true.extend(batch[1])\n",
    "\n",
    "                # Backprop and gradient descent\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Accuracy computation for epoch\n",
    "            epoch_acc = binary_accuracy(epoch_y_true, epoch_y_pred)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Record training history\n",
    "            history[\"loss\"].append(epoch_loss)\n",
    "            history[\"acc\"].append(epoch_acc)\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                    f\"loss: {epoch_loss:.6f}, \"\n",
    "                    f\"accuracy: {epoch_acc*100:.2f}%\"\n",
    "                )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=91.87404562871508, grad=0)\n",
      "Value(data=140653.8916777004, grad=0)\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(64, 1, 2, tied=True)\n",
    "\n",
    "hey = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "true = list(map(lambda x: Value(x), hey))\n",
    "\n",
    "\n",
    "optimizer = SGD(model.parameters(), learning_rate=0.05)\n",
    "data_iterator = BatchIterator(X_train, X_train, batch_size=32)\n",
    "trainer = Trainer(model, optimizer, loss=array_mean_squared_error)\n",
    "\n",
    "history = trainer.fit(data_iterator, num_epochs=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
