{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1524,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1525,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # the op that produced this node, for graphviz / debugging / etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1526,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = np.asarray(data, dtype=float) if not (isinstance(data, np.ndarray)) else data\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # the op that produced this node, for graphviz / debugging / etc\n",
    "    \n",
    "      \n",
    "    def unbroadcast(self, grad, shape):\n",
    "        grad = np.asarray(grad)\n",
    "        # remove leading dims\n",
    "        while grad.ndim > len(shape):\n",
    "            grad = grad.sum(axis=0)\n",
    "        # sum over axes where original had dim 1\n",
    "        for i, (g, s) in enumerate(zip(grad.shape, shape)):\n",
    "            if s == 1 and g != 1:\n",
    "                grad = grad.sum(axis=i, keepdims=True)\n",
    "        return grad\n",
    "            \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += self.unbroadcast(out.grad, self.data.shape)\n",
    "            other.grad += self.unbroadcast(out.grad, other.data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += self.unbroadcast(other.data * out.grad, self.data.shape)\n",
    "            other.grad += self.unbroadcast(self.data * out.grad, other.data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)\n",
    "                          ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(np.maximum(0, self.data), (self,), 'relu')\n",
    "        def _backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        EPSILON = 1e-7\n",
    "        clipped_data = np.maximum(EPSILON, self.data)\n",
    "\n",
    "        out = Value(np.log(clipped_data), (self, ), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / clipped_data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(np.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        t = 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "        out = Value(t, (self, ), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data * (1 - out.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "   \n",
    "    def sum(self):\n",
    "        out = Value(np.sum(self.data), (self,), 'sum')\n",
    "        \n",
    "        def _backward():\n",
    "            # grad of sum is 1, broadcasted to the input shape\n",
    "            self.grad += np.ones_like(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data @ other.data, (self, other), '@')\n",
    "        def _backward():\n",
    "            # handle vectors and matrices by relying on numpy broadcasting \n",
    "            self.grad += out.grad @ other.data.T\n",
    "            other.grad += self.data.T @ out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "   \n",
    "    def backward(self):\n",
    "    \n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "            \n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other.data\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        other_data = other.data if isinstance(other, Value) else other\n",
    "        return self.data < other_data\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        other_data = other.data if isinstance(other, Value) else other\n",
    "        return self.data > other_data\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1527,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "            \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "class Linear(Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        k = np.sqrt(2.0 / nin)\n",
    "        self.W = Value(np.random.uniform(-k, k, (nin, nout)))\n",
    "        self.b = Value(np.full((nout,), 0.1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # x: (B,nin) or (nin,)\n",
    "        return (x @ self.W) + self.b\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts, act_func=None):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Linear(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "        self.act_func = act_func\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i != len(self.layers)-1:\n",
    "                x = x.relu()\n",
    "            else:\n",
    "                if self.act_func is not None:\n",
    "                    x = self.act_func(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "class AutoEncoder(Module):\n",
    "    def __init__(self, in_embeds=1, hidden_layers=[], latent_dim=1, act_func=None):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.act_func = act_func\n",
    "        self.encoder = MLP(in_embeds, hidden_layers + [latent_dim])\n",
    "        self.decoder = MLP(latent_dim, list(\n",
    "            reversed(hidden_layers)) + [in_embeds], act_func=act_func)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        compressed = self.encoder(x)\n",
    "        out = self.decoder(compressed)\n",
    "        return out\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, x):\n",
    "        decoded = self.decoder(x)\n",
    "        return decoded\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.encoder.parameters() + self.decoder.parameters()\n",
    "\n",
    "    def layers(self):\n",
    "        return self.encoder.layers + self.decoder.layers\n",
    "\n",
    "    def pretty(self):\n",
    "        if self.act_func != None:\n",
    "            hey = str(self.act_func)\n",
    "            return hey.split()[1][6:]\n",
    "        else:\n",
    "            return \"no function\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"encoder has {len(self.encoder.layers)}, decoder has {len(self.decoder.layers)}, latent dim is {self.latent_dim} activated with {self.pretty()}\"\n",
    "    \n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers\"\"\"\n",
    "\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "        \n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"Take a step of gradient descent\"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, parameters, learning_rate=0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.learning_rate = learning_rate \n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            p.data = p.data - (self.learning_rate * p.grad)\n",
    "\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     total_loss = sum([(true - pred)**2 for true, pred in zip(y_true, y_pred)])\n",
    "\n",
    "#     return total_loss\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    loss = ((pred - target) ** 2).sum()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def mse_mean(target, pred):\n",
    "    diff = pred - target\n",
    "    return (diff*diff).sum() * (1.0 / diff.data.size)\n",
    "\n",
    "\n",
    "def vae_loss(reconstruction, target, mu, log_var, beta=1.0):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder loss = Reconstruction Loss + beta * KL Divergence\n",
    "\n",
    "    Args:\n",
    "        reconstruction: Value or List of Value objects (reconstructed output)\n",
    "        target: List of Value objects (original input)\n",
    "        mu: List of Value objects (mean of latent distribution)\n",
    "        log_var: List of Value objects (log variance of latent distribution)\n",
    "        beta: Weight for KL divergence term (default 1.0)\n",
    "\n",
    "    Returns:\n",
    "        Total VAE loss as a Value object\n",
    "    \"\"\"\n",
    "    # Ensure reconstruction is a list\n",
    "    if not isinstance(reconstruction, list):\n",
    "        reconstruction = [reconstruction]\n",
    "    if not isinstance(target, list):\n",
    "        target = [target]\n",
    "\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = mean_squared_error(target, reconstruction)\n",
    "\n",
    "    # KL divergence: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "    # This encourages the latent distribution to be close to N(0,1)\n",
    "    kl_terms = []\n",
    "    for mu_i, log_var_i in zip(mu, log_var):\n",
    "        # KL term for one dimension: -0.5 * (1 + log_var - mu^2 - exp(log_var))\n",
    "        kl_term = -0.5 * (Value(1.0) + log_var_i - mu_i**2 - log_var_i.exp())\n",
    "        kl_terms.append(kl_term)\n",
    "\n",
    "    kl_loss = sum(kl_terms) if kl_terms else Value(0.0)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1528,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "]\n",
    "targ_x = Value(np.array(x, dtype=float))        # shape (5, 5)\n",
    "target = Value(np.array(x, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) False -0.6287859868760816 0.6291530304070804\n",
      "(3,) False 0.1 0.1\n",
      "(3, 2) False -0.7987022424909085 0.773155961583631\n",
      "(2,) False 0.1 0.1\n",
      "(2, 3) False -0.6757651484800478 0.8222448031206269\n",
      "(3,) False 0.1 0.1\n",
      "(3, 5) False -0.5266925751676413 0.6704715830524097\n",
      "(5,) False 0.1 0.1\n"
     ]
    }
   ],
   "source": [
    "auto = AutoEncoder(\n",
    "    in_embeds=5,# input dimension \n",
    "    hidden_layers=[3],# len = number of layers, i = size of layer\n",
    "    latent_dim=2, # compressed layer dimensions\n",
    "    act_func=Value.sigmoid, # activation function for final decoder layer\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = optimizer = SGD(auto.parameters(), learning_rate=0.1)\n",
    "\n",
    "\n",
    "for p in auto.parameters():\n",
    "    print(p.data.shape, np.isnan(p.data).any(), p.data.min(), p.data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, AutoEncoder Loss: 6.753892859672\n",
      "Iteration: 10, AutoEncoder Loss: 4.435918171812\n",
      "Iteration: 20, AutoEncoder Loss: 4.008833027870\n",
      "Iteration: 30, AutoEncoder Loss: 3.951953483310\n",
      "Iteration: 40, AutoEncoder Loss: 3.895236167980\n",
      "Iteration: 50, AutoEncoder Loss: 3.797511803108\n",
      "Iteration: 60, AutoEncoder Loss: 3.672095296078\n",
      "Iteration: 70, AutoEncoder Loss: 3.477095240012\n",
      "Iteration: 80, AutoEncoder Loss: 3.172970719593\n",
      "Iteration: 90, AutoEncoder Loss: 2.910107839118\n",
      "Iteration: 100, AutoEncoder Loss: 2.732536033329\n",
      "Iteration: 110, AutoEncoder Loss: 2.608587015995\n",
      "Iteration: 120, AutoEncoder Loss: 2.517729244166\n",
      "Iteration: 130, AutoEncoder Loss: 2.494025377702\n",
      "Iteration: 140, AutoEncoder Loss: 2.345426306355\n",
      "Iteration: 150, AutoEncoder Loss: 2.343552359123\n",
      "Iteration: 160, AutoEncoder Loss: 2.250331650139\n",
      "Iteration: 170, AutoEncoder Loss: 2.104093812627\n",
      "Iteration: 180, AutoEncoder Loss: 2.055092089597\n",
      "Iteration: 190, AutoEncoder Loss: 2.022239615661\n",
      "Iteration: 200, AutoEncoder Loss: 1.974317031105\n",
      "Iteration: 210, AutoEncoder Loss: 1.928347583148\n",
      "Iteration: 220, AutoEncoder Loss: 1.875278642664\n",
      "Iteration: 230, AutoEncoder Loss: 1.811066207858\n",
      "Iteration: 240, AutoEncoder Loss: 1.722512608869\n",
      "Iteration: 250, AutoEncoder Loss: 1.701816324774\n",
      "Iteration: 260, AutoEncoder Loss: 1.650779100601\n",
      "Iteration: 270, AutoEncoder Loss: 1.524926410510\n",
      "Iteration: 280, AutoEncoder Loss: 1.698105813023\n",
      "Iteration: 290, AutoEncoder Loss: 1.873354850384\n",
      "Iteration: 300, AutoEncoder Loss: 1.545388027526\n",
      "Iteration: 310, AutoEncoder Loss: 1.335705512798\n",
      "Iteration: 320, AutoEncoder Loss: 1.604142106993\n",
      "Iteration: 330, AutoEncoder Loss: 1.514984376855\n",
      "Iteration: 340, AutoEncoder Loss: 1.501355659976\n",
      "Iteration: 350, AutoEncoder Loss: 1.479128416321\n",
      "Iteration: 360, AutoEncoder Loss: 1.434881671589\n",
      "Iteration: 370, AutoEncoder Loss: 1.454173998970\n",
      "Iteration: 380, AutoEncoder Loss: 1.418672573709\n",
      "Iteration: 390, AutoEncoder Loss: 1.090204521185\n",
      "Iteration: 400, AutoEncoder Loss: 1.353064475044\n",
      "Iteration: 410, AutoEncoder Loss: 1.270884043818\n",
      "Iteration: 420, AutoEncoder Loss: 1.175099320380\n",
      "Iteration: 430, AutoEncoder Loss: 1.246089747258\n",
      "Iteration: 440, AutoEncoder Loss: 1.241047559523\n",
      "Iteration: 450, AutoEncoder Loss: 1.048289994972\n",
      "Iteration: 460, AutoEncoder Loss: 1.170744953530\n",
      "Iteration: 470, AutoEncoder Loss: 1.104022913648\n",
      "Iteration: 480, AutoEncoder Loss: 0.939829032742\n",
      "Iteration: 490, AutoEncoder Loss: 0.808085378366\n",
      "Iteration: 500, AutoEncoder Loss: 0.860530977577\n",
      "Iteration: 510, AutoEncoder Loss: 0.716377886488\n",
      "Iteration: 520, AutoEncoder Loss: 0.775427870049\n",
      "Iteration: 530, AutoEncoder Loss: 0.619480496449\n",
      "Iteration: 540, AutoEncoder Loss: 0.524776423617\n",
      "Iteration: 550, AutoEncoder Loss: 0.519552725844\n",
      "Iteration: 560, AutoEncoder Loss: 0.414763647715\n",
      "Iteration: 570, AutoEncoder Loss: 0.647154423009\n",
      "Iteration: 580, AutoEncoder Loss: 0.334269082642\n",
      "Iteration: 590, AutoEncoder Loss: 0.321173841328\n",
      "Iteration: 600, AutoEncoder Loss: 0.502716124456\n",
      "Iteration: 610, AutoEncoder Loss: 1.001998265176\n",
      "Iteration: 620, AutoEncoder Loss: 0.173998941770\n",
      "Iteration: 630, AutoEncoder Loss: 0.147059438848\n",
      "Iteration: 640, AutoEncoder Loss: 0.135627094168\n",
      "Iteration: 650, AutoEncoder Loss: 0.126513518570\n",
      "Iteration: 660, AutoEncoder Loss: 0.118800325018\n",
      "Iteration: 670, AutoEncoder Loss: 0.112113612648\n",
      "Iteration: 680, AutoEncoder Loss: 0.106231764308\n",
      "Iteration: 690, AutoEncoder Loss: 0.101000883018\n",
      "Iteration: 700, AutoEncoder Loss: 0.096339372409\n",
      "Iteration: 710, AutoEncoder Loss: 0.092127388528\n",
      "Iteration: 720, AutoEncoder Loss: 0.088287893073\n",
      "Iteration: 730, AutoEncoder Loss: 0.084769867377\n",
      "Iteration: 740, AutoEncoder Loss: 0.081585554685\n",
      "Iteration: 750, AutoEncoder Loss: 0.078587123728\n",
      "Iteration: 760, AutoEncoder Loss: 0.075821943544\n",
      "Iteration: 770, AutoEncoder Loss: 0.073251186868\n",
      "Iteration: 780, AutoEncoder Loss: 0.070873112534\n",
      "Iteration: 790, AutoEncoder Loss: 0.068613239931\n",
      "Iteration: 800, AutoEncoder Loss: 0.066505057764\n",
      "Iteration: 810, AutoEncoder Loss: 0.064528002676\n",
      "Iteration: 820, AutoEncoder Loss: 0.062666254221\n",
      "Iteration: 830, AutoEncoder Loss: 0.060904608266\n",
      "Iteration: 840, AutoEncoder Loss: 0.059250360551\n",
      "Iteration: 850, AutoEncoder Loss: 0.057654987043\n",
      "Iteration: 860, AutoEncoder Loss: 0.056157451354\n",
      "Iteration: 870, AutoEncoder Loss: 0.054735691554\n",
      "Iteration: 880, AutoEncoder Loss: 0.053383616209\n",
      "Iteration: 890, AutoEncoder Loss: 0.052089266891\n",
      "Iteration: 900, AutoEncoder Loss: 0.050860116404\n",
      "Iteration: 910, AutoEncoder Loss: 0.049686456243\n",
      "Iteration: 920, AutoEncoder Loss: 0.048560251709\n",
      "Iteration: 930, AutoEncoder Loss: 0.047487528710\n",
      "Iteration: 940, AutoEncoder Loss: 0.046458791914\n",
      "Iteration: 950, AutoEncoder Loss: 0.045469858568\n",
      "Iteration: 960, AutoEncoder Loss: 0.044524439468\n",
      "Iteration: 970, AutoEncoder Loss: 0.043615937318\n",
      "Iteration: 980, AutoEncoder Loss: 0.042742031701\n",
      "Iteration: 990, AutoEncoder Loss: 0.041900670364\n",
      "Iteration: 1000, AutoEncoder Loss: 0.041092890146\n",
      "Iteration: 1010, AutoEncoder Loss: 0.040314062447\n",
      "Iteration: 1020, AutoEncoder Loss: 0.039561917348\n",
      "Iteration: 1030, AutoEncoder Loss: 0.038839770706\n",
      "Iteration: 1040, AutoEncoder Loss: 0.038139938902\n",
      "Iteration: 1050, AutoEncoder Loss: 0.037464716603\n",
      "Iteration: 1060, AutoEncoder Loss: 0.036813654802\n",
      "Iteration: 1070, AutoEncoder Loss: 0.036181421699\n",
      "Iteration: 1080, AutoEncoder Loss: 0.035572648288\n",
      "Iteration: 1090, AutoEncoder Loss: 0.034987588712\n",
      "Iteration: 1100, AutoEncoder Loss: 0.034410225154\n",
      "Iteration: 1110, AutoEncoder Loss: 0.033857175333\n",
      "Iteration: 1120, AutoEncoder Loss: 0.033319393065\n",
      "Iteration: 1130, AutoEncoder Loss: 0.032799648974\n",
      "Iteration: 1140, AutoEncoder Loss: 0.032296052617\n",
      "Iteration: 1150, AutoEncoder Loss: 0.031804526555\n",
      "Iteration: 1160, AutoEncoder Loss: 0.031329248637\n",
      "Iteration: 1170, AutoEncoder Loss: 0.030866617764\n",
      "Iteration: 1180, AutoEncoder Loss: 0.030417131869\n",
      "Iteration: 1190, AutoEncoder Loss: 0.030000183547\n",
      "Iteration: 1200, AutoEncoder Loss: 0.029571425684\n",
      "Iteration: 1210, AutoEncoder Loss: 0.029156617504\n",
      "Iteration: 1220, AutoEncoder Loss: 0.028753544336\n",
      "Iteration: 1230, AutoEncoder Loss: 0.028362525763\n",
      "Iteration: 1240, AutoEncoder Loss: 0.027979843840\n",
      "Iteration: 1250, AutoEncoder Loss: 0.027608386058\n",
      "Iteration: 1260, AutoEncoder Loss: 0.027245251595\n",
      "Iteration: 1270, AutoEncoder Loss: 0.026892432358\n",
      "Iteration: 1280, AutoEncoder Loss: 0.027905945146\n",
      "Iteration: 1290, AutoEncoder Loss: 0.026227035063\n",
      "Iteration: 1300, AutoEncoder Loss: 0.025896622204\n",
      "Iteration: 1310, AutoEncoder Loss: 0.025575765795\n",
      "Iteration: 1320, AutoEncoder Loss: 0.025262447164\n",
      "Iteration: 1330, AutoEncoder Loss: 0.024956390796\n",
      "Iteration: 1340, AutoEncoder Loss: 0.024658449108\n",
      "Iteration: 1350, AutoEncoder Loss: 0.024366370867\n",
      "Iteration: 1360, AutoEncoder Loss: 0.024081848021\n",
      "Iteration: 1370, AutoEncoder Loss: 0.023803461250\n",
      "Iteration: 1380, AutoEncoder Loss: 0.023542369731\n",
      "Iteration: 1390, AutoEncoder Loss: 0.023273906786\n",
      "Iteration: 1400, AutoEncoder Loss: 0.023012594315\n",
      "Iteration: 1410, AutoEncoder Loss: 0.022756950033\n",
      "Iteration: 1420, AutoEncoder Loss: 0.022506786833\n",
      "Iteration: 1430, AutoEncoder Loss: 0.022262599735\n",
      "Iteration: 1440, AutoEncoder Loss: 0.022022744652\n",
      "Iteration: 1450, AutoEncoder Loss: 0.021788580676\n",
      "Iteration: 1460, AutoEncoder Loss: 0.021604961015\n",
      "Iteration: 1470, AutoEncoder Loss: 0.021343113036\n",
      "Iteration: 1480, AutoEncoder Loss: 0.021120940109\n",
      "Iteration: 1490, AutoEncoder Loss: 0.020904215371\n",
      "Iteration: 1500, AutoEncoder Loss: 0.020691762451\n",
      "Iteration: 1510, AutoEncoder Loss: 0.020483630926\n",
      "Iteration: 1520, AutoEncoder Loss: 0.020279888558\n",
      "Iteration: 1530, AutoEncoder Loss: 0.020081494650\n",
      "Iteration: 1540, AutoEncoder Loss: 0.019883640360\n",
      "Iteration: 1550, AutoEncoder Loss: 0.019706520273\n",
      "Iteration: 1560, AutoEncoder Loss: 0.019509538248\n",
      "Iteration: 1570, AutoEncoder Loss: 0.019322978277\n",
      "Iteration: 1580, AutoEncoder Loss: 0.019140379064\n",
      "Iteration: 1590, AutoEncoder Loss: 0.018961198165\n",
      "Iteration: 1600, AutoEncoder Loss: 0.018787387592\n",
      "Iteration: 1610, AutoEncoder Loss: 0.018612936939\n",
      "Iteration: 1620, AutoEncoder Loss: 0.018443754216\n",
      "Iteration: 1630, AutoEncoder Loss: 0.018276735640\n",
      "Iteration: 1640, AutoEncoder Loss: 0.018122299408\n",
      "Iteration: 1650, AutoEncoder Loss: 0.017958240635\n",
      "Iteration: 1660, AutoEncoder Loss: 0.017799511540\n",
      "Iteration: 1670, AutoEncoder Loss: 0.017643717436\n",
      "Iteration: 1680, AutoEncoder Loss: 0.017490483884\n",
      "Iteration: 1690, AutoEncoder Loss: 0.017340447635\n",
      "Iteration: 1700, AutoEncoder Loss: 0.017192270044\n",
      "Iteration: 1710, AutoEncoder Loss: 0.017047007319\n",
      "Iteration: 1720, AutoEncoder Loss: 0.016903653042\n",
      "Iteration: 1730, AutoEncoder Loss: 0.016768177647\n",
      "Iteration: 1740, AutoEncoder Loss: 0.016628700103\n",
      "Iteration: 1750, AutoEncoder Loss: 0.016492033658\n",
      "Iteration: 1760, AutoEncoder Loss: 0.016357948898\n",
      "Iteration: 1770, AutoEncoder Loss: 0.016225610221\n",
      "Iteration: 1780, AutoEncoder Loss: 0.016095736810\n",
      "Iteration: 1790, AutoEncoder Loss: 0.015967417185\n",
      "Iteration: 1800, AutoEncoder Loss: 0.015841438319\n",
      "Iteration: 1810, AutoEncoder Loss: 0.015722375252\n",
      "Iteration: 1820, AutoEncoder Loss: 0.015598706318\n",
      "Iteration: 1830, AutoEncoder Loss: 0.015477865589\n",
      "Iteration: 1840, AutoEncoder Loss: 0.015358971024\n",
      "Iteration: 1850, AutoEncoder Loss: 0.015242110434\n",
      "Iteration: 1860, AutoEncoder Loss: 0.015126736341\n",
      "Iteration: 1870, AutoEncoder Loss: 0.015013123647\n",
      "Iteration: 1880, AutoEncoder Loss: 0.014915804202\n",
      "Iteration: 1890, AutoEncoder Loss: 0.014794956995\n",
      "Iteration: 1900, AutoEncoder Loss: 0.014685388417\n",
      "Iteration: 1910, AutoEncoder Loss: 0.014577910578\n",
      "Iteration: 1920, AutoEncoder Loss: 0.014471916315\n",
      "Iteration: 1930, AutoEncoder Loss: 0.014367886692\n",
      "Iteration: 1940, AutoEncoder Loss: 0.014264722762\n",
      "Iteration: 1950, AutoEncoder Loss: 0.014163479807\n",
      "Iteration: 1960, AutoEncoder Loss: 0.014063205311\n",
      "Iteration: 1970, AutoEncoder Loss: 0.013968854183\n",
      "Iteration: 1980, AutoEncoder Loss: 0.013870045016\n",
      "Iteration: 1990, AutoEncoder Loss: 0.013773707428\n",
      "Iteration: 2000, AutoEncoder Loss: 0.013679080657\n",
      "Iteration: 2010, AutoEncoder Loss: 0.013585340740\n",
      "Iteration: 2020, AutoEncoder Loss: 0.013493117302\n",
      "Iteration: 2030, AutoEncoder Loss: 0.013401870230\n",
      "Iteration: 2040, AutoEncoder Loss: 0.013312058352\n",
      "Iteration: 2050, AutoEncoder Loss: 0.013227186986\n",
      "Iteration: 2060, AutoEncoder Loss: 0.013138228300\n",
      "Iteration: 2070, AutoEncoder Loss: 0.013051490815\n",
      "Iteration: 2080, AutoEncoder Loss: 0.012965906182\n",
      "Iteration: 2090, AutoEncoder Loss: 0.012881638148\n",
      "Iteration: 2100, AutoEncoder Loss: 0.012798220988\n",
      "Iteration: 2110, AutoEncoder Loss: 0.012716046963\n",
      "Iteration: 2120, AutoEncoder Loss: 0.012634666316\n",
      "Iteration: 2130, AutoEncoder Loss: 0.012557901396\n",
      "Iteration: 2140, AutoEncoder Loss: 0.012477751376\n",
      "Iteration: 2150, AutoEncoder Loss: 0.012399228917\n",
      "Iteration: 2160, AutoEncoder Loss: 0.012321698684\n",
      "Iteration: 2170, AutoEncoder Loss: 0.012245219166\n",
      "Iteration: 2180, AutoEncoder Loss: 0.012169665277\n",
      "Iteration: 2190, AutoEncoder Loss: 0.012095670368\n",
      "Iteration: 2200, AutoEncoder Loss: 0.012021245107\n",
      "Iteration: 2210, AutoEncoder Loss: 0.011952080483\n",
      "Iteration: 2220, AutoEncoder Loss: 0.011878430050\n",
      "Iteration: 2230, AutoEncoder Loss: 0.011806999460\n",
      "Iteration: 2240, AutoEncoder Loss: 0.011736785721\n",
      "Iteration: 2250, AutoEncoder Loss: 0.011666978559\n",
      "Iteration: 2260, AutoEncoder Loss: 0.011598370446\n",
      "Iteration: 2270, AutoEncoder Loss: 0.011530145123\n",
      "Iteration: 2280, AutoEncoder Loss: 0.011468167152\n",
      "Iteration: 2290, AutoEncoder Loss: 0.011398559261\n",
      "Iteration: 2300, AutoEncoder Loss: 0.011332489594\n",
      "Iteration: 2310, AutoEncoder Loss: 0.011267348644\n",
      "Iteration: 2320, AutoEncoder Loss: 0.011202938354\n",
      "Iteration: 2330, AutoEncoder Loss: 0.011139396567\n",
      "Iteration: 2340, AutoEncoder Loss: 0.011076400349\n",
      "Iteration: 2350, AutoEncoder Loss: 0.011014237682\n",
      "Iteration: 2360, AutoEncoder Loss: 0.010952598243\n",
      "Iteration: 2370, AutoEncoder Loss: 0.010893768522\n",
      "Iteration: 2380, AutoEncoder Loss: 0.010833112459\n",
      "Iteration: 2390, AutoEncoder Loss: 0.010773397181\n",
      "Iteration: 2400, AutoEncoder Loss: 0.010714477163\n",
      "Iteration: 2410, AutoEncoder Loss: 0.010656042828\n",
      "Iteration: 2420, AutoEncoder Loss: 0.010598363439\n",
      "Iteration: 2430, AutoEncoder Loss: 0.010541142616\n",
      "Iteration: 2440, AutoEncoder Loss: 0.010486817442\n",
      "Iteration: 2450, AutoEncoder Loss: 0.010430327616\n",
      "Iteration: 2460, AutoEncoder Loss: 0.010374765940\n",
      "Iteration: 2470, AutoEncoder Loss: 0.010319836607\n",
      "Iteration: 2480, AutoEncoder Loss: 0.010265993252\n",
      "Iteration: 2490, AutoEncoder Loss: 0.010211824625\n",
      "Iteration: 2500, AutoEncoder Loss: 0.010158791457\n",
      "Iteration: 2510, AutoEncoder Loss: 0.010105984037\n",
      "Iteration: 2520, AutoEncoder Loss: 0.010058365485\n",
      "Iteration: 2530, AutoEncoder Loss: 0.010003769967\n",
      "Iteration: 2540, AutoEncoder Loss: 0.009952481539\n",
      "Iteration: 2550, AutoEncoder Loss: 0.009901977550\n",
      "Iteration: 2560, AutoEncoder Loss: 0.009851757211\n",
      "Iteration: 2570, AutoEncoder Loss: 0.009802241560\n",
      "Iteration: 2580, AutoEncoder Loss: 0.009753003135\n",
      "Iteration: 2590, AutoEncoder Loss: 0.009708585898\n",
      "Iteration: 2600, AutoEncoder Loss: 0.009657701718\n",
      "Iteration: 2610, AutoEncoder Loss: 0.009609717858\n",
      "Iteration: 2620, AutoEncoder Loss: 0.009562347570\n",
      "Iteration: 2630, AutoEncoder Loss: 0.009515436886\n",
      "Iteration: 2640, AutoEncoder Loss: 0.009469124339\n",
      "Iteration: 2650, AutoEncoder Loss: 0.009423082908\n",
      "Iteration: 2660, AutoEncoder Loss: 0.009377630204\n",
      "Iteration: 2670, AutoEncoder Loss: 0.009332452265\n",
      "Iteration: 2680, AutoEncoder Loss: 0.009289484208\n",
      "Iteration: 2690, AutoEncoder Loss: 0.009244833901\n",
      "Iteration: 2700, AutoEncoder Loss: 0.009200852953\n",
      "Iteration: 2710, AutoEncoder Loss: 0.009157315707\n",
      "Iteration: 2720, AutoEncoder Loss: 0.009114178563\n",
      "Iteration: 2730, AutoEncoder Loss: 0.009071542047\n",
      "Iteration: 2740, AutoEncoder Loss: 0.009029188742\n",
      "Iteration: 2750, AutoEncoder Loss: 0.008988889568\n",
      "Iteration: 2760, AutoEncoder Loss: 0.008946953444\n",
      "Iteration: 2770, AutoEncoder Loss: 0.008905667746\n",
      "Iteration: 2780, AutoEncoder Loss: 0.008864790946\n",
      "Iteration: 2790, AutoEncoder Loss: 0.008824640223\n",
      "Iteration: 2800, AutoEncoder Loss: 0.008784231781\n",
      "Iteration: 2810, AutoEncoder Loss: 0.008744784461\n",
      "Iteration: 2820, AutoEncoder Loss: 0.008705079492\n",
      "Iteration: 2830, AutoEncoder Loss: 0.008668249576\n",
      "Iteration: 2840, AutoEncoder Loss: 0.008628581553\n",
      "Iteration: 2850, AutoEncoder Loss: 0.008589986216\n",
      "Iteration: 2860, AutoEncoder Loss: 0.008551864615\n",
      "Iteration: 2870, AutoEncoder Loss: 0.008514073612\n",
      "Iteration: 2880, AutoEncoder Loss: 0.008476935280\n",
      "Iteration: 2890, AutoEncoder Loss: 0.008439555851\n",
      "Iteration: 2900, AutoEncoder Loss: 0.008410970734\n",
      "Iteration: 2910, AutoEncoder Loss: 0.008367498065\n",
      "Iteration: 2920, AutoEncoder Loss: 0.008331106398\n",
      "Iteration: 2930, AutoEncoder Loss: 0.008295172728\n",
      "Iteration: 2940, AutoEncoder Loss: 0.008259545262\n",
      "Iteration: 2950, AutoEncoder Loss: 0.008224329829\n",
      "Iteration: 2960, AutoEncoder Loss: 0.008189270036\n",
      "Iteration: 2970, AutoEncoder Loss: 0.008154659517\n",
      "Iteration: 2980, AutoEncoder Loss: 0.008120157029\n",
      "Iteration: 2990, AutoEncoder Loss: 0.008087243759\n",
      "Iteration: 3000, AutoEncoder Loss: 0.008053172487\n",
      "Iteration: 3010, AutoEncoder Loss: 0.008019506584\n",
      "Iteration: 3020, AutoEncoder Loss: 0.007986135553\n",
      "Iteration: 3030, AutoEncoder Loss: 0.007953033876\n",
      "Iteration: 3040, AutoEncoder Loss: 0.007920298541\n",
      "Iteration: 3050, AutoEncoder Loss: 0.007889068525\n",
      "Iteration: 3060, AutoEncoder Loss: 0.007856351733\n",
      "Iteration: 3070, AutoEncoder Loss: 0.007824241458\n",
      "Iteration: 3080, AutoEncoder Loss: 0.007792424546\n",
      "Iteration: 3090, AutoEncoder Loss: 0.007760953478\n",
      "Iteration: 3100, AutoEncoder Loss: 0.007729636393\n",
      "Iteration: 3110, AutoEncoder Loss: 0.007698643962\n",
      "Iteration: 3120, AutoEncoder Loss: 0.007667812614\n",
      "Iteration: 3130, AutoEncoder Loss: 0.007638391977\n",
      "Iteration: 3140, AutoEncoder Loss: 0.007607865229\n",
      "Iteration: 3150, AutoEncoder Loss: 0.007577707775\n",
      "Iteration: 3160, AutoEncoder Loss: 0.007547803599\n",
      "Iteration: 3170, AutoEncoder Loss: 0.007518378619\n",
      "Iteration: 3180, AutoEncoder Loss: 0.007488763659\n",
      "Iteration: 3190, AutoEncoder Loss: 0.007462157733\n",
      "Iteration: 3200, AutoEncoder Loss: 0.007431548728\n",
      "Iteration: 3210, AutoEncoder Loss: 0.007402634111\n",
      "Iteration: 3220, AutoEncoder Loss: 0.007374048871\n",
      "Iteration: 3230, AutoEncoder Loss: 0.007345685836\n",
      "Iteration: 3240, AutoEncoder Loss: 0.007317691104\n",
      "Iteration: 3250, AutoEncoder Loss: 0.007289670611\n",
      "Iteration: 3260, AutoEncoder Loss: 0.007265632034\n",
      "Iteration: 3270, AutoEncoder Loss: 0.007235373741\n",
      "Iteration: 3280, AutoEncoder Loss: 0.007207916231\n",
      "Iteration: 3290, AutoEncoder Loss: 0.007180772791\n",
      "Iteration: 3300, AutoEncoder Loss: 0.007153828241\n",
      "Iteration: 3310, AutoEncoder Loss: 0.007127167970\n",
      "Iteration: 3320, AutoEncoder Loss: 0.007100603488\n",
      "Iteration: 3330, AutoEncoder Loss: 0.007077250597\n",
      "Iteration: 3340, AutoEncoder Loss: 0.007048980974\n",
      "Iteration: 3350, AutoEncoder Loss: 0.007022886796\n",
      "Iteration: 3360, AutoEncoder Loss: 0.006997073281\n",
      "Iteration: 3370, AutoEncoder Loss: 0.006971449646\n",
      "Iteration: 3380, AutoEncoder Loss: 0.006946072537\n",
      "Iteration: 3390, AutoEncoder Loss: 0.006920811410\n",
      "Iteration: 3400, AutoEncoder Loss: 0.006895803230\n",
      "Iteration: 3410, AutoEncoder Loss: 0.006873226212\n",
      "Iteration: 3420, AutoEncoder Loss: 0.006847003244\n",
      "Iteration: 3430, AutoEncoder Loss: 0.006822339123\n",
      "Iteration: 3440, AutoEncoder Loss: 0.006797935355\n",
      "Iteration: 3450, AutoEncoder Loss: 0.006773708162\n",
      "Iteration: 3460, AutoEncoder Loss: 0.006749643475\n",
      "Iteration: 3470, AutoEncoder Loss: 0.006725811808\n",
      "Iteration: 3480, AutoEncoder Loss: 0.006702082423\n",
      "Iteration: 3490, AutoEncoder Loss: 0.006679422950\n",
      "Iteration: 3500, AutoEncoder Loss: 0.006655849782\n",
      "Iteration: 3510, AutoEncoder Loss: 0.006632562938\n",
      "Iteration: 3520, AutoEncoder Loss: 0.006609466441\n",
      "Iteration: 3530, AutoEncoder Loss: 0.006586710410\n",
      "Iteration: 3540, AutoEncoder Loss: 0.006563793108\n",
      "Iteration: 3550, AutoEncoder Loss: 0.006547476887\n",
      "Iteration: 3560, AutoEncoder Loss: 0.006519461264\n",
      "Iteration: 3570, AutoEncoder Loss: 0.006497007448\n",
      "Iteration: 3580, AutoEncoder Loss: 0.006474804236\n",
      "Iteration: 3590, AutoEncoder Loss: 0.006452759979\n",
      "Iteration: 3600, AutoEncoder Loss: 0.006431037262\n",
      "Iteration: 3610, AutoEncoder Loss: 0.006409159668\n",
      "Iteration: 3620, AutoEncoder Loss: 0.006392009821\n",
      "Iteration: 3630, AutoEncoder Loss: 0.006366777004\n",
      "Iteration: 3640, AutoEncoder Loss: 0.006345372448\n",
      "Iteration: 3650, AutoEncoder Loss: 0.006324172727\n",
      "Iteration: 3660, AutoEncoder Loss: 0.006303113919\n",
      "Iteration: 3670, AutoEncoder Loss: 0.006282254348\n",
      "Iteration: 3680, AutoEncoder Loss: 0.006261447209\n",
      "Iteration: 3690, AutoEncoder Loss: 0.006243262516\n",
      "Iteration: 3700, AutoEncoder Loss: 0.006220963767\n",
      "Iteration: 3710, AutoEncoder Loss: 0.006200482652\n",
      "Iteration: 3720, AutoEncoder Loss: 0.006180204318\n",
      "Iteration: 3730, AutoEncoder Loss: 0.006160056297\n",
      "Iteration: 3740, AutoEncoder Loss: 0.006140096055\n",
      "Iteration: 3750, AutoEncoder Loss: 0.006120208911\n",
      "Iteration: 3760, AutoEncoder Loss: 0.006100498217\n",
      "Iteration: 3770, AutoEncoder Loss: 0.006082605953\n",
      "Iteration: 3780, AutoEncoder Loss: 0.006061981103\n",
      "Iteration: 3790, AutoEncoder Loss: 0.006042518923\n",
      "Iteration: 3800, AutoEncoder Loss: 0.006023233697\n",
      "Iteration: 3810, AutoEncoder Loss: 0.006004072562\n",
      "Iteration: 3820, AutoEncoder Loss: 0.005985032665\n",
      "Iteration: 3830, AutoEncoder Loss: 0.005966889636\n",
      "Iteration: 3840, AutoEncoder Loss: 0.005947874429\n",
      "Iteration: 3850, AutoEncoder Loss: 0.005929132020\n",
      "Iteration: 3860, AutoEncoder Loss: 0.005910543748\n",
      "Iteration: 3870, AutoEncoder Loss: 0.005892172008\n",
      "Iteration: 3880, AutoEncoder Loss: 0.005873759974\n",
      "Iteration: 3890, AutoEncoder Loss: 0.005855648297\n",
      "Iteration: 3900, AutoEncoder Loss: 0.005837420874\n",
      "Iteration: 3910, AutoEncoder Loss: 0.005821207230\n",
      "Iteration: 3920, AutoEncoder Loss: 0.005802067403\n",
      "Iteration: 3930, AutoEncoder Loss: 0.005784168948\n",
      "Iteration: 3940, AutoEncoder Loss: 0.005766445310\n",
      "Iteration: 3950, AutoEncoder Loss: 0.005748833793\n",
      "Iteration: 3960, AutoEncoder Loss: 0.005731426854\n",
      "Iteration: 3970, AutoEncoder Loss: 0.005713976907\n",
      "Iteration: 3980, AutoEncoder Loss: 0.005700235838\n",
      "Iteration: 3990, AutoEncoder Loss: 0.005680040549\n",
      "Iteration: 4000, AutoEncoder Loss: 0.005662878032\n",
      "Iteration: 4010, AutoEncoder Loss: 0.005645874631\n",
      "Iteration: 4020, AutoEncoder Loss: 0.005628974818\n",
      "Iteration: 4030, AutoEncoder Loss: 0.005612223394\n",
      "Iteration: 4040, AutoEncoder Loss: 0.005595508868\n",
      "Iteration: 4050, AutoEncoder Loss: 0.005580965394\n",
      "Iteration: 4060, AutoEncoder Loss: 0.005562942071\n",
      "Iteration: 4070, AutoEncoder Loss: 0.005546444486\n",
      "Iteration: 4080, AutoEncoder Loss: 0.005530105624\n",
      "Iteration: 4090, AutoEncoder Loss: 0.005513871894\n",
      "Iteration: 4100, AutoEncoder Loss: 0.005497771345\n",
      "Iteration: 4110, AutoEncoder Loss: 0.005481717016\n",
      "Iteration: 4120, AutoEncoder Loss: 0.005465805248\n",
      "Iteration: 4130, AutoEncoder Loss: 0.005451417497\n",
      "Iteration: 4140, AutoEncoder Loss: 0.005434659495\n",
      "Iteration: 4150, AutoEncoder Loss: 0.005418914150\n",
      "Iteration: 4160, AutoEncoder Loss: 0.005403301741\n",
      "Iteration: 4170, AutoEncoder Loss: 0.005387779407\n",
      "Iteration: 4180, AutoEncoder Loss: 0.005372350229\n",
      "Iteration: 4190, AutoEncoder Loss: 0.005357558477\n",
      "Iteration: 4200, AutoEncoder Loss: 0.005342179581\n",
      "Iteration: 4210, AutoEncoder Loss: 0.005326976348\n",
      "Iteration: 4220, AutoEncoder Loss: 0.005311877806\n",
      "Iteration: 4230, AutoEncoder Loss: 0.005296972458\n",
      "Iteration: 4240, AutoEncoder Loss: 0.005281966950\n",
      "Iteration: 4250, AutoEncoder Loss: 0.005267197116\n",
      "Iteration: 4260, AutoEncoder Loss: 0.005252385495\n",
      "Iteration: 4270, AutoEncoder Loss: 0.005238724880\n",
      "Iteration: 4280, AutoEncoder Loss: 0.005223556504\n",
      "Iteration: 4290, AutoEncoder Loss: 0.005208966857\n",
      "Iteration: 4300, AutoEncoder Loss: 0.005194506497\n",
      "Iteration: 4310, AutoEncoder Loss: 0.005180124035\n",
      "Iteration: 4320, AutoEncoder Loss: 0.005165904218\n",
      "Iteration: 4330, AutoEncoder Loss: 0.005151639920\n",
      "Iteration: 4340, AutoEncoder Loss: 0.005140350638\n",
      "Iteration: 4350, AutoEncoder Loss: 0.005123867685\n",
      "Iteration: 4360, AutoEncoder Loss: 0.005109816124\n",
      "Iteration: 4370, AutoEncoder Loss: 0.005095887356\n",
      "Iteration: 4380, AutoEncoder Loss: 0.005082033977\n",
      "Iteration: 4390, AutoEncoder Loss: 0.005068297570\n",
      "Iteration: 4400, AutoEncoder Loss: 0.005054586065\n",
      "Iteration: 4410, AutoEncoder Loss: 0.005042046680\n",
      "Iteration: 4420, AutoEncoder Loss: 0.005027826545\n",
      "Iteration: 4430, AutoEncoder Loss: 0.005014276557\n",
      "Iteration: 4440, AutoEncoder Loss: 0.005000846445\n",
      "Iteration: 4450, AutoEncoder Loss: 0.004987487198\n",
      "Iteration: 4460, AutoEncoder Loss: 0.004974236862\n",
      "Iteration: 4470, AutoEncoder Loss: 0.004961021478\n",
      "Iteration: 4480, AutoEncoder Loss: 0.004947903578\n",
      "Iteration: 4490, AutoEncoder Loss: 0.004936103790\n",
      "Iteration: 4500, AutoEncoder Loss: 0.004922215610\n",
      "Iteration: 4510, AutoEncoder Loss: 0.004909227806\n",
      "Iteration: 4520, AutoEncoder Loss: 0.004896343151\n",
      "Iteration: 4530, AutoEncoder Loss: 0.004883520730\n",
      "Iteration: 4540, AutoEncoder Loss: 0.004870777394\n",
      "Iteration: 4550, AutoEncoder Loss: 0.004858541942\n",
      "Iteration: 4560, AutoEncoder Loss: 0.004845818211\n",
      "Iteration: 4570, AutoEncoder Loss: 0.004833237046\n",
      "Iteration: 4580, AutoEncoder Loss: 0.004820736519\n",
      "Iteration: 4590, AutoEncoder Loss: 0.004808351280\n",
      "Iteration: 4600, AutoEncoder Loss: 0.004795955685\n",
      "Iteration: 4610, AutoEncoder Loss: 0.004783731974\n",
      "Iteration: 4620, AutoEncoder Loss: 0.004771422509\n",
      "Iteration: 4630, AutoEncoder Loss: 0.004760557694\n",
      "Iteration: 4640, AutoEncoder Loss: 0.004747496513\n",
      "Iteration: 4650, AutoEncoder Loss: 0.004735370355\n",
      "Iteration: 4660, AutoEncoder Loss: 0.004723348936\n",
      "Iteration: 4670, AutoEncoder Loss: 0.004711396244\n",
      "Iteration: 4680, AutoEncoder Loss: 0.004699552792\n",
      "Iteration: 4690, AutoEncoder Loss: 0.004687694644\n",
      "Iteration: 4700, AutoEncoder Loss: 0.004678217835\n",
      "Iteration: 4710, AutoEncoder Loss: 0.004664563825\n",
      "Iteration: 4720, AutoEncoder Loss: 0.004652851861\n",
      "Iteration: 4730, AutoEncoder Loss: 0.004641239266\n",
      "Iteration: 4740, AutoEncoder Loss: 0.004629688495\n",
      "Iteration: 4750, AutoEncoder Loss: 0.004618226438\n",
      "Iteration: 4760, AutoEncoder Loss: 0.004606779237\n",
      "Iteration: 4770, AutoEncoder Loss: 0.004596330266\n",
      "Iteration: 4780, AutoEncoder Loss: 0.004584420948\n",
      "Iteration: 4790, AutoEncoder Loss: 0.004573096033\n",
      "Iteration: 4800, AutoEncoder Loss: 0.004561865423\n",
      "Iteration: 4810, AutoEncoder Loss: 0.004550688162\n",
      "Iteration: 4820, AutoEncoder Loss: 0.004539600647\n",
      "Iteration: 4830, AutoEncoder Loss: 0.004528532125\n",
      "Iteration: 4840, AutoEncoder Loss: 0.004517547497\n",
      "Iteration: 4850, AutoEncoder Loss: 0.004507791443\n",
      "Iteration: 4860, AutoEncoder Loss: 0.004496025259\n",
      "Iteration: 4870, AutoEncoder Loss: 0.004485115547\n",
      "Iteration: 4880, AutoEncoder Loss: 0.004474299249\n",
      "Iteration: 4890, AutoEncoder Loss: 0.004463539076\n",
      "Iteration: 4900, AutoEncoder Loss: 0.004452828408\n",
      "Iteration: 4910, AutoEncoder Loss: 0.004442199038\n",
      "Iteration: 4920, AutoEncoder Loss: 0.004431586499\n",
      "Iteration: 4930, AutoEncoder Loss: 0.004421408132\n",
      "Iteration: 4940, AutoEncoder Loss: 0.004410815722\n",
      "Iteration: 4950, AutoEncoder Loss: 0.004400339249\n",
      "Iteration: 4960, AutoEncoder Loss: 0.004389919442\n",
      "Iteration: 4970, AutoEncoder Loss: 0.004379607810\n",
      "Iteration: 4980, AutoEncoder Loss: 0.004369257652\n",
      "Iteration: 4990, AutoEncoder Loss: 0.004366238927\n",
      "Iteration: 5000, AutoEncoder Loss: 0.004349103575\n",
      "Iteration: 5010, AutoEncoder Loss: 0.004338861271\n",
      "Iteration: 5020, AutoEncoder Loss: 0.004328716198\n",
      "Iteration: 5030, AutoEncoder Loss: 0.004318626554\n",
      "Iteration: 5040, AutoEncoder Loss: 0.004308625337\n",
      "Iteration: 5050, AutoEncoder Loss: 0.004298609306\n",
      "Iteration: 5060, AutoEncoder Loss: 0.004288700396\n",
      "Iteration: 5070, AutoEncoder Loss: 0.004286139138\n",
      "Iteration: 5080, AutoEncoder Loss: 0.004269225016\n",
      "Iteration: 5090, AutoEncoder Loss: 0.004259361229\n",
      "Iteration: 5100, AutoEncoder Loss: 0.004249579730\n",
      "Iteration: 5110, AutoEncoder Loss: 0.004239842737\n",
      "Iteration: 5120, AutoEncoder Loss: 0.004230153945\n",
      "Iteration: 5130, AutoEncoder Loss: 0.004221307117\n",
      "Iteration: 5140, AutoEncoder Loss: 0.004211198801\n",
      "Iteration: 5150, AutoEncoder Loss: 0.004201599255\n",
      "Iteration: 5160, AutoEncoder Loss: 0.004192070081\n",
      "Iteration: 5170, AutoEncoder Loss: 0.004182591541\n",
      "Iteration: 5180, AutoEncoder Loss: 0.004173175401\n",
      "Iteration: 5190, AutoEncoder Loss: 0.004163780814\n",
      "Iteration: 5200, AutoEncoder Loss: 0.004154448973\n",
      "Iteration: 5210, AutoEncoder Loss: 0.004145144058\n",
      "Iteration: 5220, AutoEncoder Loss: 0.004136185139\n",
      "Iteration: 5230, AutoEncoder Loss: 0.004126887426\n",
      "Iteration: 5240, AutoEncoder Loss: 0.004117677617\n",
      "Iteration: 5250, AutoEncoder Loss: 0.004108521964\n",
      "Iteration: 5260, AutoEncoder Loss: 0.004099403410\n",
      "Iteration: 5270, AutoEncoder Loss: 0.004090352720\n",
      "Iteration: 5280, AutoEncoder Loss: 0.004081313489\n",
      "Iteration: 5290, AutoEncoder Loss: 0.004072731919\n",
      "Iteration: 5300, AutoEncoder Loss: 0.004063612630\n",
      "Iteration: 5310, AutoEncoder Loss: 0.004054673836\n",
      "Iteration: 5320, AutoEncoder Loss: 0.004045785869\n",
      "Iteration: 5330, AutoEncoder Loss: 0.004036940619\n",
      "Iteration: 5340, AutoEncoder Loss: 0.004028152400\n",
      "Iteration: 5350, AutoEncoder Loss: 0.004019382691\n",
      "Iteration: 5360, AutoEncoder Loss: 0.004010998762\n",
      "Iteration: 5370, AutoEncoder Loss: 0.004002201906\n",
      "Iteration: 5380, AutoEncoder Loss: 0.003993522040\n",
      "Iteration: 5390, AutoEncoder Loss: 0.003984890880\n",
      "Iteration: 5400, AutoEncoder Loss: 0.003976346699\n",
      "Iteration: 5410, AutoEncoder Loss: 0.003967768529\n",
      "Iteration: 5420, AutoEncoder Loss: 0.003959296830\n",
      "Iteration: 5430, AutoEncoder Loss: 0.003950790377\n",
      "Iteration: 5440, AutoEncoder Loss: 0.003942707509\n",
      "Iteration: 5450, AutoEncoder Loss: 0.003934178304\n",
      "Iteration: 5460, AutoEncoder Loss: 0.003925778482\n",
      "Iteration: 5470, AutoEncoder Loss: 0.003917429509\n",
      "Iteration: 5480, AutoEncoder Loss: 0.003909120765\n",
      "Iteration: 5490, AutoEncoder Loss: 0.003900877356\n",
      "Iteration: 5500, AutoEncoder Loss: 0.003892626244\n",
      "Iteration: 5510, AutoEncoder Loss: 0.003885951479\n",
      "Iteration: 5520, AutoEncoder Loss: 0.003876494000\n",
      "Iteration: 5530, AutoEncoder Loss: 0.003868323255\n",
      "Iteration: 5540, AutoEncoder Loss: 0.003860211697\n",
      "Iteration: 5550, AutoEncoder Loss: 0.003852137067\n",
      "Iteration: 5560, AutoEncoder Loss: 0.003844121142\n",
      "Iteration: 5570, AutoEncoder Loss: 0.003836106686\n",
      "Iteration: 5580, AutoEncoder Loss: 0.003828524836\n",
      "Iteration: 5590, AutoEncoder Loss: 0.003820420213\n",
      "Iteration: 5600, AutoEncoder Loss: 0.003812482918\n",
      "Iteration: 5610, AutoEncoder Loss: 0.003804597548\n",
      "Iteration: 5620, AutoEncoder Loss: 0.003796746242\n",
      "Iteration: 5630, AutoEncoder Loss: 0.003788947856\n",
      "Iteration: 5640, AutoEncoder Loss: 0.003781160247\n",
      "Iteration: 5650, AutoEncoder Loss: 0.003773424725\n",
      "Iteration: 5660, AutoEncoder Loss: 0.003766251502\n",
      "Iteration: 5670, AutoEncoder Loss: 0.003758234787\n",
      "Iteration: 5680, AutoEncoder Loss: 0.003750545914\n",
      "Iteration: 5690, AutoEncoder Loss: 0.003742908260\n",
      "Iteration: 5700, AutoEncoder Loss: 0.003735304187\n",
      "Iteration: 5710, AutoEncoder Loss: 0.003727728567\n",
      "Iteration: 5720, AutoEncoder Loss: 0.003720523774\n",
      "Iteration: 5730, AutoEncoder Loss: 0.003712891216\n",
      "Iteration: 5740, AutoEncoder Loss: 0.003705382804\n",
      "Iteration: 5750, AutoEncoder Loss: 0.003697923605\n",
      "Iteration: 5760, AutoEncoder Loss: 0.003690535406\n",
      "Iteration: 5770, AutoEncoder Loss: 0.003683115391\n",
      "Iteration: 5780, AutoEncoder Loss: 0.003675776417\n",
      "Iteration: 5790, AutoEncoder Loss: 0.003668423821\n",
      "Iteration: 5800, AutoEncoder Loss: 0.003663087391\n",
      "Iteration: 5810, AutoEncoder Loss: 0.003654063687\n",
      "Iteration: 5820, AutoEncoder Loss: 0.003646766281\n",
      "Iteration: 5830, AutoEncoder Loss: 0.003639530397\n",
      "Iteration: 5840, AutoEncoder Loss: 0.003632329270\n",
      "Iteration: 5850, AutoEncoder Loss: 0.003625193965\n",
      "Iteration: 5860, AutoEncoder Loss: 0.003618028074\n",
      "Iteration: 5870, AutoEncoder Loss: 0.003615565967\n",
      "Iteration: 5880, AutoEncoder Loss: 0.003604037702\n",
      "Iteration: 5890, AutoEncoder Loss: 0.003596938766\n",
      "Iteration: 5900, AutoEncoder Loss: 0.003589897318\n",
      "Iteration: 5910, AutoEncoder Loss: 0.003582887376\n",
      "Iteration: 5920, AutoEncoder Loss: 0.003575924261\n",
      "Iteration: 5930, AutoEncoder Loss: 0.003568962849\n",
      "Iteration: 5940, AutoEncoder Loss: 0.003562630363\n",
      "Iteration: 5950, AutoEncoder Loss: 0.003555338256\n",
      "Iteration: 5960, AutoEncoder Loss: 0.003548427913\n",
      "Iteration: 5970, AutoEncoder Loss: 0.003541569690\n",
      "Iteration: 5980, AutoEncoder Loss: 0.003534738835\n",
      "Iteration: 5990, AutoEncoder Loss: 0.003527955688\n",
      "Iteration: 6000, AutoEncoder Loss: 0.003521175254\n",
      "Iteration: 6010, AutoEncoder Loss: 0.003514443299\n",
      "Iteration: 6020, AutoEncoder Loss: 0.003507714670\n",
      "Iteration: 6030, AutoEncoder Loss: 0.003501247174\n",
      "Iteration: 6040, AutoEncoder Loss: 0.003494520107\n",
      "Iteration: 6050, AutoEncoder Loss: 0.003487855746\n",
      "Iteration: 6060, AutoEncoder Loss: 0.003481226131\n",
      "Iteration: 6070, AutoEncoder Loss: 0.003474620503\n",
      "Iteration: 6080, AutoEncoder Loss: 0.003468058942\n",
      "Iteration: 6090, AutoEncoder Loss: 0.003461502593\n",
      "Iteration: 6100, AutoEncoder Loss: 0.003455241764\n",
      "Iteration: 6110, AutoEncoder Loss: 0.003448645481\n",
      "Iteration: 6120, AutoEncoder Loss: 0.003442147132\n",
      "Iteration: 6130, AutoEncoder Loss: 0.003435687060\n",
      "Iteration: 6140, AutoEncoder Loss: 0.003429265669\n",
      "Iteration: 6150, AutoEncoder Loss: 0.003422853212\n",
      "Iteration: 6160, AutoEncoder Loss: 0.003416470484\n",
      "Iteration: 6170, AutoEncoder Loss: 0.003410336390\n",
      "Iteration: 6180, AutoEncoder Loss: 0.003403930640\n",
      "Iteration: 6190, AutoEncoder Loss: 0.003397597011\n",
      "Iteration: 6200, AutoEncoder Loss: 0.003391294592\n",
      "Iteration: 6210, AutoEncoder Loss: 0.003385047757\n",
      "Iteration: 6220, AutoEncoder Loss: 0.003378780708\n",
      "Iteration: 6230, AutoEncoder Loss: 0.003372592010\n",
      "Iteration: 6240, AutoEncoder Loss: 0.003366357865\n",
      "Iteration: 6250, AutoEncoder Loss: 0.003360558496\n",
      "Iteration: 6260, AutoEncoder Loss: 0.003354193472\n",
      "Iteration: 6270, AutoEncoder Loss: 0.003348027754\n",
      "Iteration: 6280, AutoEncoder Loss: 0.003341902970\n",
      "Iteration: 6290, AutoEncoder Loss: 0.003335802689\n",
      "Iteration: 6300, AutoEncoder Loss: 0.003329751234\n",
      "Iteration: 6310, AutoEncoder Loss: 0.003323684722\n",
      "Iteration: 6320, AutoEncoder Loss: 0.003318693562\n",
      "Iteration: 6330, AutoEncoder Loss: 0.003311818575\n",
      "Iteration: 6340, AutoEncoder Loss: 0.003305799255\n",
      "Iteration: 6350, AutoEncoder Loss: 0.003299824744\n",
      "Iteration: 6360, AutoEncoder Loss: 0.003293875128\n",
      "Iteration: 6370, AutoEncoder Loss: 0.003287963716\n",
      "Iteration: 6380, AutoEncoder Loss: 0.003282051737\n",
      "Iteration: 6390, AutoEncoder Loss: 0.003276786196\n",
      "Iteration: 6400, AutoEncoder Loss: 0.003270501606\n",
      "Iteration: 6410, AutoEncoder Loss: 0.003264622830\n",
      "Iteration: 6420, AutoEncoder Loss: 0.003258786923\n",
      "Iteration: 6430, AutoEncoder Loss: 0.003252978869\n",
      "Iteration: 6440, AutoEncoder Loss: 0.003247191564\n",
      "Iteration: 6450, AutoEncoder Loss: 0.003241423757\n",
      "Iteration: 6460, AutoEncoder Loss: 0.003235692825\n",
      "Iteration: 6470, AutoEncoder Loss: 0.003233618309\n",
      "Iteration: 6480, AutoEncoder Loss: 0.003224434225\n",
      "Iteration: 6490, AutoEncoder Loss: 0.003218716859\n",
      "Iteration: 6500, AutoEncoder Loss: 0.003213044829\n",
      "Iteration: 6510, AutoEncoder Loss: 0.003207396047\n",
      "Iteration: 6520, AutoEncoder Loss: 0.003201765646\n",
      "Iteration: 6530, AutoEncoder Loss: 0.003196411300\n",
      "Iteration: 6540, AutoEncoder Loss: 0.003190720906\n",
      "Iteration: 6550, AutoEncoder Loss: 0.003185132655\n",
      "Iteration: 6560, AutoEncoder Loss: 0.003179575370\n",
      "Iteration: 6570, AutoEncoder Loss: 0.003174074314\n",
      "Iteration: 6580, AutoEncoder Loss: 0.003168536278\n",
      "Iteration: 6590, AutoEncoder Loss: 0.003163072671\n",
      "Iteration: 6600, AutoEncoder Loss: 0.003157573173\n",
      "Iteration: 6610, AutoEncoder Loss: 0.003152117164\n",
      "Iteration: 6620, AutoEncoder Loss: 0.003146876845\n",
      "Iteration: 6630, AutoEncoder Loss: 0.003141396057\n",
      "Iteration: 6640, AutoEncoder Loss: 0.003135978687\n",
      "Iteration: 6650, AutoEncoder Loss: 0.003130586795\n",
      "Iteration: 6660, AutoEncoder Loss: 0.003125235979\n",
      "Iteration: 6670, AutoEncoder Loss: 0.003119876594\n",
      "Iteration: 6680, AutoEncoder Loss: 0.003114576864\n",
      "Iteration: 6690, AutoEncoder Loss: 0.003109238959\n",
      "Iteration: 6700, AutoEncoder Loss: 0.003104248796\n",
      "Iteration: 6710, AutoEncoder Loss: 0.003098807744\n",
      "Iteration: 6720, AutoEncoder Loss: 0.003093528212\n",
      "Iteration: 6730, AutoEncoder Loss: 0.003088279733\n",
      "Iteration: 6740, AutoEncoder Loss: 0.003083049669\n",
      "Iteration: 6750, AutoEncoder Loss: 0.003078762089\n",
      "Iteration: 6760, AutoEncoder Loss: 0.003072807101\n",
      "Iteration: 6770, AutoEncoder Loss: 0.003067597779\n",
      "Iteration: 6780, AutoEncoder Loss: 0.003062430882\n",
      "Iteration: 6790, AutoEncoder Loss: 0.003057283380\n",
      "Iteration: 6800, AutoEncoder Loss: 0.003052169156\n",
      "Iteration: 6810, AutoEncoder Loss: 0.003047056362\n",
      "Iteration: 6820, AutoEncoder Loss: 0.003041978624\n",
      "Iteration: 6830, AutoEncoder Loss: 0.003036896808\n",
      "Iteration: 6840, AutoEncoder Loss: 0.003032241931\n",
      "Iteration: 6850, AutoEncoder Loss: 0.003026960090\n",
      "Iteration: 6860, AutoEncoder Loss: 0.003021905704\n",
      "Iteration: 6870, AutoEncoder Loss: 0.003016887623\n",
      "Iteration: 6880, AutoEncoder Loss: 0.003011890187\n",
      "Iteration: 6890, AutoEncoder Loss: 0.003006908326\n",
      "Iteration: 6900, AutoEncoder Loss: 0.003001942839\n",
      "Iteration: 6910, AutoEncoder Loss: 0.002997008213\n",
      "Iteration: 6920, AutoEncoder Loss: 0.002992074139\n",
      "Iteration: 6930, AutoEncoder Loss: 0.002987327922\n",
      "Iteration: 6940, AutoEncoder Loss: 0.002982386318\n",
      "Iteration: 6950, AutoEncoder Loss: 0.002977492606\n",
      "Iteration: 6960, AutoEncoder Loss: 0.002972621625\n",
      "Iteration: 6970, AutoEncoder Loss: 0.002967766899\n",
      "Iteration: 6980, AutoEncoder Loss: 0.002963189940\n",
      "Iteration: 6990, AutoEncoder Loss: 0.002958257115\n",
      "Iteration: 7000, AutoEncoder Loss: 0.002953428722\n",
      "Iteration: 7010, AutoEncoder Loss: 0.002948630514\n",
      "Iteration: 7020, AutoEncoder Loss: 0.002943850833\n",
      "Iteration: 7030, AutoEncoder Loss: 0.002939087099\n",
      "Iteration: 7040, AutoEncoder Loss: 0.002934348010\n",
      "Iteration: 7050, AutoEncoder Loss: 0.002929615799\n",
      "Iteration: 7060, AutoEncoder Loss: 0.002924900650\n",
      "Iteration: 7070, AutoEncoder Loss: 0.002920372094\n",
      "Iteration: 7080, AutoEncoder Loss: 0.002915630763\n",
      "Iteration: 7090, AutoEncoder Loss: 0.002910946390\n",
      "Iteration: 7100, AutoEncoder Loss: 0.002906284207\n",
      "Iteration: 7110, AutoEncoder Loss: 0.002901656773\n",
      "Iteration: 7120, AutoEncoder Loss: 0.002897017098\n",
      "Iteration: 7130, AutoEncoder Loss: 0.002892423599\n",
      "Iteration: 7140, AutoEncoder Loss: 0.002887807958\n",
      "Iteration: 7150, AutoEncoder Loss: 0.002883697025\n",
      "Iteration: 7160, AutoEncoder Loss: 0.002878779065\n",
      "Iteration: 7170, AutoEncoder Loss: 0.002874199909\n",
      "Iteration: 7180, AutoEncoder Loss: 0.002869651638\n",
      "Iteration: 7190, AutoEncoder Loss: 0.002865118092\n",
      "Iteration: 7200, AutoEncoder Loss: 0.002860619635\n",
      "Iteration: 7210, AutoEncoder Loss: 0.002856106387\n",
      "Iteration: 7220, AutoEncoder Loss: 0.002851785056\n",
      "Iteration: 7230, AutoEncoder Loss: 0.002847260352\n",
      "Iteration: 7240, AutoEncoder Loss: 0.002842787656\n",
      "Iteration: 7250, AutoEncoder Loss: 0.002838335127\n",
      "Iteration: 7260, AutoEncoder Loss: 0.002833897112\n",
      "Iteration: 7270, AutoEncoder Loss: 0.002829493828\n",
      "Iteration: 7280, AutoEncoder Loss: 0.002825075333\n",
      "Iteration: 7290, AutoEncoder Loss: 0.002821449483\n",
      "Iteration: 7300, AutoEncoder Loss: 0.002816452856\n",
      "Iteration: 7310, AutoEncoder Loss: 0.002812055836\n",
      "Iteration: 7320, AutoEncoder Loss: 0.002807689372\n",
      "Iteration: 7330, AutoEncoder Loss: 0.002803344788\n",
      "Iteration: 7340, AutoEncoder Loss: 0.002799013208\n",
      "Iteration: 7350, AutoEncoder Loss: 0.002794694946\n",
      "Iteration: 7360, AutoEncoder Loss: 0.002790401971\n",
      "Iteration: 7370, AutoEncoder Loss: 0.002786109711\n",
      "Iteration: 7380, AutoEncoder Loss: 0.002782040549\n",
      "Iteration: 7390, AutoEncoder Loss: 0.002777681544\n",
      "Iteration: 7400, AutoEncoder Loss: 0.002773416983\n",
      "Iteration: 7410, AutoEncoder Loss: 0.002769174229\n",
      "Iteration: 7420, AutoEncoder Loss: 0.002764945779\n",
      "Iteration: 7430, AutoEncoder Loss: 0.002760742382\n",
      "Iteration: 7440, AutoEncoder Loss: 0.002757567492\n",
      "Iteration: 7450, AutoEncoder Loss: 0.002752481621\n",
      "Iteration: 7460, AutoEncoder Loss: 0.002748277277\n",
      "Iteration: 7470, AutoEncoder Loss: 0.002744106537\n",
      "Iteration: 7480, AutoEncoder Loss: 0.002739950377\n",
      "Iteration: 7490, AutoEncoder Loss: 0.002735808013\n",
      "Iteration: 7500, AutoEncoder Loss: 0.002731688858\n",
      "Iteration: 7510, AutoEncoder Loss: 0.002727571095\n",
      "Iteration: 7520, AutoEncoder Loss: 0.002723676971\n",
      "Iteration: 7530, AutoEncoder Loss: 0.002719483067\n",
      "Iteration: 7540, AutoEncoder Loss: 0.002715392185\n",
      "Iteration: 7550, AutoEncoder Loss: 0.002711320812\n",
      "Iteration: 7560, AutoEncoder Loss: 0.002707263172\n",
      "Iteration: 7570, AutoEncoder Loss: 0.002703226905\n",
      "Iteration: 7580, AutoEncoder Loss: 0.002699192547\n",
      "Iteration: 7590, AutoEncoder Loss: 0.002695369604\n",
      "Iteration: 7600, AutoEncoder Loss: 0.002691288578\n",
      "Iteration: 7610, AutoEncoder Loss: 0.002687273234\n",
      "Iteration: 7620, AutoEncoder Loss: 0.002683282506\n",
      "Iteration: 7630, AutoEncoder Loss: 0.002679304938\n",
      "Iteration: 7640, AutoEncoder Loss: 0.002675339340\n",
      "Iteration: 7650, AutoEncoder Loss: 0.002671409688\n",
      "Iteration: 7660, AutoEncoder Loss: 0.002667453940\n",
      "Iteration: 7670, AutoEncoder Loss: 0.002663958121\n",
      "Iteration: 7680, AutoEncoder Loss: 0.002659725847\n",
      "Iteration: 7690, AutoEncoder Loss: 0.002655796522\n",
      "Iteration: 7700, AutoEncoder Loss: 0.002651895347\n",
      "Iteration: 7710, AutoEncoder Loss: 0.002648007459\n",
      "Iteration: 7720, AutoEncoder Loss: 0.002644148546\n",
      "Iteration: 7730, AutoEncoder Loss: 0.002640277588\n",
      "Iteration: 7740, AutoEncoder Loss: 0.002636444291\n",
      "Iteration: 7750, AutoEncoder Loss: 0.002632676579\n",
      "Iteration: 7760, AutoEncoder Loss: 0.002628887227\n",
      "Iteration: 7770, AutoEncoder Loss: 0.002625038273\n",
      "Iteration: 7780, AutoEncoder Loss: 0.002621223827\n",
      "Iteration: 7790, AutoEncoder Loss: 0.002617424861\n",
      "Iteration: 7800, AutoEncoder Loss: 0.002613636225\n",
      "Iteration: 7810, AutoEncoder Loss: 0.002610493587\n",
      "Iteration: 7820, AutoEncoder Loss: 0.002606225826\n",
      "Iteration: 7830, AutoEncoder Loss: 0.002602445158\n",
      "Iteration: 7840, AutoEncoder Loss: 0.002598691674\n",
      "Iteration: 7850, AutoEncoder Loss: 0.002594954241\n",
      "Iteration: 7860, AutoEncoder Loss: 0.002591228859\n",
      "Iteration: 7870, AutoEncoder Loss: 0.002587514171\n",
      "Iteration: 7880, AutoEncoder Loss: 0.002583821136\n",
      "Iteration: 7890, AutoEncoder Loss: 0.002580124639\n",
      "Iteration: 7900, AutoEncoder Loss: 0.002576679177\n",
      "Iteration: 7910, AutoEncoder Loss: 0.002572875371\n",
      "Iteration: 7920, AutoEncoder Loss: 0.002569197681\n",
      "Iteration: 7930, AutoEncoder Loss: 0.002565540930\n",
      "Iteration: 7940, AutoEncoder Loss: 0.002561896990\n",
      "Iteration: 7950, AutoEncoder Loss: 0.002558272389\n",
      "Iteration: 7960, AutoEncoder Loss: 0.002554649093\n",
      "Iteration: 7970, AutoEncoder Loss: 0.002551048820\n",
      "Iteration: 7980, AutoEncoder Loss: 0.002549551368\n",
      "Iteration: 7990, AutoEncoder Loss: 0.002543962158\n",
      "Iteration: 8000, AutoEncoder Loss: 0.002540356059\n",
      "Iteration: 8010, AutoEncoder Loss: 0.002536779471\n",
      "Iteration: 8020, AutoEncoder Loss: 0.002533214615\n",
      "Iteration: 8030, AutoEncoder Loss: 0.002529660187\n",
      "Iteration: 8040, AutoEncoder Loss: 0.002526346850\n",
      "Iteration: 8050, AutoEncoder Loss: 0.002522697248\n",
      "Iteration: 8060, AutoEncoder Loss: 0.002519154803\n",
      "Iteration: 8070, AutoEncoder Loss: 0.002515635355\n",
      "Iteration: 8080, AutoEncoder Loss: 0.002512128512\n",
      "Iteration: 8090, AutoEncoder Loss: 0.002508631179\n",
      "Iteration: 8100, AutoEncoder Loss: 0.002505143704\n",
      "Iteration: 8110, AutoEncoder Loss: 0.002501674828\n",
      "Iteration: 8120, AutoEncoder Loss: 0.002498206565\n",
      "Iteration: 8130, AutoEncoder Loss: 0.002494942765\n",
      "Iteration: 8140, AutoEncoder Loss: 0.002491409247\n",
      "Iteration: 8150, AutoEncoder Loss: 0.002487953569\n",
      "Iteration: 8160, AutoEncoder Loss: 0.002484519673\n",
      "Iteration: 8170, AutoEncoder Loss: 0.002481096182\n",
      "Iteration: 8180, AutoEncoder Loss: 0.002477682715\n",
      "Iteration: 8190, AutoEncoder Loss: 0.002474278853\n",
      "Iteration: 8200, AutoEncoder Loss: 0.002470893056\n",
      "Iteration: 8210, AutoEncoder Loss: 0.002469585342\n",
      "Iteration: 8220, AutoEncoder Loss: 0.002464237540\n",
      "Iteration: 8230, AutoEncoder Loss: 0.002460848043\n",
      "Iteration: 8240, AutoEncoder Loss: 0.002457486336\n",
      "Iteration: 8250, AutoEncoder Loss: 0.002454135660\n",
      "Iteration: 8260, AutoEncoder Loss: 0.002450794653\n",
      "Iteration: 8270, AutoEncoder Loss: 0.002447471321\n",
      "Iteration: 8280, AutoEncoder Loss: 0.002444306767\n",
      "Iteration: 8290, AutoEncoder Loss: 0.002440927381\n",
      "Iteration: 8300, AutoEncoder Loss: 0.002437610620\n",
      "Iteration: 8310, AutoEncoder Loss: 0.002434311796\n",
      "Iteration: 8320, AutoEncoder Loss: 0.002431022369\n",
      "Iteration: 8330, AutoEncoder Loss: 0.002427760757\n",
      "Iteration: 8340, AutoEncoder Loss: 0.002424479177\n",
      "Iteration: 8350, AutoEncoder Loss: 0.002421217294\n",
      "Iteration: 8360, AutoEncoder Loss: 0.002418097628\n",
      "Iteration: 8370, AutoEncoder Loss: 0.002414815269\n",
      "Iteration: 8380, AutoEncoder Loss: 0.002411568177\n",
      "Iteration: 8390, AutoEncoder Loss: 0.002408337906\n",
      "Iteration: 8400, AutoEncoder Loss: 0.002405116594\n",
      "Iteration: 8410, AutoEncoder Loss: 0.002401904263\n",
      "Iteration: 8420, AutoEncoder Loss: 0.002398715804\n",
      "Iteration: 8430, AutoEncoder Loss: 0.002395513702\n",
      "Iteration: 8440, AutoEncoder Loss: 0.002393991159\n",
      "Iteration: 8450, AutoEncoder Loss: 0.002389248907\n",
      "Iteration: 8460, AutoEncoder Loss: 0.002386056163\n",
      "Iteration: 8470, AutoEncoder Loss: 0.002382891386\n",
      "Iteration: 8480, AutoEncoder Loss: 0.002379736692\n",
      "Iteration: 8490, AutoEncoder Loss: 0.002376603851\n",
      "Iteration: 8500, AutoEncoder Loss: 0.002373460532\n",
      "Iteration: 8510, AutoEncoder Loss: 0.002370503340\n",
      "Iteration: 8520, AutoEncoder Loss: 0.002367298615\n",
      "Iteration: 8530, AutoEncoder Loss: 0.002364173016\n",
      "Iteration: 8540, AutoEncoder Loss: 0.002361064051\n",
      "Iteration: 8550, AutoEncoder Loss: 0.002357965257\n",
      "Iteration: 8560, AutoEncoder Loss: 0.002354883857\n",
      "Iteration: 8570, AutoEncoder Loss: 0.002351799970\n",
      "Iteration: 8580, AutoEncoder Loss: 0.002348736629\n",
      "Iteration: 8590, AutoEncoder Loss: 0.002345997659\n",
      "Iteration: 8600, AutoEncoder Loss: 0.002342704496\n",
      "Iteration: 8610, AutoEncoder Loss: 0.002339634897\n",
      "Iteration: 8620, AutoEncoder Loss: 0.002336587895\n",
      "Iteration: 8630, AutoEncoder Loss: 0.002333551304\n",
      "Iteration: 8640, AutoEncoder Loss: 0.002330522959\n",
      "Iteration: 8650, AutoEncoder Loss: 0.002327511291\n",
      "Iteration: 8660, AutoEncoder Loss: 0.002324496721\n",
      "Iteration: 8670, AutoEncoder Loss: 0.002321737805\n",
      "Iteration: 8680, AutoEncoder Loss: 0.002318579314\n",
      "Iteration: 8690, AutoEncoder Loss: 0.002315576671\n",
      "Iteration: 8700, AutoEncoder Loss: 0.002312592674\n",
      "Iteration: 8710, AutoEncoder Loss: 0.002309617055\n",
      "Iteration: 8720, AutoEncoder Loss: 0.002306657046\n",
      "Iteration: 8730, AutoEncoder Loss: 0.002303696842\n",
      "Iteration: 8740, AutoEncoder Loss: 0.002300909978\n",
      "Iteration: 8750, AutoEncoder Loss: 0.002297892895\n",
      "Iteration: 8760, AutoEncoder Loss: 0.002294941255\n",
      "Iteration: 8770, AutoEncoder Loss: 0.002292007950\n",
      "Iteration: 8780, AutoEncoder Loss: 0.002289083080\n",
      "Iteration: 8790, AutoEncoder Loss: 0.002286166221\n",
      "Iteration: 8800, AutoEncoder Loss: 0.002283256970\n",
      "Iteration: 8810, AutoEncoder Loss: 0.002280362557\n",
      "Iteration: 8820, AutoEncoder Loss: 0.002278763569\n",
      "Iteration: 8830, AutoEncoder Loss: 0.002274679816\n",
      "Iteration: 8840, AutoEncoder Loss: 0.002271775549\n",
      "Iteration: 8850, AutoEncoder Loss: 0.002268896701\n",
      "Iteration: 8860, AutoEncoder Loss: 0.002266029464\n",
      "Iteration: 8870, AutoEncoder Loss: 0.002263176437\n",
      "Iteration: 8880, AutoEncoder Loss: 0.002260323829\n",
      "Iteration: 8890, AutoEncoder Loss: 0.002257478402\n",
      "Iteration: 8900, AutoEncoder Loss: 0.002254647285\n",
      "Iteration: 8910, AutoEncoder Loss: 0.002251942496\n",
      "Iteration: 8920, AutoEncoder Loss: 0.002249066692\n",
      "Iteration: 8930, AutoEncoder Loss: 0.002246241451\n",
      "Iteration: 8940, AutoEncoder Loss: 0.002243430129\n",
      "Iteration: 8950, AutoEncoder Loss: 0.002240625894\n",
      "Iteration: 8960, AutoEncoder Loss: 0.002237832962\n",
      "Iteration: 8970, AutoEncoder Loss: 0.002235156035\n",
      "Iteration: 8980, AutoEncoder Loss: 0.002232339357\n",
      "Iteration: 8990, AutoEncoder Loss: 0.002229553837\n",
      "Iteration: 9000, AutoEncoder Loss: 0.002226781504\n",
      "Iteration: 9010, AutoEncoder Loss: 0.002224017288\n",
      "Iteration: 9020, AutoEncoder Loss: 0.002221260320\n",
      "Iteration: 9030, AutoEncoder Loss: 0.002218526791\n",
      "Iteration: 9040, AutoEncoder Loss: 0.002215773776\n",
      "Iteration: 9050, AutoEncoder Loss: 0.002213046305\n",
      "Iteration: 9060, AutoEncoder Loss: 0.002210422580\n",
      "Iteration: 9070, AutoEncoder Loss: 0.002207666027\n",
      "Iteration: 9080, AutoEncoder Loss: 0.002204940353\n",
      "Iteration: 9090, AutoEncoder Loss: 0.002202227689\n",
      "Iteration: 9100, AutoEncoder Loss: 0.002199522616\n",
      "Iteration: 9110, AutoEncoder Loss: 0.002196824536\n",
      "Iteration: 9120, AutoEncoder Loss: 0.002194146173\n",
      "Iteration: 9130, AutoEncoder Loss: 0.002191455291\n",
      "Iteration: 9140, AutoEncoder Loss: 0.002188983033\n",
      "Iteration: 9150, AutoEncoder Loss: 0.002186180728\n",
      "Iteration: 9160, AutoEncoder Loss: 0.002183503626\n",
      "Iteration: 9170, AutoEncoder Loss: 0.002180842714\n",
      "Iteration: 9180, AutoEncoder Loss: 0.002178189060\n",
      "Iteration: 9190, AutoEncoder Loss: 0.002175553259\n",
      "Iteration: 9200, AutoEncoder Loss: 0.002172940832\n",
      "Iteration: 9210, AutoEncoder Loss: 0.002170370412\n",
      "Iteration: 9220, AutoEncoder Loss: 0.002167715266\n",
      "Iteration: 9230, AutoEncoder Loss: 0.002165087853\n",
      "Iteration: 9240, AutoEncoder Loss: 0.002162470286\n",
      "Iteration: 9250, AutoEncoder Loss: 0.002159859862\n",
      "Iteration: 9260, AutoEncoder Loss: 0.002157265103\n",
      "Iteration: 9270, AutoEncoder Loss: 0.002154665360\n",
      "Iteration: 9280, AutoEncoder Loss: 0.002152322944\n",
      "Iteration: 9290, AutoEncoder Loss: 0.002149577919\n",
      "Iteration: 9300, AutoEncoder Loss: 0.002146982456\n",
      "Iteration: 9310, AutoEncoder Loss: 0.002144404664\n",
      "Iteration: 9320, AutoEncoder Loss: 0.002141836292\n",
      "Iteration: 9330, AutoEncoder Loss: 0.002139274413\n",
      "Iteration: 9340, AutoEncoder Loss: 0.002136718865\n",
      "Iteration: 9350, AutoEncoder Loss: 0.002134177816\n",
      "Iteration: 9360, AutoEncoder Loss: 0.002131632439\n",
      "Iteration: 9370, AutoEncoder Loss: 0.002129324102\n",
      "Iteration: 9380, AutoEncoder Loss: 0.002126645574\n",
      "Iteration: 9390, AutoEncoder Loss: 0.002124106613\n",
      "Iteration: 9400, AutoEncoder Loss: 0.002121583624\n",
      "Iteration: 9410, AutoEncoder Loss: 0.002119068797\n",
      "Iteration: 9420, AutoEncoder Loss: 0.002116560296\n",
      "Iteration: 9430, AutoEncoder Loss: 0.002114057744\n",
      "Iteration: 9440, AutoEncoder Loss: 0.002111699875\n",
      "Iteration: 9450, AutoEncoder Loss: 0.002109141777\n",
      "Iteration: 9460, AutoEncoder Loss: 0.002106646602\n",
      "Iteration: 9470, AutoEncoder Loss: 0.002104164434\n",
      "Iteration: 9480, AutoEncoder Loss: 0.002101689834\n",
      "Iteration: 9490, AutoEncoder Loss: 0.002099227012\n",
      "Iteration: 9500, AutoEncoder Loss: 0.002096763989\n",
      "Iteration: 9510, AutoEncoder Loss: 0.002094314035\n",
      "Iteration: 9520, AutoEncoder Loss: 0.002092900672\n",
      "Iteration: 9530, AutoEncoder Loss: 0.002089500854\n",
      "Iteration: 9540, AutoEncoder Loss: 0.002087039047\n",
      "Iteration: 9550, AutoEncoder Loss: 0.002084600128\n",
      "Iteration: 9560, AutoEncoder Loss: 0.002082169302\n",
      "Iteration: 9570, AutoEncoder Loss: 0.002079745079\n",
      "Iteration: 9580, AutoEncoder Loss: 0.002077332677\n",
      "Iteration: 9590, AutoEncoder Loss: 0.002074919926\n",
      "Iteration: 9600, AutoEncoder Loss: 0.002072518813\n",
      "Iteration: 9610, AutoEncoder Loss: 0.002070224567\n",
      "Iteration: 9620, AutoEncoder Loss: 0.002067783808\n",
      "Iteration: 9630, AutoEncoder Loss: 0.002065385899\n",
      "Iteration: 9640, AutoEncoder Loss: 0.002062998936\n",
      "Iteration: 9650, AutoEncoder Loss: 0.002060618449\n",
      "Iteration: 9660, AutoEncoder Loss: 0.002058243414\n",
      "Iteration: 9670, AutoEncoder Loss: 0.002055982926\n",
      "Iteration: 9680, AutoEncoder Loss: 0.002053581608\n",
      "Iteration: 9690, AutoEncoder Loss: 0.002051214463\n",
      "Iteration: 9700, AutoEncoder Loss: 0.002048858372\n",
      "Iteration: 9710, AutoEncoder Loss: 0.002046509232\n",
      "Iteration: 9720, AutoEncoder Loss: 0.002044165567\n",
      "Iteration: 9730, AutoEncoder Loss: 0.002041828566\n",
      "Iteration: 9740, AutoEncoder Loss: 0.002039500237\n",
      "Iteration: 9750, AutoEncoder Loss: 0.002037172724\n",
      "Iteration: 9760, AutoEncoder Loss: 0.002034957482\n",
      "Iteration: 9770, AutoEncoder Loss: 0.002032603367\n",
      "Iteration: 9780, AutoEncoder Loss: 0.002030283253\n",
      "Iteration: 9790, AutoEncoder Loss: 0.002027974664\n",
      "Iteration: 9800, AutoEncoder Loss: 0.002025671845\n",
      "Iteration: 9810, AutoEncoder Loss: 0.002023374603\n",
      "Iteration: 9820, AutoEncoder Loss: 0.002021090391\n",
      "Iteration: 9830, AutoEncoder Loss: 0.002018801749\n",
      "Iteration: 9840, AutoEncoder Loss: 0.002017005796\n",
      "Iteration: 9850, AutoEncoder Loss: 0.002014313160\n",
      "Iteration: 9860, AutoEncoder Loss: 0.002012026741\n",
      "Iteration: 9870, AutoEncoder Loss: 0.002009757468\n",
      "Iteration: 9880, AutoEncoder Loss: 0.002007495150\n",
      "Iteration: 9890, AutoEncoder Loss: 0.002005251547\n",
      "Iteration: 9900, AutoEncoder Loss: 0.002002992044\n",
      "Iteration: 9910, AutoEncoder Loss: 0.002000855467\n",
      "Iteration: 9920, AutoEncoder Loss: 0.001998566584\n",
      "Iteration: 9930, AutoEncoder Loss: 0.001996321084\n",
      "Iteration: 9940, AutoEncoder Loss: 0.001994086620\n",
      "Iteration: 9950, AutoEncoder Loss: 0.001991858528\n",
      "Iteration: 9960, AutoEncoder Loss: 0.001989643954\n",
      "Iteration: 9970, AutoEncoder Loss: 0.001987423291\n",
      "Iteration: 9980, AutoEncoder Loss: 0.001985219692\n",
      "Iteration: 9990, AutoEncoder Loss: 0.001983236221\n",
      "Iteration: 10000, AutoEncoder Loss: 0.001980873848\n",
      "Iteration: 10010, AutoEncoder Loss: 0.001978661235\n",
      "Iteration: 10020, AutoEncoder Loss: 0.001976464589\n",
      "Iteration: 10030, AutoEncoder Loss: 0.001974274425\n",
      "Iteration: 10040, AutoEncoder Loss: 0.001972089534\n",
      "Iteration: 10050, AutoEncoder Loss: 0.001969918573\n",
      "Iteration: 10060, AutoEncoder Loss: 0.001967739853\n",
      "Iteration: 10070, AutoEncoder Loss: 0.001966691436\n",
      "Iteration: 10080, AutoEncoder Loss: 0.001963479707\n",
      "Iteration: 10090, AutoEncoder Loss: 0.001961302255\n",
      "Iteration: 10100, AutoEncoder Loss: 0.001959141792\n",
      "Iteration: 10110, AutoEncoder Loss: 0.001956989898\n",
      "Iteration: 10120, AutoEncoder Loss: 0.001954842551\n",
      "Iteration: 10130, AutoEncoder Loss: 0.001952699961\n",
      "Iteration: 10140, AutoEncoder Loss: 0.001950765434\n",
      "Iteration: 10150, AutoEncoder Loss: 0.001948502602\n",
      "Iteration: 10160, AutoEncoder Loss: 0.001946359473\n",
      "Iteration: 10170, AutoEncoder Loss: 0.001944231214\n",
      "Iteration: 10180, AutoEncoder Loss: 0.001942110611\n",
      "Iteration: 10190, AutoEncoder Loss: 0.001939994771\n",
      "Iteration: 10200, AutoEncoder Loss: 0.001937883596\n",
      "Iteration: 10210, AutoEncoder Loss: 0.001935784965\n",
      "Iteration: 10220, AutoEncoder Loss: 0.001933680109\n",
      "Iteration: 10230, AutoEncoder Loss: 0.001931700085\n",
      "Iteration: 10240, AutoEncoder Loss: 0.001929549897\n",
      "Iteration: 10250, AutoEncoder Loss: 0.001927452515\n",
      "Iteration: 10260, AutoEncoder Loss: 0.001925366049\n",
      "Iteration: 10270, AutoEncoder Loss: 0.001923285561\n",
      "Iteration: 10280, AutoEncoder Loss: 0.001921215857\n",
      "Iteration: 10290, AutoEncoder Loss: 0.001919143485\n",
      "Iteration: 10300, AutoEncoder Loss: 0.001917083603\n",
      "Iteration: 10310, AutoEncoder Loss: 0.001916184205\n",
      "Iteration: 10320, AutoEncoder Loss: 0.001913028415\n",
      "Iteration: 10330, AutoEncoder Loss: 0.001910958136\n",
      "Iteration: 10340, AutoEncoder Loss: 0.001908905468\n",
      "Iteration: 10350, AutoEncoder Loss: 0.001906859417\n",
      "Iteration: 10360, AutoEncoder Loss: 0.001904818280\n",
      "Iteration: 10370, AutoEncoder Loss: 0.001902914584\n",
      "Iteration: 10380, AutoEncoder Loss: 0.001900815010\n",
      "Iteration: 10390, AutoEncoder Loss: 0.001898776085\n",
      "Iteration: 10400, AutoEncoder Loss: 0.001896749690\n",
      "Iteration: 10410, AutoEncoder Loss: 0.001894728771\n",
      "Iteration: 10420, AutoEncoder Loss: 0.001892712673\n",
      "Iteration: 10430, AutoEncoder Loss: 0.001890701033\n",
      "Iteration: 10440, AutoEncoder Loss: 0.001888698760\n",
      "Iteration: 10450, AutoEncoder Loss: 0.001886695490\n",
      "Iteration: 10460, AutoEncoder Loss: 0.001884810741\n",
      "Iteration: 10470, AutoEncoder Loss: 0.001882764499\n",
      "Iteration: 10480, AutoEncoder Loss: 0.001880764568\n",
      "Iteration: 10490, AutoEncoder Loss: 0.001878775721\n",
      "Iteration: 10500, AutoEncoder Loss: 0.001876792436\n",
      "Iteration: 10510, AutoEncoder Loss: 0.001874813487\n",
      "Iteration: 10520, AutoEncoder Loss: 0.001872838758\n",
      "Iteration: 10530, AutoEncoder Loss: 0.001870873259\n",
      "Iteration: 10540, AutoEncoder Loss: 0.001868907008\n",
      "Iteration: 10550, AutoEncoder Loss: 0.001867049516\n",
      "Iteration: 10560, AutoEncoder Loss: 0.001865045006\n",
      "Iteration: 10570, AutoEncoder Loss: 0.001863082879\n",
      "Iteration: 10580, AutoEncoder Loss: 0.001861131378\n",
      "Iteration: 10590, AutoEncoder Loss: 0.001859184579\n",
      "Iteration: 10600, AutoEncoder Loss: 0.001857242101\n",
      "Iteration: 10610, AutoEncoder Loss: 0.001855303686\n",
      "Iteration: 10620, AutoEncoder Loss: 0.001853447688\n",
      "Iteration: 10630, AutoEncoder Loss: 0.001851489952\n",
      "Iteration: 10640, AutoEncoder Loss: 0.001849558503\n",
      "Iteration: 10650, AutoEncoder Loss: 0.001847634141\n",
      "Iteration: 10660, AutoEncoder Loss: 0.001845725480\n",
      "Iteration: 10670, AutoEncoder Loss: 0.001843804233\n",
      "Iteration: 10680, AutoEncoder Loss: 0.001841897139\n",
      "Iteration: 10690, AutoEncoder Loss: 0.001839990266\n",
      "Iteration: 10700, AutoEncoder Loss: 0.001838242167\n",
      "Iteration: 10710, AutoEncoder Loss: 0.001836245504\n",
      "Iteration: 10720, AutoEncoder Loss: 0.001834338563\n",
      "Iteration: 10730, AutoEncoder Loss: 0.001832444249\n",
      "Iteration: 10740, AutoEncoder Loss: 0.001830555367\n",
      "Iteration: 10750, AutoEncoder Loss: 0.001828681434\n",
      "Iteration: 10760, AutoEncoder Loss: 0.001826794361\n",
      "Iteration: 10770, AutoEncoder Loss: 0.001824921376\n",
      "Iteration: 10780, AutoEncoder Loss: 0.001823049208\n",
      "Iteration: 10790, AutoEncoder Loss: 0.001821256479\n",
      "Iteration: 10800, AutoEncoder Loss: 0.001819361749\n",
      "Iteration: 10810, AutoEncoder Loss: 0.001817495269\n",
      "Iteration: 10820, AutoEncoder Loss: 0.001815636491\n",
      "Iteration: 10830, AutoEncoder Loss: 0.001813781797\n",
      "Iteration: 10840, AutoEncoder Loss: 0.001811939524\n",
      "Iteration: 10850, AutoEncoder Loss: 0.001810163436\n",
      "Iteration: 10860, AutoEncoder Loss: 0.001808295288\n",
      "Iteration: 10870, AutoEncoder Loss: 0.001806449441\n",
      "Iteration: 10880, AutoEncoder Loss: 0.001804612189\n",
      "Iteration: 10890, AutoEncoder Loss: 0.001802779019\n",
      "Iteration: 10900, AutoEncoder Loss: 0.001800949882\n",
      "Iteration: 10910, AutoEncoder Loss: 0.001799133288\n",
      "Iteration: 10920, AutoEncoder Loss: 0.001797306935\n",
      "Iteration: 10930, AutoEncoder Loss: 0.001795729941\n",
      "Iteration: 10940, AutoEncoder Loss: 0.001793747872\n",
      "Iteration: 10950, AutoEncoder Loss: 0.001791917232\n",
      "Iteration: 10960, AutoEncoder Loss: 0.001790104194\n",
      "Iteration: 10970, AutoEncoder Loss: 0.001788299426\n",
      "Iteration: 10980, AutoEncoder Loss: 0.001786498561\n",
      "Iteration: 10990, AutoEncoder Loss: 0.001784701606\n",
      "Iteration: 11000, AutoEncoder Loss: 0.001782917042\n",
      "Iteration: 11010, AutoEncoder Loss: 0.001781122764\n",
      "Iteration: 11020, AutoEncoder Loss: 0.001779690399\n",
      "Iteration: 11030, AutoEncoder Loss: 0.001777621022\n",
      "Iteration: 11040, AutoEncoder Loss: 0.001775824228\n",
      "Iteration: 11050, AutoEncoder Loss: 0.001774044606\n",
      "Iteration: 11060, AutoEncoder Loss: 0.001772271449\n",
      "Iteration: 11070, AutoEncoder Loss: 0.001770502496\n",
      "Iteration: 11080, AutoEncoder Loss: 0.001768736985\n",
      "Iteration: 11090, AutoEncoder Loss: 0.001767119197\n",
      "Iteration: 11100, AutoEncoder Loss: 0.001765270700\n",
      "Iteration: 11110, AutoEncoder Loss: 0.001763505012\n",
      "Iteration: 11120, AutoEncoder Loss: 0.001761751073\n",
      "Iteration: 11130, AutoEncoder Loss: 0.001760001923\n",
      "Iteration: 11140, AutoEncoder Loss: 0.001758261780\n",
      "Iteration: 11150, AutoEncoder Loss: 0.001756519156\n",
      "Iteration: 11160, AutoEncoder Loss: 0.001754787744\n",
      "Iteration: 11170, AutoEncoder Loss: 0.001753050119\n",
      "Iteration: 11180, AutoEncoder Loss: 0.001751396485\n",
      "Iteration: 11190, AutoEncoder Loss: 0.001749638004\n",
      "Iteration: 11200, AutoEncoder Loss: 0.001747907453\n",
      "Iteration: 11210, AutoEncoder Loss: 0.001746184153\n",
      "Iteration: 11220, AutoEncoder Loss: 0.001744465367\n",
      "Iteration: 11230, AutoEncoder Loss: 0.001742755188\n",
      "Iteration: 11240, AutoEncoder Loss: 0.001741042499\n",
      "Iteration: 11250, AutoEncoder Loss: 0.001739339670\n",
      "Iteration: 11260, AutoEncoder Loss: 0.001738354424\n",
      "Iteration: 11270, AutoEncoder Loss: 0.001735987330\n",
      "Iteration: 11280, AutoEncoder Loss: 0.001734273932\n",
      "Iteration: 11290, AutoEncoder Loss: 0.001732575820\n",
      "Iteration: 11300, AutoEncoder Loss: 0.001730883314\n",
      "Iteration: 11310, AutoEncoder Loss: 0.001729194229\n",
      "Iteration: 11320, AutoEncoder Loss: 0.001727635208\n",
      "Iteration: 11330, AutoEncoder Loss: 0.001725882764\n",
      "Iteration: 11340, AutoEncoder Loss: 0.001724192855\n",
      "Iteration: 11350, AutoEncoder Loss: 0.001722513628\n",
      "Iteration: 11360, AutoEncoder Loss: 0.001720839964\n",
      "Iteration: 11370, AutoEncoder Loss: 0.001719169709\n",
      "Iteration: 11380, AutoEncoder Loss: 0.001717502720\n",
      "Iteration: 11390, AutoEncoder Loss: 0.001715843997\n",
      "Iteration: 11400, AutoEncoder Loss: 0.001714182932\n",
      "Iteration: 11410, AutoEncoder Loss: 0.001712650366\n",
      "Iteration: 11420, AutoEncoder Loss: 0.001710927524\n",
      "Iteration: 11430, AutoEncoder Loss: 0.001709266162\n",
      "Iteration: 11440, AutoEncoder Loss: 0.001707615489\n",
      "Iteration: 11450, AutoEncoder Loss: 0.001705969831\n",
      "Iteration: 11460, AutoEncoder Loss: 0.001704327700\n",
      "Iteration: 11470, AutoEncoder Loss: 0.001702688771\n",
      "Iteration: 11480, AutoEncoder Loss: 0.001701057227\n",
      "Iteration: 11490, AutoEncoder Loss: 0.001699424857\n",
      "Iteration: 11500, AutoEncoder Loss: 0.001697893113\n",
      "Iteration: 11510, AutoEncoder Loss: 0.001696218821\n",
      "Iteration: 11520, AutoEncoder Loss: 0.001694587597\n",
      "Iteration: 11530, AutoEncoder Loss: 0.001692965907\n",
      "Iteration: 11540, AutoEncoder Loss: 0.001691348424\n",
      "Iteration: 11550, AutoEncoder Loss: 0.001689734031\n",
      "Iteration: 11560, AutoEncoder Loss: 0.001688122611\n",
      "Iteration: 11570, AutoEncoder Loss: 0.001686606782\n",
      "Iteration: 11580, AutoEncoder Loss: 0.001684960519\n",
      "Iteration: 11590, AutoEncoder Loss: 0.001683350050\n",
      "Iteration: 11600, AutoEncoder Loss: 0.001681748795\n",
      "Iteration: 11610, AutoEncoder Loss: 0.001680151542\n",
      "Iteration: 11620, AutoEncoder Loss: 0.001678557763\n",
      "Iteration: 11630, AutoEncoder Loss: 0.001676967147\n",
      "Iteration: 11640, AutoEncoder Loss: 0.001675383496\n",
      "Iteration: 11650, AutoEncoder Loss: 0.001673808499\n",
      "Iteration: 11660, AutoEncoder Loss: 0.001672277401\n",
      "Iteration: 11670, AutoEncoder Loss: 0.001670679874\n",
      "Iteration: 11680, AutoEncoder Loss: 0.001669099932\n",
      "Iteration: 11690, AutoEncoder Loss: 0.001667525649\n",
      "Iteration: 11700, AutoEncoder Loss: 0.001665955054\n",
      "Iteration: 11710, AutoEncoder Loss: 0.001664391312\n",
      "Iteration: 11720, AutoEncoder Loss: 0.001662826953\n",
      "Iteration: 11730, AutoEncoder Loss: 0.001661269363\n",
      "Iteration: 11740, AutoEncoder Loss: 0.001659861728\n",
      "Iteration: 11750, AutoEncoder Loss: 0.001658205102\n",
      "Iteration: 11760, AutoEncoder Loss: 0.001656640953\n",
      "Iteration: 11770, AutoEncoder Loss: 0.001655088247\n",
      "Iteration: 11780, AutoEncoder Loss: 0.001653539987\n",
      "Iteration: 11790, AutoEncoder Loss: 0.001651995248\n",
      "Iteration: 11800, AutoEncoder Loss: 0.001650457086\n",
      "Iteration: 11810, AutoEncoder Loss: 0.001648918281\n",
      "Iteration: 11820, AutoEncoder Loss: 0.001647385957\n",
      "Iteration: 11830, AutoEncoder Loss: 0.001645923225\n",
      "Iteration: 11840, AutoEncoder Loss: 0.001644361888\n",
      "Iteration: 11850, AutoEncoder Loss: 0.001642829481\n",
      "Iteration: 11860, AutoEncoder Loss: 0.001641303603\n",
      "Iteration: 11870, AutoEncoder Loss: 0.001639781296\n",
      "Iteration: 11880, AutoEncoder Loss: 0.001638261726\n",
      "Iteration: 11890, AutoEncoder Loss: 0.001636816290\n",
      "Iteration: 11900, AutoEncoder Loss: 0.001635277295\n",
      "Iteration: 11910, AutoEncoder Loss: 0.001633759767\n",
      "Iteration: 11920, AutoEncoder Loss: 0.001632249940\n",
      "Iteration: 11930, AutoEncoder Loss: 0.001630743464\n",
      "Iteration: 11940, AutoEncoder Loss: 0.001629240127\n",
      "Iteration: 11950, AutoEncoder Loss: 0.001627746220\n",
      "Iteration: 11960, AutoEncoder Loss: 0.001626245628\n",
      "Iteration: 11970, AutoEncoder Loss: 0.001624750372\n",
      "Iteration: 11980, AutoEncoder Loss: 0.001623328300\n",
      "Iteration: 11990, AutoEncoder Loss: 0.001621813958\n",
      "Iteration: 12000, AutoEncoder Loss: 0.001620321141\n",
      "Iteration: 12010, AutoEncoder Loss: 0.001618835510\n",
      "Iteration: 12020, AutoEncoder Loss: 0.001617353212\n",
      "Iteration: 12030, AutoEncoder Loss: 0.001615873931\n",
      "Iteration: 12040, AutoEncoder Loss: 0.001614404363\n",
      "Iteration: 12050, AutoEncoder Loss: 0.001612926919\n",
      "Iteration: 12060, AutoEncoder Loss: 0.001611457860\n",
      "Iteration: 12070, AutoEncoder Loss: 0.001610046808\n",
      "Iteration: 12080, AutoEncoder Loss: 0.001608562441\n",
      "Iteration: 12090, AutoEncoder Loss: 0.001607094938\n",
      "Iteration: 12100, AutoEncoder Loss: 0.001605633761\n",
      "Iteration: 12110, AutoEncoder Loss: 0.001604175403\n",
      "Iteration: 12120, AutoEncoder Loss: 0.001602719859\n",
      "Iteration: 12130, AutoEncoder Loss: 0.001601551138\n",
      "Iteration: 12140, AutoEncoder Loss: 0.001599876128\n",
      "Iteration: 12150, AutoEncoder Loss: 0.001598411548\n",
      "Iteration: 12160, AutoEncoder Loss: 0.001596961481\n",
      "Iteration: 12170, AutoEncoder Loss: 0.001595517468\n",
      "Iteration: 12180, AutoEncoder Loss: 0.001594076760\n",
      "Iteration: 12190, AutoEncoder Loss: 0.001592638903\n",
      "Iteration: 12200, AutoEncoder Loss: 0.001591209757\n",
      "Iteration: 12210, AutoEncoder Loss: 0.001589774666\n",
      "Iteration: 12220, AutoEncoder Loss: 0.001588652615\n",
      "Iteration: 12230, AutoEncoder Loss: 0.001586965648\n",
      "Iteration: 12240, AutoEncoder Loss: 0.001585527989\n",
      "Iteration: 12250, AutoEncoder Loss: 0.001584102842\n",
      "Iteration: 12260, AutoEncoder Loss: 0.001582681663\n",
      "Iteration: 12270, AutoEncoder Loss: 0.001581269853\n",
      "Iteration: 12280, AutoEncoder Loss: 0.001579851860\n",
      "Iteration: 12290, AutoEncoder Loss: 0.001578445972\n",
      "Iteration: 12300, AutoEncoder Loss: 0.001577032141\n",
      "Iteration: 12310, AutoEncoder Loss: 0.001575710759\n",
      "Iteration: 12320, AutoEncoder Loss: 0.001574258304\n",
      "Iteration: 12330, AutoEncoder Loss: 0.001572848655\n",
      "Iteration: 12340, AutoEncoder Loss: 0.001571447085\n",
      "Iteration: 12350, AutoEncoder Loss: 0.001570048793\n",
      "Iteration: 12360, AutoEncoder Loss: 0.001568658348\n",
      "Iteration: 12370, AutoEncoder Loss: 0.001567263495\n",
      "Iteration: 12380, AutoEncoder Loss: 0.001565930341\n",
      "Iteration: 12390, AutoEncoder Loss: 0.001564521618\n",
      "Iteration: 12400, AutoEncoder Loss: 0.001563131530\n",
      "Iteration: 12410, AutoEncoder Loss: 0.001561746924\n",
      "Iteration: 12420, AutoEncoder Loss: 0.001560365332\n",
      "Iteration: 12430, AutoEncoder Loss: 0.001558991436\n",
      "Iteration: 12440, AutoEncoder Loss: 0.001557612970\n",
      "Iteration: 12450, AutoEncoder Loss: 0.001556244757\n",
      "Iteration: 12460, AutoEncoder Loss: 0.001555343821\n",
      "Iteration: 12470, AutoEncoder Loss: 0.001553551015\n",
      "Iteration: 12480, AutoEncoder Loss: 0.001552170013\n",
      "Iteration: 12490, AutoEncoder Loss: 0.001550802201\n",
      "Iteration: 12500, AutoEncoder Loss: 0.001549438812\n",
      "Iteration: 12510, AutoEncoder Loss: 0.001548078444\n",
      "Iteration: 12520, AutoEncoder Loss: 0.001546725349\n",
      "Iteration: 12530, AutoEncoder Loss: 0.001545368736\n",
      "Iteration: 12540, AutoEncoder Loss: 0.001544183356\n",
      "Iteration: 12550, AutoEncoder Loss: 0.001542710840\n",
      "Iteration: 12560, AutoEncoder Loss: 0.001541352310\n",
      "Iteration: 12570, AutoEncoder Loss: 0.001540003870\n",
      "Iteration: 12580, AutoEncoder Loss: 0.001538659495\n",
      "Iteration: 12590, AutoEncoder Loss: 0.001537317881\n",
      "Iteration: 12600, AutoEncoder Loss: 0.001535978686\n",
      "Iteration: 12610, AutoEncoder Loss: 0.001534760116\n",
      "Iteration: 12620, AutoEncoder Loss: 0.001533354121\n",
      "Iteration: 12630, AutoEncoder Loss: 0.001532011680\n",
      "Iteration: 12640, AutoEncoder Loss: 0.001530679002\n",
      "Iteration: 12650, AutoEncoder Loss: 0.001529350392\n",
      "Iteration: 12660, AutoEncoder Loss: 0.001528024394\n",
      "Iteration: 12670, AutoEncoder Loss: 0.001526700700\n",
      "Iteration: 12680, AutoEncoder Loss: 0.001525383344\n",
      "Iteration: 12690, AutoEncoder Loss: 0.001524063937\n",
      "Iteration: 12700, AutoEncoder Loss: 0.001522862517\n",
      "Iteration: 12710, AutoEncoder Loss: 0.001521479241\n",
      "Iteration: 12720, AutoEncoder Loss: 0.001520157070\n",
      "Iteration: 12730, AutoEncoder Loss: 0.001518844333\n",
      "Iteration: 12740, AutoEncoder Loss: 0.001517535684\n",
      "Iteration: 12750, AutoEncoder Loss: 0.001516229647\n",
      "Iteration: 12760, AutoEncoder Loss: 0.001514926050\n",
      "Iteration: 12770, AutoEncoder Loss: 0.001513628735\n",
      "Iteration: 12780, AutoEncoder Loss: 0.001512328514\n",
      "Iteration: 12790, AutoEncoder Loss: 0.001511138137\n",
      "Iteration: 12800, AutoEncoder Loss: 0.001509779528\n",
      "Iteration: 12810, AutoEncoder Loss: 0.001508478327\n",
      "Iteration: 12820, AutoEncoder Loss: 0.001507185900\n",
      "Iteration: 12830, AutoEncoder Loss: 0.001505897341\n",
      "Iteration: 12840, AutoEncoder Loss: 0.001504611111\n",
      "Iteration: 12850, AutoEncoder Loss: 0.001503327109\n",
      "Iteration: 12860, AutoEncoder Loss: 0.001502126356\n",
      "Iteration: 12870, AutoEncoder Loss: 0.001500807774\n",
      "Iteration: 12880, AutoEncoder Loss: 0.001499522501\n",
      "Iteration: 12890, AutoEncoder Loss: 0.001498245236\n",
      "Iteration: 12900, AutoEncoder Loss: 0.001496971139\n",
      "Iteration: 12910, AutoEncoder Loss: 0.001495699595\n",
      "Iteration: 12920, AutoEncoder Loss: 0.001494430362\n",
      "Iteration: 12930, AutoEncoder Loss: 0.001493166304\n",
      "Iteration: 12940, AutoEncoder Loss: 0.001491901194\n",
      "Iteration: 12950, AutoEncoder Loss: 0.001490701712\n",
      "Iteration: 12960, AutoEncoder Loss: 0.001489413728\n",
      "Iteration: 12970, AutoEncoder Loss: 0.001488149445\n",
      "Iteration: 12980, AutoEncoder Loss: 0.001486891387\n",
      "Iteration: 12990, AutoEncoder Loss: 0.001485636411\n",
      "Iteration: 13000, AutoEncoder Loss: 0.001484386707\n",
      "Iteration: 13010, AutoEncoder Loss: 0.001483135910\n",
      "Iteration: 13020, AutoEncoder Loss: 0.001481891094\n",
      "Iteration: 13030, AutoEncoder Loss: 0.001480658136\n",
      "Iteration: 13040, AutoEncoder Loss: 0.001479444558\n",
      "Iteration: 13050, AutoEncoder Loss: 0.001478188350\n",
      "Iteration: 13060, AutoEncoder Loss: 0.001476945381\n",
      "Iteration: 13070, AutoEncoder Loss: 0.001475706687\n",
      "Iteration: 13080, AutoEncoder Loss: 0.001474470434\n",
      "Iteration: 13090, AutoEncoder Loss: 0.001473239247\n",
      "Iteration: 13100, AutoEncoder Loss: 0.001472103995\n",
      "Iteration: 13110, AutoEncoder Loss: 0.001470815514\n",
      "Iteration: 13120, AutoEncoder Loss: 0.001469579187\n",
      "Iteration: 13130, AutoEncoder Loss: 0.001468351274\n",
      "Iteration: 13140, AutoEncoder Loss: 0.001467126679\n",
      "Iteration: 13150, AutoEncoder Loss: 0.001465904237\n",
      "Iteration: 13160, AutoEncoder Loss: 0.001464686806\n",
      "Iteration: 13170, AutoEncoder Loss: 0.001463468561\n",
      "Iteration: 13180, AutoEncoder Loss: 0.001462255431\n",
      "Iteration: 13190, AutoEncoder Loss: 0.001461121983\n",
      "Iteration: 13200, AutoEncoder Loss: 0.001459865513\n",
      "Iteration: 13210, AutoEncoder Loss: 0.001458648439\n",
      "Iteration: 13220, AutoEncoder Loss: 0.001457438501\n",
      "Iteration: 13230, AutoEncoder Loss: 0.001456231440\n",
      "Iteration: 13240, AutoEncoder Loss: 0.001455026766\n",
      "Iteration: 13250, AutoEncoder Loss: 0.001453827109\n",
      "Iteration: 13260, AutoEncoder Loss: 0.001452626402\n",
      "Iteration: 13270, AutoEncoder Loss: 0.001451495556\n",
      "Iteration: 13280, AutoEncoder Loss: 0.001450269358\n",
      "Iteration: 13290, AutoEncoder Loss: 0.001449068311\n",
      "Iteration: 13300, AutoEncoder Loss: 0.001447873918\n",
      "Iteration: 13310, AutoEncoder Loss: 0.001446682545\n",
      "Iteration: 13320, AutoEncoder Loss: 0.001445493211\n",
      "Iteration: 13330, AutoEncoder Loss: 0.001444305848\n",
      "Iteration: 13340, AutoEncoder Loss: 0.001443123542\n",
      "Iteration: 13350, AutoEncoder Loss: 0.001441940383\n",
      "Iteration: 13360, AutoEncoder Loss: 0.001440820809\n",
      "Iteration: 13370, AutoEncoder Loss: 0.001439614615\n",
      "Iteration: 13380, AutoEncoder Loss: 0.001438431577\n",
      "Iteration: 13390, AutoEncoder Loss: 0.001437254727\n",
      "Iteration: 13400, AutoEncoder Loss: 0.001436080518\n",
      "Iteration: 13410, AutoEncoder Loss: 0.001434908496\n",
      "Iteration: 13420, AutoEncoder Loss: 0.001433738444\n",
      "Iteration: 13430, AutoEncoder Loss: 0.001432630445\n",
      "Iteration: 13440, AutoEncoder Loss: 0.001431440154\n",
      "Iteration: 13450, AutoEncoder Loss: 0.001430270122\n",
      "Iteration: 13460, AutoEncoder Loss: 0.001429106112\n",
      "Iteration: 13470, AutoEncoder Loss: 0.001427944659\n",
      "Iteration: 13480, AutoEncoder Loss: 0.001426785427\n",
      "Iteration: 13490, AutoEncoder Loss: 0.001425629168\n",
      "Iteration: 13500, AutoEncoder Loss: 0.001424475666\n",
      "Iteration: 13510, AutoEncoder Loss: 0.001423321927\n",
      "Iteration: 13520, AutoEncoder Loss: 0.001422229755\n",
      "Iteration: 13530, AutoEncoder Loss: 0.001421056506\n",
      "Iteration: 13540, AutoEncoder Loss: 0.001419902899\n",
      "Iteration: 13550, AutoEncoder Loss: 0.001418755252\n",
      "Iteration: 13560, AutoEncoder Loss: 0.001417610357\n",
      "Iteration: 13570, AutoEncoder Loss: 0.001416467392\n",
      "Iteration: 13580, AutoEncoder Loss: 0.001415326438\n",
      "Iteration: 13590, AutoEncoder Loss: 0.001414190146\n",
      "Iteration: 13600, AutoEncoder Loss: 0.001413052860\n",
      "Iteration: 13610, AutoEncoder Loss: 0.001411971805\n",
      "Iteration: 13620, AutoEncoder Loss: 0.001410817046\n",
      "Iteration: 13630, AutoEncoder Loss: 0.001409680390\n",
      "Iteration: 13640, AutoEncoder Loss: 0.001408549246\n",
      "Iteration: 13650, AutoEncoder Loss: 0.001407420552\n",
      "Iteration: 13660, AutoEncoder Loss: 0.001406293905\n",
      "Iteration: 13670, AutoEncoder Loss: 0.001405169458\n",
      "Iteration: 13680, AutoEncoder Loss: 0.001404095608\n",
      "Iteration: 13690, AutoEncoder Loss: 0.001402958104\n",
      "Iteration: 13700, AutoEncoder Loss: 0.001401834318\n",
      "Iteration: 13710, AutoEncoder Loss: 0.001400715500\n",
      "Iteration: 13720, AutoEncoder Loss: 0.001399598890\n",
      "Iteration: 13730, AutoEncoder Loss: 0.001398484323\n",
      "Iteration: 13740, AutoEncoder Loss: 0.001397371606\n",
      "Iteration: 13750, AutoEncoder Loss: 0.001396263198\n",
      "Iteration: 13760, AutoEncoder Loss: 0.001395154014\n",
      "Iteration: 13770, AutoEncoder Loss: 0.001394092974\n",
      "Iteration: 13780, AutoEncoder Loss: 0.001392970848\n",
      "Iteration: 13790, AutoEncoder Loss: 0.001391862978\n",
      "Iteration: 13800, AutoEncoder Loss: 0.001390759475\n",
      "Iteration: 13810, AutoEncoder Loss: 0.001389664336\n",
      "Iteration: 13820, AutoEncoder Loss: 0.001388561909\n",
      "Iteration: 13830, AutoEncoder Loss: 0.001387466866\n",
      "Iteration: 13840, AutoEncoder Loss: 0.001386371765\n",
      "Iteration: 13850, AutoEncoder Loss: 0.001385301893\n",
      "Iteration: 13860, AutoEncoder Loss: 0.001384225210\n",
      "Iteration: 13870, AutoEncoder Loss: 0.001383122109\n",
      "Iteration: 13880, AutoEncoder Loss: 0.001382030434\n",
      "Iteration: 13890, AutoEncoder Loss: 0.001380942869\n",
      "Iteration: 13900, AutoEncoder Loss: 0.001379861932\n",
      "Iteration: 13910, AutoEncoder Loss: 0.001378776016\n",
      "Iteration: 13920, AutoEncoder Loss: 0.001377793948\n",
      "Iteration: 13930, AutoEncoder Loss: 0.001376649092\n",
      "Iteration: 13940, AutoEncoder Loss: 0.001375561619\n",
      "Iteration: 13950, AutoEncoder Loss: 0.001374482331\n",
      "Iteration: 13960, AutoEncoder Loss: 0.001373406136\n",
      "Iteration: 13970, AutoEncoder Loss: 0.001372336273\n",
      "Iteration: 13980, AutoEncoder Loss: 0.001371262132\n",
      "Iteration: 13990, AutoEncoder Loss: 0.001370197166\n",
      "Iteration: 14000, AutoEncoder Loss: 0.001369124882\n",
      "Iteration: 14010, AutoEncoder Loss: 0.001368139688\n",
      "Iteration: 14020, AutoEncoder Loss: 0.001367025591\n",
      "Iteration: 14030, AutoEncoder Loss: 0.001365953996\n",
      "Iteration: 14040, AutoEncoder Loss: 0.001364889813\n",
      "Iteration: 14050, AutoEncoder Loss: 0.001363828226\n",
      "Iteration: 14060, AutoEncoder Loss: 0.001362773002\n",
      "Iteration: 14070, AutoEncoder Loss: 0.001361713415\n",
      "Iteration: 14080, AutoEncoder Loss: 0.001360662979\n",
      "Iteration: 14090, AutoEncoder Loss: 0.001359605206\n",
      "Iteration: 14100, AutoEncoder Loss: 0.001358607960\n",
      "Iteration: 14110, AutoEncoder Loss: 0.001357529519\n",
      "Iteration: 14120, AutoEncoder Loss: 0.001356475069\n",
      "Iteration: 14130, AutoEncoder Loss: 0.001355426017\n",
      "Iteration: 14140, AutoEncoder Loss: 0.001354379352\n",
      "Iteration: 14150, AutoEncoder Loss: 0.001353339089\n",
      "Iteration: 14160, AutoEncoder Loss: 0.001352293325\n",
      "Iteration: 14170, AutoEncoder Loss: 0.001351295282\n",
      "Iteration: 14180, AutoEncoder Loss: 0.001350239632\n",
      "Iteration: 14190, AutoEncoder Loss: 0.001349197285\n",
      "Iteration: 14200, AutoEncoder Loss: 0.001348159044\n",
      "Iteration: 14210, AutoEncoder Loss: 0.001347123106\n",
      "Iteration: 14220, AutoEncoder Loss: 0.001346093385\n",
      "Iteration: 14230, AutoEncoder Loss: 0.001345058831\n",
      "Iteration: 14240, AutoEncoder Loss: 0.001344034304\n",
      "Iteration: 14250, AutoEncoder Loss: 0.001343041057\n",
      "Iteration: 14260, AutoEncoder Loss: 0.001342000733\n",
      "Iteration: 14270, AutoEncoder Loss: 0.001340970981\n",
      "Iteration: 14280, AutoEncoder Loss: 0.001339945231\n",
      "Iteration: 14290, AutoEncoder Loss: 0.001338921605\n",
      "Iteration: 14300, AutoEncoder Loss: 0.001337899582\n",
      "Iteration: 14310, AutoEncoder Loss: 0.001336883852\n",
      "Iteration: 14320, AutoEncoder Loss: 0.001335862950\n",
      "Iteration: 14330, AutoEncoder Loss: 0.001334899187\n",
      "Iteration: 14340, AutoEncoder Loss: 0.001333872066\n",
      "Iteration: 14350, AutoEncoder Loss: 0.001332845652\n",
      "Iteration: 14360, AutoEncoder Loss: 0.001331829701\n",
      "Iteration: 14370, AutoEncoder Loss: 0.001330817578\n",
      "Iteration: 14380, AutoEncoder Loss: 0.001329807591\n",
      "Iteration: 14390, AutoEncoder Loss: 0.001328799169\n",
      "Iteration: 14400, AutoEncoder Loss: 0.001327796935\n",
      "Iteration: 14410, AutoEncoder Loss: 0.001326789611\n",
      "Iteration: 14420, AutoEncoder Loss: 0.001325862895\n",
      "Iteration: 14430, AutoEncoder Loss: 0.001324822128\n",
      "Iteration: 14440, AutoEncoder Loss: 0.001323810453\n",
      "Iteration: 14450, AutoEncoder Loss: 0.001322808215\n",
      "Iteration: 14460, AutoEncoder Loss: 0.001321809983\n",
      "Iteration: 14470, AutoEncoder Loss: 0.001320813493\n",
      "Iteration: 14480, AutoEncoder Loss: 0.001319818664\n",
      "Iteration: 14490, AutoEncoder Loss: 0.001319083667\n",
      "Iteration: 14500, AutoEncoder Loss: 0.001317872202\n",
      "Iteration: 14510, AutoEncoder Loss: 0.001316871241\n",
      "Iteration: 14520, AutoEncoder Loss: 0.001315879397\n",
      "Iteration: 14530, AutoEncoder Loss: 0.001314891165\n",
      "Iteration: 14540, AutoEncoder Loss: 0.001313904692\n",
      "Iteration: 14550, AutoEncoder Loss: 0.001312919836\n",
      "Iteration: 14560, AutoEncoder Loss: 0.001311940780\n",
      "Iteration: 14570, AutoEncoder Loss: 0.001310957010\n",
      "Iteration: 14580, AutoEncoder Loss: 0.001310229340\n",
      "Iteration: 14590, AutoEncoder Loss: 0.001309036084\n",
      "Iteration: 14600, AutoEncoder Loss: 0.001308048254\n",
      "Iteration: 14610, AutoEncoder Loss: 0.001307069451\n",
      "Iteration: 14620, AutoEncoder Loss: 0.001306094043\n",
      "Iteration: 14630, AutoEncoder Loss: 0.001305120494\n",
      "Iteration: 14640, AutoEncoder Loss: 0.001304148424\n",
      "Iteration: 14650, AutoEncoder Loss: 0.001303182127\n",
      "Iteration: 14660, AutoEncoder Loss: 0.001302211270\n",
      "Iteration: 14670, AutoEncoder Loss: 0.001301618702\n",
      "Iteration: 14680, AutoEncoder Loss: 0.001300312587\n",
      "Iteration: 14690, AutoEncoder Loss: 0.001299338597\n",
      "Iteration: 14700, AutoEncoder Loss: 0.001298372830\n",
      "Iteration: 14710, AutoEncoder Loss: 0.001297410474\n",
      "Iteration: 14720, AutoEncoder Loss: 0.001296449737\n",
      "Iteration: 14730, AutoEncoder Loss: 0.001295490558\n",
      "Iteration: 14740, AutoEncoder Loss: 0.001294782228\n",
      "Iteration: 14750, AutoEncoder Loss: 0.001293613888\n",
      "Iteration: 14760, AutoEncoder Loss: 0.001292648584\n",
      "Iteration: 14770, AutoEncoder Loss: 0.001291692240\n",
      "Iteration: 14780, AutoEncoder Loss: 0.001290739159\n",
      "Iteration: 14790, AutoEncoder Loss: 0.001289787918\n",
      "Iteration: 14800, AutoEncoder Loss: 0.001288838102\n",
      "Iteration: 14810, AutoEncoder Loss: 0.001287893266\n",
      "Iteration: 14820, AutoEncoder Loss: 0.001286945275\n",
      "Iteration: 14830, AutoEncoder Loss: 0.001286243860\n",
      "Iteration: 14840, AutoEncoder Loss: 0.001285092882\n",
      "Iteration: 14850, AutoEncoder Loss: 0.001284140242\n",
      "Iteration: 14860, AutoEncoder Loss: 0.001283195965\n",
      "Iteration: 14870, AutoEncoder Loss: 0.001282254944\n",
      "Iteration: 14880, AutoEncoder Loss: 0.001281315900\n",
      "Iteration: 14890, AutoEncoder Loss: 0.001280378298\n",
      "Iteration: 14900, AutoEncoder Loss: 0.001279446147\n",
      "Iteration: 14910, AutoEncoder Loss: 0.001278509807\n",
      "Iteration: 14920, AutoEncoder Loss: 0.001277749991\n",
      "Iteration: 14930, AutoEncoder Loss: 0.001276680319\n",
      "Iteration: 14940, AutoEncoder Loss: 0.001275739150\n",
      "Iteration: 14950, AutoEncoder Loss: 0.001274807155\n",
      "Iteration: 14960, AutoEncoder Loss: 0.001273878502\n",
      "Iteration: 14970, AutoEncoder Loss: 0.001272951651\n",
      "Iteration: 14980, AutoEncoder Loss: 0.001272026106\n",
      "Iteration: 14990, AutoEncoder Loss: 0.001271215821\n",
      "Iteration: 15000, AutoEncoder Loss: 0.001270213949\n",
      "Iteration: 15010, AutoEncoder Loss: 0.001269283786\n",
      "Iteration: 15020, AutoEncoder Loss: 0.001268361311\n",
      "Iteration: 15030, AutoEncoder Loss: 0.001267441537\n",
      "Iteration: 15040, AutoEncoder Loss: 0.001266523646\n",
      "Iteration: 15050, AutoEncoder Loss: 0.001265607225\n",
      "Iteration: 15060, AutoEncoder Loss: 0.001264695936\n",
      "Iteration: 15070, AutoEncoder Loss: 0.001263780682\n",
      "Iteration: 15080, AutoEncoder Loss: 0.001262976859\n",
      "Iteration: 15090, AutoEncoder Loss: 0.001261991103\n",
      "Iteration: 15100, AutoEncoder Loss: 0.001261072841\n",
      "Iteration: 15110, AutoEncoder Loss: 0.001260162027\n",
      "Iteration: 15120, AutoEncoder Loss: 0.001259254026\n",
      "Iteration: 15130, AutoEncoder Loss: 0.001258347698\n",
      "Iteration: 15140, AutoEncoder Loss: 0.001257442845\n",
      "Iteration: 15150, AutoEncoder Loss: 0.001256542719\n",
      "Iteration: 15160, AutoEncoder Loss: 0.001255639119\n",
      "Iteration: 15170, AutoEncoder Loss: 0.001254841527\n",
      "Iteration: 15180, AutoEncoder Loss: 0.001253872404\n",
      "Iteration: 15190, AutoEncoder Loss: 0.001252965724\n",
      "Iteration: 15200, AutoEncoder Loss: 0.001252066061\n",
      "Iteration: 15210, AutoEncoder Loss: 0.001251169555\n",
      "Iteration: 15220, AutoEncoder Loss: 0.001250274568\n",
      "Iteration: 15230, AutoEncoder Loss: 0.001249381012\n",
      "Iteration: 15240, AutoEncoder Loss: 0.001248492021\n",
      "Iteration: 15250, AutoEncoder Loss: 0.001247599893\n",
      "Iteration: 15260, AutoEncoder Loss: 0.001246783824\n",
      "Iteration: 15270, AutoEncoder Loss: 0.001245849261\n",
      "Iteration: 15280, AutoEncoder Loss: 0.001244955347\n",
      "Iteration: 15290, AutoEncoder Loss: 0.001244068032\n",
      "Iteration: 15300, AutoEncoder Loss: 0.001243182812\n",
      "Iteration: 15310, AutoEncoder Loss: 0.001242302996\n",
      "Iteration: 15320, AutoEncoder Loss: 0.001241419092\n",
      "Iteration: 15330, AutoEncoder Loss: 0.001240595859\n",
      "Iteration: 15340, AutoEncoder Loss: 0.001239685442\n",
      "Iteration: 15350, AutoEncoder Loss: 0.001238801169\n",
      "Iteration: 15360, AutoEncoder Loss: 0.001237922382\n",
      "Iteration: 15370, AutoEncoder Loss: 0.001237045733\n",
      "Iteration: 15380, AutoEncoder Loss: 0.001236173359\n",
      "Iteration: 15390, AutoEncoder Loss: 0.001235298752\n",
      "Iteration: 15400, AutoEncoder Loss: 0.001234429273\n",
      "Iteration: 15410, AutoEncoder Loss: 0.001233556857\n",
      "Iteration: 15420, AutoEncoder Loss: 0.001232743782\n",
      "Iteration: 15430, AutoEncoder Loss: 0.001231844548\n",
      "Iteration: 15440, AutoEncoder Loss: 0.001230971392\n",
      "Iteration: 15450, AutoEncoder Loss: 0.001230103293\n",
      "Iteration: 15460, AutoEncoder Loss: 0.001229237456\n",
      "Iteration: 15470, AutoEncoder Loss: 0.001228376234\n",
      "Iteration: 15480, AutoEncoder Loss: 0.001227511914\n",
      "Iteration: 15490, AutoEncoder Loss: 0.001226653684\n",
      "Iteration: 15500, AutoEncoder Loss: 0.001225791561\n",
      "Iteration: 15510, AutoEncoder Loss: 0.001224983652\n",
      "Iteration: 15520, AutoEncoder Loss: 0.001224098259\n",
      "Iteration: 15530, AutoEncoder Loss: 0.001223236314\n",
      "Iteration: 15540, AutoEncoder Loss: 0.001222379244\n",
      "Iteration: 15550, AutoEncoder Loss: 0.001221524363\n",
      "Iteration: 15560, AutoEncoder Loss: 0.001220673861\n",
      "Iteration: 15570, AutoEncoder Loss: 0.001219820218\n",
      "Iteration: 15580, AutoEncoder Loss: 0.001219012355\n",
      "Iteration: 15590, AutoEncoder Loss: 0.001218143417\n",
      "Iteration: 15600, AutoEncoder Loss: 0.001217290457\n",
      "Iteration: 15610, AutoEncoder Loss: 0.001216441533\n",
      "Iteration: 15620, AutoEncoder Loss: 0.001215594414\n",
      "Iteration: 15630, AutoEncoder Loss: 0.001214751214\n",
      "Iteration: 15640, AutoEncoder Loss: 0.001213906406\n",
      "Iteration: 15650, AutoEncoder Loss: 0.001213066325\n",
      "Iteration: 15660, AutoEncoder Loss: 0.001212223097\n",
      "Iteration: 15670, AutoEncoder Loss: 0.001211424549\n",
      "Iteration: 15680, AutoEncoder Loss: 0.001210566869\n",
      "Iteration: 15690, AutoEncoder Loss: 0.001209724060\n",
      "Iteration: 15700, AutoEncoder Loss: 0.001208885458\n",
      "Iteration: 15710, AutoEncoder Loss: 0.001208048738\n",
      "Iteration: 15720, AutoEncoder Loss: 0.001207215931\n",
      "Iteration: 15730, AutoEncoder Loss: 0.001206381030\n",
      "Iteration: 15740, AutoEncoder Loss: 0.001205551537\n",
      "Iteration: 15750, AutoEncoder Loss: 0.001204718214\n",
      "Iteration: 15760, AutoEncoder Loss: 0.001203922337\n",
      "Iteration: 15770, AutoEncoder Loss: 0.001203079263\n",
      "Iteration: 15780, AutoEncoder Loss: 0.001202247806\n",
      "Iteration: 15790, AutoEncoder Loss: 0.001201419755\n",
      "Iteration: 15800, AutoEncoder Loss: 0.001200593288\n",
      "Iteration: 15810, AutoEncoder Loss: 0.001199770739\n",
      "Iteration: 15820, AutoEncoder Loss: 0.001198950914\n",
      "Iteration: 15830, AutoEncoder Loss: 0.001198155587\n",
      "Iteration: 15840, AutoEncoder Loss: 0.001197323051\n",
      "Iteration: 15850, AutoEncoder Loss: 0.001196499750\n",
      "Iteration: 15860, AutoEncoder Loss: 0.001195679268\n",
      "Iteration: 15870, AutoEncoder Loss: 0.001194860413\n",
      "Iteration: 15880, AutoEncoder Loss: 0.001194045712\n",
      "Iteration: 15890, AutoEncoder Loss: 0.001193228407\n",
      "Iteration: 15900, AutoEncoder Loss: 0.001192416277\n",
      "Iteration: 15910, AutoEncoder Loss: 0.001191604691\n",
      "Iteration: 15920, AutoEncoder Loss: 0.001190819924\n",
      "Iteration: 15930, AutoEncoder Loss: 0.001189997534\n",
      "Iteration: 15940, AutoEncoder Loss: 0.001189183974\n",
      "Iteration: 15950, AutoEncoder Loss: 0.001188373377\n",
      "Iteration: 15960, AutoEncoder Loss: 0.001187564247\n",
      "Iteration: 15970, AutoEncoder Loss: 0.001186758958\n",
      "Iteration: 15980, AutoEncoder Loss: 0.001185951564\n",
      "Iteration: 15990, AutoEncoder Loss: 0.001185149385\n",
      "Iteration: 16000, AutoEncoder Loss: 0.001184343511\n",
      "Iteration: 16010, AutoEncoder Loss: 0.001183574030\n",
      "Iteration: 16020, AutoEncoder Loss: 0.001182759718\n",
      "Iteration: 16030, AutoEncoder Loss: 0.001181955358\n",
      "Iteration: 16040, AutoEncoder Loss: 0.001181154271\n",
      "Iteration: 16050, AutoEncoder Loss: 0.001180354736\n",
      "Iteration: 16060, AutoEncoder Loss: 0.001179559369\n",
      "Iteration: 16070, AutoEncoder Loss: 0.001178761275\n",
      "Iteration: 16080, AutoEncoder Loss: 0.001177968960\n",
      "Iteration: 16090, AutoEncoder Loss: 0.001177172140\n",
      "Iteration: 16100, AutoEncoder Loss: 0.001176409209\n",
      "Iteration: 16110, AutoEncoder Loss: 0.001175605434\n",
      "Iteration: 16120, AutoEncoder Loss: 0.001174811033\n",
      "Iteration: 16130, AutoEncoder Loss: 0.001174019685\n",
      "Iteration: 16140, AutoEncoder Loss: 0.001173229796\n",
      "Iteration: 16150, AutoEncoder Loss: 0.001172444446\n",
      "Iteration: 16160, AutoEncoder Loss: 0.001171895782\n",
      "Iteration: 16170, AutoEncoder Loss: 0.001170898070\n",
      "Iteration: 16180, AutoEncoder Loss: 0.001170103728\n",
      "Iteration: 16190, AutoEncoder Loss: 0.001169316928\n",
      "Iteration: 16200, AutoEncoder Loss: 0.001168532745\n",
      "Iteration: 16210, AutoEncoder Loss: 0.001167749954\n",
      "Iteration: 16220, AutoEncoder Loss: 0.001166971209\n",
      "Iteration: 16230, AutoEncoder Loss: 0.001166189532\n",
      "Iteration: 16240, AutoEncoder Loss: 0.001165413342\n",
      "Iteration: 16250, AutoEncoder Loss: 0.001164865848\n",
      "Iteration: 16260, AutoEncoder Loss: 0.001163885163\n",
      "Iteration: 16270, AutoEncoder Loss: 0.001163100109\n",
      "Iteration: 16280, AutoEncoder Loss: 0.001162322482\n",
      "Iteration: 16290, AutoEncoder Loss: 0.001161547425\n",
      "Iteration: 16300, AutoEncoder Loss: 0.001160773743\n",
      "Iteration: 16310, AutoEncoder Loss: 0.001160003957\n",
      "Iteration: 16320, AutoEncoder Loss: 0.001159231726\n",
      "Iteration: 16330, AutoEncoder Loss: 0.001158465388\n",
      "Iteration: 16340, AutoEncoder Loss: 0.001157705904\n",
      "Iteration: 16350, AutoEncoder Loss: 0.001156953446\n",
      "Iteration: 16360, AutoEncoder Loss: 0.001156177084\n",
      "Iteration: 16370, AutoEncoder Loss: 0.001155408504\n",
      "Iteration: 16380, AutoEncoder Loss: 0.001154642686\n",
      "Iteration: 16390, AutoEncoder Loss: 0.001153878142\n",
      "Iteration: 16400, AutoEncoder Loss: 0.001153117804\n",
      "Iteration: 16410, AutoEncoder Loss: 0.001152419154\n",
      "Iteration: 16420, AutoEncoder Loss: 0.001151618318\n",
      "Iteration: 16430, AutoEncoder Loss: 0.001150851448\n",
      "Iteration: 16440, AutoEncoder Loss: 0.001150090413\n",
      "Iteration: 16450, AutoEncoder Loss: 0.001149331299\n",
      "Iteration: 16460, AutoEncoder Loss: 0.001148573553\n",
      "Iteration: 16470, AutoEncoder Loss: 0.001147820131\n",
      "Iteration: 16480, AutoEncoder Loss: 0.001147063150\n",
      "Iteration: 16490, AutoEncoder Loss: 0.001146312026\n",
      "Iteration: 16500, AutoEncoder Loss: 0.001145683383\n",
      "Iteration: 16510, AutoEncoder Loss: 0.001144830641\n",
      "Iteration: 16520, AutoEncoder Loss: 0.001144071780\n",
      "Iteration: 16530, AutoEncoder Loss: 0.001143319178\n",
      "Iteration: 16540, AutoEncoder Loss: 0.001142568855\n",
      "Iteration: 16550, AutoEncoder Loss: 0.001141819814\n",
      "Iteration: 16560, AutoEncoder Loss: 0.001141074590\n",
      "Iteration: 16570, AutoEncoder Loss: 0.001140326576\n",
      "Iteration: 16580, AutoEncoder Loss: 0.001139583754\n",
      "Iteration: 16590, AutoEncoder Loss: 0.001139069444\n",
      "Iteration: 16600, AutoEncoder Loss: 0.001138119120\n",
      "Iteration: 16610, AutoEncoder Loss: 0.001137368484\n",
      "Iteration: 16620, AutoEncoder Loss: 0.001136624582\n",
      "Iteration: 16630, AutoEncoder Loss: 0.001135882892\n",
      "Iteration: 16640, AutoEncoder Loss: 0.001135142586\n",
      "Iteration: 16650, AutoEncoder Loss: 0.001134406442\n",
      "Iteration: 16660, AutoEncoder Loss: 0.001133792097\n",
      "Iteration: 16670, AutoEncoder Loss: 0.001132955445\n",
      "Iteration: 16680, AutoEncoder Loss: 0.001132211824\n",
      "Iteration: 16690, AutoEncoder Loss: 0.001131474415\n",
      "Iteration: 16700, AutoEncoder Loss: 0.001130739290\n",
      "Iteration: 16710, AutoEncoder Loss: 0.001130005355\n",
      "Iteration: 16720, AutoEncoder Loss: 0.001129274790\n",
      "Iteration: 16730, AutoEncoder Loss: 0.001128542289\n",
      "Iteration: 16740, AutoEncoder Loss: 0.001127814464\n",
      "Iteration: 16750, AutoEncoder Loss: 0.001127203344\n",
      "Iteration: 16760, AutoEncoder Loss: 0.001126379989\n",
      "Iteration: 16770, AutoEncoder Loss: 0.001125644868\n",
      "Iteration: 16780, AutoEncoder Loss: 0.001124915795\n",
      "Iteration: 16790, AutoEncoder Loss: 0.001124188946\n",
      "Iteration: 16800, AutoEncoder Loss: 0.001123463322\n",
      "Iteration: 16810, AutoEncoder Loss: 0.001122741688\n",
      "Iteration: 16820, AutoEncoder Loss: 0.001122016956\n",
      "Iteration: 16830, AutoEncoder Loss: 0.001121301518\n",
      "Iteration: 16840, AutoEncoder Loss: 0.001120602689\n",
      "Iteration: 16850, AutoEncoder Loss: 0.001119873066\n",
      "Iteration: 16860, AutoEncoder Loss: 0.001119150597\n",
      "Iteration: 16870, AutoEncoder Loss: 0.001118430780\n",
      "Iteration: 16880, AutoEncoder Loss: 0.001117712417\n",
      "Iteration: 16890, AutoEncoder Loss: 0.001116995025\n",
      "Iteration: 16900, AutoEncoder Loss: 0.001116281244\n",
      "Iteration: 16910, AutoEncoder Loss: 0.001115564920\n",
      "Iteration: 16920, AutoEncoder Loss: 0.001114853750\n",
      "Iteration: 16930, AutoEncoder Loss: 0.001114246077\n",
      "Iteration: 16940, AutoEncoder Loss: 0.001113448947\n",
      "Iteration: 16950, AutoEncoder Loss: 0.001112730921\n",
      "Iteration: 16960, AutoEncoder Loss: 0.001112018506\n",
      "Iteration: 16970, AutoEncoder Loss: 0.001111308099\n",
      "Iteration: 16980, AutoEncoder Loss: 0.001110598853\n",
      "Iteration: 16990, AutoEncoder Loss: 0.001109893493\n",
      "Iteration: 17000, AutoEncoder Loss: 0.001109300186\n",
      "Iteration: 17010, AutoEncoder Loss: 0.001108503513\n",
      "Iteration: 17020, AutoEncoder Loss: 0.001107791038\n",
      "Iteration: 17030, AutoEncoder Loss: 0.001107084470\n",
      "Iteration: 17040, AutoEncoder Loss: 0.001106379934\n",
      "Iteration: 17050, AutoEncoder Loss: 0.001105676699\n",
      "Iteration: 17060, AutoEncoder Loss: 0.001104977446\n",
      "Iteration: 17070, AutoEncoder Loss: 0.001104274907\n",
      "Iteration: 17080, AutoEncoder Loss: 0.001103577701\n",
      "Iteration: 17090, AutoEncoder Loss: 0.001102990768\n",
      "Iteration: 17100, AutoEncoder Loss: 0.001102202728\n",
      "Iteration: 17110, AutoEncoder Loss: 0.001101498343\n",
      "Iteration: 17120, AutoEncoder Loss: 0.001100799614\n",
      "Iteration: 17130, AutoEncoder Loss: 0.001100102881\n",
      "Iteration: 17140, AutoEncoder Loss: 0.001099407458\n",
      "Iteration: 17150, AutoEncoder Loss: 0.001098715954\n",
      "Iteration: 17160, AutoEncoder Loss: 0.001098021079\n",
      "Iteration: 17170, AutoEncoder Loss: 0.001097332238\n",
      "Iteration: 17180, AutoEncoder Loss: 0.001096650147\n",
      "Iteration: 17190, AutoEncoder Loss: 0.001095972755\n",
      "Iteration: 17200, AutoEncoder Loss: 0.001095274549\n",
      "Iteration: 17210, AutoEncoder Loss: 0.001094583233\n",
      "Iteration: 17220, AutoEncoder Loss: 0.001093894412\n",
      "Iteration: 17230, AutoEncoder Loss: 0.001093206728\n",
      "Iteration: 17240, AutoEncoder Loss: 0.001092523316\n",
      "Iteration: 17250, AutoEncoder Loss: 0.001092036427\n",
      "Iteration: 17260, AutoEncoder Loss: 0.001091176548\n",
      "Iteration: 17270, AutoEncoder Loss: 0.001090484748\n",
      "Iteration: 17280, AutoEncoder Loss: 0.001089799388\n",
      "Iteration: 17290, AutoEncoder Loss: 0.001089116230\n",
      "Iteration: 17300, AutoEncoder Loss: 0.001088434318\n",
      "Iteration: 17310, AutoEncoder Loss: 0.001087756275\n",
      "Iteration: 17320, AutoEncoder Loss: 0.001087075036\n",
      "Iteration: 17330, AutoEncoder Loss: 0.001086399491\n",
      "Iteration: 17340, AutoEncoder Loss: 0.001085729837\n",
      "Iteration: 17350, AutoEncoder Loss: 0.001085068605\n",
      "Iteration: 17360, AutoEncoder Loss: 0.001084383256\n",
      "Iteration: 17370, AutoEncoder Loss: 0.001083705116\n",
      "Iteration: 17380, AutoEncoder Loss: 0.001083029344\n",
      "Iteration: 17390, AutoEncoder Loss: 0.001082354899\n",
      "Iteration: 17400, AutoEncoder Loss: 0.001081684336\n",
      "Iteration: 17410, AutoEncoder Loss: 0.001081010364\n",
      "Iteration: 17420, AutoEncoder Loss: 0.001080338672\n",
      "Iteration: 17430, AutoEncoder Loss: 0.001079699370\n",
      "Iteration: 17440, AutoEncoder Loss: 0.001079018427\n",
      "Iteration: 17450, AutoEncoder Loss: 0.001078345984\n",
      "Iteration: 17460, AutoEncoder Loss: 0.001077676592\n",
      "Iteration: 17470, AutoEncoder Loss: 0.001077008532\n",
      "Iteration: 17480, AutoEncoder Loss: 0.001076341513\n",
      "Iteration: 17490, AutoEncoder Loss: 0.001075678361\n",
      "Iteration: 17500, AutoEncoder Loss: 0.001075011598\n",
      "Iteration: 17510, AutoEncoder Loss: 0.001074348332\n",
      "Iteration: 17520, AutoEncoder Loss: 0.001073688720\n",
      "Iteration: 17530, AutoEncoder Loss: 0.001073046384\n",
      "Iteration: 17540, AutoEncoder Loss: 0.001072376590\n",
      "Iteration: 17550, AutoEncoder Loss: 0.001071713396\n",
      "Iteration: 17560, AutoEncoder Loss: 0.001071052522\n",
      "Iteration: 17570, AutoEncoder Loss: 0.001070392733\n",
      "Iteration: 17580, AutoEncoder Loss: 0.001069735903\n",
      "Iteration: 17590, AutoEncoder Loss: 0.001069079893\n",
      "Iteration: 17600, AutoEncoder Loss: 0.001068446262\n",
      "Iteration: 17610, AutoEncoder Loss: 0.001067781471\n",
      "Iteration: 17620, AutoEncoder Loss: 0.001067123395\n",
      "Iteration: 17630, AutoEncoder Loss: 0.001066467806\n",
      "Iteration: 17640, AutoEncoder Loss: 0.001065813384\n",
      "Iteration: 17650, AutoEncoder Loss: 0.001065163211\n",
      "Iteration: 17660, AutoEncoder Loss: 0.001064508911\n",
      "Iteration: 17670, AutoEncoder Loss: 0.001063857128\n",
      "Iteration: 17680, AutoEncoder Loss: 0.001063207672\n",
      "Iteration: 17690, AutoEncoder Loss: 0.001062584816\n",
      "Iteration: 17700, AutoEncoder Loss: 0.001061926007\n",
      "Iteration: 17710, AutoEncoder Loss: 0.001061274803\n",
      "Iteration: 17720, AutoEncoder Loss: 0.001060626148\n",
      "Iteration: 17730, AutoEncoder Loss: 0.001059978687\n",
      "Iteration: 17740, AutoEncoder Loss: 0.001059335464\n",
      "Iteration: 17750, AutoEncoder Loss: 0.001058688138\n",
      "Iteration: 17760, AutoEncoder Loss: 0.001058043350\n",
      "Iteration: 17770, AutoEncoder Loss: 0.001057400799\n",
      "Iteration: 17780, AutoEncoder Loss: 0.001056787147\n",
      "Iteration: 17790, AutoEncoder Loss: 0.001056132412\n",
      "Iteration: 17800, AutoEncoder Loss: 0.001055487664\n",
      "Iteration: 17810, AutoEncoder Loss: 0.001054846000\n",
      "Iteration: 17820, AutoEncoder Loss: 0.001054205651\n",
      "Iteration: 17830, AutoEncoder Loss: 0.001053568137\n",
      "Iteration: 17840, AutoEncoder Loss: 0.001052928996\n",
      "Iteration: 17850, AutoEncoder Loss: 0.001052320394\n",
      "Iteration: 17860, AutoEncoder Loss: 0.001051671937\n",
      "Iteration: 17870, AutoEncoder Loss: 0.001051032648\n",
      "Iteration: 17880, AutoEncoder Loss: 0.001050396144\n",
      "Iteration: 17890, AutoEncoder Loss: 0.001049760911\n",
      "Iteration: 17900, AutoEncoder Loss: 0.001049129542\n",
      "Iteration: 17910, AutoEncoder Loss: 0.001048494620\n",
      "Iteration: 17920, AutoEncoder Loss: 0.001047861857\n",
      "Iteration: 17930, AutoEncoder Loss: 0.001047231475\n",
      "Iteration: 17940, AutoEncoder Loss: 0.001046635633\n",
      "Iteration: 17950, AutoEncoder Loss: 0.001045989063\n",
      "Iteration: 17960, AutoEncoder Loss: 0.001045355650\n",
      "Iteration: 17970, AutoEncoder Loss: 0.001044725516\n",
      "Iteration: 17980, AutoEncoder Loss: 0.001044096999\n",
      "Iteration: 17990, AutoEncoder Loss: 0.001043469743\n",
      "Iteration: 18000, AutoEncoder Loss: 0.001042843996\n",
      "Iteration: 18010, AutoEncoder Loss: 0.001042217891\n",
      "Iteration: 18020, AutoEncoder Loss: 0.001041594142\n",
      "Iteration: 18030, AutoEncoder Loss: 0.001041009246\n",
      "Iteration: 18040, AutoEncoder Loss: 0.001040365401\n",
      "Iteration: 18050, AutoEncoder Loss: 0.001039738240\n",
      "Iteration: 18060, AutoEncoder Loss: 0.001039114615\n",
      "Iteration: 18070, AutoEncoder Loss: 0.001038492666\n",
      "Iteration: 18080, AutoEncoder Loss: 0.001037871587\n",
      "Iteration: 18090, AutoEncoder Loss: 0.001037252783\n",
      "Iteration: 18100, AutoEncoder Loss: 0.001036633240\n",
      "Iteration: 18110, AutoEncoder Loss: 0.001036016026\n",
      "Iteration: 18120, AutoEncoder Loss: 0.001035444525\n",
      "Iteration: 18130, AutoEncoder Loss: 0.001034800156\n",
      "Iteration: 18140, AutoEncoder Loss: 0.001034178733\n",
      "Iteration: 18150, AutoEncoder Loss: 0.001033561626\n",
      "Iteration: 18160, AutoEncoder Loss: 0.001032946171\n",
      "Iteration: 18170, AutoEncoder Loss: 0.001032331682\n",
      "Iteration: 18180, AutoEncoder Loss: 0.001031719425\n",
      "Iteration: 18190, AutoEncoder Loss: 0.001031150314\n",
      "Iteration: 18200, AutoEncoder Loss: 0.001030514331\n",
      "Iteration: 18210, AutoEncoder Loss: 0.001029898143\n",
      "Iteration: 18220, AutoEncoder Loss: 0.001029285938\n",
      "Iteration: 18230, AutoEncoder Loss: 0.001028675302\n",
      "Iteration: 18240, AutoEncoder Loss: 0.001028068677\n",
      "Iteration: 18250, AutoEncoder Loss: 0.001027458312\n",
      "Iteration: 18260, AutoEncoder Loss: 0.001026850341\n",
      "Iteration: 18270, AutoEncoder Loss: 0.001026244457\n",
      "Iteration: 18280, AutoEncoder Loss: 0.001025742109\n",
      "Iteration: 18290, AutoEncoder Loss: 0.001025053650\n",
      "Iteration: 18300, AutoEncoder Loss: 0.001024442515\n",
      "Iteration: 18310, AutoEncoder Loss: 0.001023836281\n",
      "Iteration: 18320, AutoEncoder Loss: 0.001023231953\n",
      "Iteration: 18330, AutoEncoder Loss: 0.001022628812\n",
      "Iteration: 18340, AutoEncoder Loss: 0.001022027452\n",
      "Iteration: 18350, AutoEncoder Loss: 0.001021425732\n",
      "Iteration: 18360, AutoEncoder Loss: 0.001020826422\n",
      "Iteration: 18370, AutoEncoder Loss: 0.001020227722\n",
      "Iteration: 18380, AutoEncoder Loss: 0.001019648141\n",
      "Iteration: 18390, AutoEncoder Loss: 0.001019042057\n",
      "Iteration: 18400, AutoEncoder Loss: 0.001018441908\n",
      "Iteration: 18410, AutoEncoder Loss: 0.001017843835\n",
      "Iteration: 18420, AutoEncoder Loss: 0.001017246836\n",
      "Iteration: 18430, AutoEncoder Loss: 0.001016652054\n",
      "Iteration: 18440, AutoEncoder Loss: 0.001016065899\n",
      "Iteration: 18450, AutoEncoder Loss: 0.001015485526\n",
      "Iteration: 18460, AutoEncoder Loss: 0.001014883726\n",
      "Iteration: 18470, AutoEncoder Loss: 0.001014288022\n",
      "Iteration: 18480, AutoEncoder Loss: 0.001013694537\n",
      "Iteration: 18490, AutoEncoder Loss: 0.001013102100\n",
      "Iteration: 18500, AutoEncoder Loss: 0.001012511970\n",
      "Iteration: 18510, AutoEncoder Loss: 0.001011921141\n",
      "Iteration: 18520, AutoEncoder Loss: 0.001011332533\n",
      "Iteration: 18530, AutoEncoder Loss: 0.001010742974\n",
      "Iteration: 18540, AutoEncoder Loss: 0.001010179172\n",
      "Iteration: 18550, AutoEncoder Loss: 0.001009582479\n",
      "Iteration: 18560, AutoEncoder Loss: 0.001008992759\n",
      "Iteration: 18570, AutoEncoder Loss: 0.001008405230\n",
      "Iteration: 18580, AutoEncoder Loss: 0.001007818803\n",
      "Iteration: 18590, AutoEncoder Loss: 0.001007234627\n",
      "Iteration: 18600, AutoEncoder Loss: 0.001006649856\n",
      "Iteration: 18610, AutoEncoder Loss: 0.001006067514\n",
      "Iteration: 18620, AutoEncoder Loss: 0.001005483681\n",
      "Iteration: 18630, AutoEncoder Loss: 0.001004927974\n",
      "Iteration: 18640, AutoEncoder Loss: 0.001004334556\n",
      "Iteration: 18650, AutoEncoder Loss: 0.001003750445\n",
      "Iteration: 18660, AutoEncoder Loss: 0.001003168882\n",
      "Iteration: 18670, AutoEncoder Loss: 0.001002588559\n",
      "Iteration: 18680, AutoEncoder Loss: 0.001002010500\n",
      "Iteration: 18690, AutoEncoder Loss: 0.001001431571\n",
      "Iteration: 18700, AutoEncoder Loss: 0.001000882800\n",
      "Iteration: 18710, AutoEncoder Loss: 0.001000293142\n",
      "Iteration: 18720, AutoEncoder Loss: 0.000999713135\n",
      "Iteration: 18730, AutoEncoder Loss: 0.000999135984\n",
      "Iteration: 18740, AutoEncoder Loss: 0.000998560015\n",
      "Iteration: 18750, AutoEncoder Loss: 0.000997986384\n",
      "Iteration: 18760, AutoEncoder Loss: 0.000997412003\n",
      "Iteration: 18770, AutoEncoder Loss: 0.000996840257\n",
      "Iteration: 18780, AutoEncoder Loss: 0.000996266725\n",
      "Iteration: 18790, AutoEncoder Loss: 0.000995726779\n",
      "Iteration: 18800, AutoEncoder Loss: 0.000995140204\n",
      "Iteration: 18810, AutoEncoder Loss: 0.000994565723\n",
      "Iteration: 18820, AutoEncoder Loss: 0.000993994358\n",
      "Iteration: 18830, AutoEncoder Loss: 0.000993424175\n",
      "Iteration: 18840, AutoEncoder Loss: 0.000992856213\n",
      "Iteration: 18850, AutoEncoder Loss: 0.000992287674\n",
      "Iteration: 18860, AutoEncoder Loss: 0.000991721596\n",
      "Iteration: 18870, AutoEncoder Loss: 0.000991153888\n",
      "Iteration: 18880, AutoEncoder Loss: 0.000990628973\n",
      "Iteration: 18890, AutoEncoder Loss: 0.000990042042\n",
      "Iteration: 18900, AutoEncoder Loss: 0.000989471955\n",
      "Iteration: 18910, AutoEncoder Loss: 0.000988905834\n",
      "Iteration: 18920, AutoEncoder Loss: 0.000988341313\n",
      "Iteration: 18930, AutoEncoder Loss: 0.000987777678\n",
      "Iteration: 18940, AutoEncoder Loss: 0.000987214750\n",
      "Iteration: 18950, AutoEncoder Loss: 0.000986654226\n",
      "Iteration: 18960, AutoEncoder Loss: 0.000986092307\n",
      "Iteration: 18970, AutoEncoder Loss: 0.000985586050\n",
      "Iteration: 18980, AutoEncoder Loss: 0.000984990926\n",
      "Iteration: 18990, AutoEncoder Loss: 0.000984426347\n",
      "Iteration: 19000, AutoEncoder Loss: 0.000983865800\n",
      "Iteration: 19010, AutoEncoder Loss: 0.000983307025\n",
      "Iteration: 19020, AutoEncoder Loss: 0.000982749090\n",
      "Iteration: 19030, AutoEncoder Loss: 0.000982191915\n",
      "Iteration: 19040, AutoEncoder Loss: 0.000981677164\n",
      "Iteration: 19050, AutoEncoder Loss: 0.000981099863\n",
      "Iteration: 19060, AutoEncoder Loss: 0.000980539667\n",
      "Iteration: 19070, AutoEncoder Loss: 0.000979983488\n",
      "Iteration: 19080, AutoEncoder Loss: 0.000979428929\n",
      "Iteration: 19090, AutoEncoder Loss: 0.000978875209\n",
      "Iteration: 19100, AutoEncoder Loss: 0.000978322260\n",
      "Iteration: 19110, AutoEncoder Loss: 0.000977771919\n",
      "Iteration: 19120, AutoEncoder Loss: 0.000977219518\n",
      "Iteration: 19130, AutoEncoder Loss: 0.000976793668\n",
      "Iteration: 19140, AutoEncoder Loss: 0.000976139989\n",
      "Iteration: 19150, AutoEncoder Loss: 0.000975584178\n",
      "Iteration: 19160, AutoEncoder Loss: 0.000975033192\n",
      "Iteration: 19170, AutoEncoder Loss: 0.000974483938\n",
      "Iteration: 19180, AutoEncoder Loss: 0.000973935708\n",
      "Iteration: 19190, AutoEncoder Loss: 0.000973388127\n",
      "Iteration: 19200, AutoEncoder Loss: 0.000972843529\n",
      "Iteration: 19210, AutoEncoder Loss: 0.000972296346\n",
      "Iteration: 19220, AutoEncoder Loss: 0.000971809445\n",
      "Iteration: 19230, AutoEncoder Loss: 0.000971226814\n",
      "Iteration: 19240, AutoEncoder Loss: 0.000970676271\n",
      "Iteration: 19250, AutoEncoder Loss: 0.000970130638\n",
      "Iteration: 19260, AutoEncoder Loss: 0.000969586923\n",
      "Iteration: 19270, AutoEncoder Loss: 0.000969044173\n",
      "Iteration: 19280, AutoEncoder Loss: 0.000968502103\n",
      "Iteration: 19290, AutoEncoder Loss: 0.000968019706\n",
      "Iteration: 19300, AutoEncoder Loss: 0.000967440546\n",
      "Iteration: 19310, AutoEncoder Loss: 0.000966894996\n",
      "Iteration: 19320, AutoEncoder Loss: 0.000966353724\n",
      "Iteration: 19330, AutoEncoder Loss: 0.000965814058\n",
      "Iteration: 19340, AutoEncoder Loss: 0.000965275374\n",
      "Iteration: 19350, AutoEncoder Loss: 0.000964737317\n",
      "Iteration: 19360, AutoEncoder Loss: 0.000964202182\n",
      "Iteration: 19370, AutoEncoder Loss: 0.000963664490\n",
      "Iteration: 19380, AutoEncoder Loss: 0.000963170806\n",
      "Iteration: 19390, AutoEncoder Loss: 0.000962615164\n",
      "Iteration: 19400, AutoEncoder Loss: 0.000962073653\n",
      "Iteration: 19410, AutoEncoder Loss: 0.000961537273\n",
      "Iteration: 19420, AutoEncoder Loss: 0.000961002798\n",
      "Iteration: 19430, AutoEncoder Loss: 0.000960469253\n",
      "Iteration: 19440, AutoEncoder Loss: 0.000959936489\n",
      "Iteration: 19450, AutoEncoder Loss: 0.000959406799\n",
      "Iteration: 19460, AutoEncoder Loss: 0.000958874113\n",
      "Iteration: 19470, AutoEncoder Loss: 0.000958345372\n",
      "Iteration: 19480, AutoEncoder Loss: 0.000957836200\n",
      "Iteration: 19490, AutoEncoder Loss: 0.000957299108\n",
      "Iteration: 19500, AutoEncoder Loss: 0.000956767667\n",
      "Iteration: 19510, AutoEncoder Loss: 0.000956238322\n",
      "Iteration: 19520, AutoEncoder Loss: 0.000955709964\n",
      "Iteration: 19530, AutoEncoder Loss: 0.000955182349\n",
      "Iteration: 19540, AutoEncoder Loss: 0.000954657653\n",
      "Iteration: 19550, AutoEncoder Loss: 0.000954130149\n",
      "Iteration: 19560, AutoEncoder Loss: 0.000953606152\n",
      "Iteration: 19570, AutoEncoder Loss: 0.000953100497\n",
      "Iteration: 19580, AutoEncoder Loss: 0.000952569180\n",
      "Iteration: 19590, AutoEncoder Loss: 0.000952043162\n",
      "Iteration: 19600, AutoEncoder Loss: 0.000951519084\n",
      "Iteration: 19610, AutoEncoder Loss: 0.000950995918\n",
      "Iteration: 19620, AutoEncoder Loss: 0.000950473501\n",
      "Iteration: 19630, AutoEncoder Loss: 0.000949955293\n",
      "Iteration: 19640, AutoEncoder Loss: 0.000949452325\n",
      "Iteration: 19650, AutoEncoder Loss: 0.000948925198\n",
      "Iteration: 19660, AutoEncoder Loss: 0.000948403011\n",
      "Iteration: 19670, AutoEncoder Loss: 0.000947882759\n",
      "Iteration: 19680, AutoEncoder Loss: 0.000947363457\n",
      "Iteration: 19690, AutoEncoder Loss: 0.000946844867\n",
      "Iteration: 19700, AutoEncoder Loss: 0.000946329449\n",
      "Iteration: 19710, AutoEncoder Loss: 0.000945810623\n",
      "Iteration: 19720, AutoEncoder Loss: 0.000945293850\n",
      "Iteration: 19730, AutoEncoder Loss: 0.000944801604\n",
      "Iteration: 19740, AutoEncoder Loss: 0.000944277850\n",
      "Iteration: 19750, AutoEncoder Loss: 0.000943760291\n",
      "Iteration: 19760, AutoEncoder Loss: 0.000943244839\n",
      "Iteration: 19770, AutoEncoder Loss: 0.000942730431\n",
      "Iteration: 19780, AutoEncoder Loss: 0.000942216756\n",
      "Iteration: 19790, AutoEncoder Loss: 0.000941706213\n",
      "Iteration: 19800, AutoEncoder Loss: 0.000941192506\n",
      "Iteration: 19810, AutoEncoder Loss: 0.000940680577\n",
      "Iteration: 19820, AutoEncoder Loss: 0.000940170488\n",
      "Iteration: 19830, AutoEncoder Loss: 0.000939685244\n",
      "Iteration: 19840, AutoEncoder Loss: 0.000939164493\n",
      "Iteration: 19850, AutoEncoder Loss: 0.000938652134\n",
      "Iteration: 19860, AutoEncoder Loss: 0.000938142106\n",
      "Iteration: 19870, AutoEncoder Loss: 0.000937633122\n",
      "Iteration: 19880, AutoEncoder Loss: 0.000937127395\n",
      "Iteration: 19890, AutoEncoder Loss: 0.000936618467\n",
      "Iteration: 19900, AutoEncoder Loss: 0.000936111470\n",
      "Iteration: 19910, AutoEncoder Loss: 0.000935606207\n",
      "Iteration: 19920, AutoEncoder Loss: 0.000935136919\n",
      "Iteration: 19930, AutoEncoder Loss: 0.000934610793\n",
      "Iteration: 19940, AutoEncoder Loss: 0.000934101891\n",
      "Iteration: 19950, AutoEncoder Loss: 0.000933596411\n",
      "Iteration: 19960, AutoEncoder Loss: 0.000933092244\n",
      "Iteration: 19970, AutoEncoder Loss: 0.000932588887\n",
      "Iteration: 19980, AutoEncoder Loss: 0.000932087358\n",
      "Iteration: 19990, AutoEncoder Loss: 0.000931670860\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    reconstruction = auto(targ_x)\n",
    "    loss = mean_squared_error(target, reconstruction)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration: {i}, AutoEncoder Loss: {loss.data:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99372441, 0.        , 0.00188588, 0.        , 0.00532508])"
      ]
     },
     "execution_count": 1533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected 1, result, 0.9937245240333988\n",
      "expected 0, result, 4.105767168258025e-05\n",
      "expected 0, result, 0.013248325385966863\n",
      "expected 0, result, 0.0008177478201871155\n",
      "expected 0, result, 0.002828088838075048\n",
      "--------------------------------\n",
      "expected 0, result, 1.6283061438051871e-19\n",
      "expected 1, result, 0.9957851096311384\n",
      "expected 0, result, 1.4884960651020122e-06\n",
      "expected 0, result, 0.012124381112724729\n",
      "expected 0, result, 0.0013718469096606204\n",
      "--------------------------------\n",
      "expected 0, result, 0.0018852724066549596\n",
      "expected 0, result, 7.381796433418657e-18\n",
      "expected 1, result, 0.99380081599181\n",
      "expected 0, result, 0.01322322805113037\n",
      "expected 0, result, 1.0308214588240528e-90\n",
      "--------------------------------\n",
      "expected 0, result, 3.2942816057357597e-34\n",
      "expected 0, result, 0.0008143493135999044\n",
      "expected 0, result, 0.0036209043339512537\n",
      "expected 1, result, 0.9878373069377615\n",
      "expected 0, result, 2.1867664888037745e-80\n",
      "--------------------------------\n",
      "expected 0, result, 0.005325048845978279\n",
      "expected 0, result, 0.010488844259549244\n",
      "expected 0, result, 0.0013085303391774127\n",
      "expected 0, result, 0.001790021128772191\n",
      "expected 1, result, 0.9956349079953762\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x[0])):\n",
    "    for j in range(len(x)):\n",
    "        print(f\"expected {x[j][i]}, result, {auto(targ_x).data[j][i]}\")\n",
    "    print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
