{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)\n",
    "                          ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # fix dead neuron problem\n",
    "    def leaky_relu(self):\n",
    "        out = Value(self.data * 0.01 if self.data <\n",
    "                    0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            local_grad = 1.0 if self.data > 0 else 0.01\n",
    "            self.grad += local_grad * out.grad \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "\n",
    "        out = Value(math.log(self.data), (self, ), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        t = 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "        out = Value(t, (self, ), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data * (1 - out.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other.data\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.data > other.data\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13336\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def layers(self):\n",
    "        return []\n",
    "\n",
    "    def summary(self):\n",
    "        return f\"{len(self.layers())} layers, {len(self.parameters())} parameters\"\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # I want to introduce weight sharing, which means I need to be able to\n",
    "    # initialise a neuron with pre defined weights, but leave the bias?\n",
    "\n",
    "    def __init__(self, nin, nonlin=True, **kwargs):\n",
    "        tied_weights = kwargs.get('tied_weights', None)\n",
    "        self.w = tied_weights if tied_weights is not None else [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        self.nonlin = nonlin\n",
    "        self.activate = kwargs.get('activate', None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, (Value, float, int)):\n",
    "            # This is for a single input, likely at the start of a layer\n",
    "            act = (self.w[0] * x) + self.b\n",
    "        else:\n",
    "            act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        if self.activate and self.nonlin == False:\n",
    "            return self.activate(act)\n",
    "        else:\n",
    "            return act.leaky_relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b] if isinstance(self.w[0], Value) else [p for w_list in self.w for p in w_list] + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else '{self.activate}'}Neuron({len(self.w)})\"\n",
    "\n",
    "\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, tied_to_layer=None, **kwargs):\n",
    "        if tied_to_layer is None:\n",
    "            # Standard Layer initialization\n",
    "            self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "        else:\n",
    "            # Tied Layer initialization\n",
    "            # The weights for this layer are the transpose of the tied_to_layer's weights\n",
    "            # This requires careful construction.\n",
    "            # Number of inputs for this layer = number of outputs of the tied layer\n",
    "            # Number of outputs for this layer = number of inputs of the tied layer\n",
    "            self.tied_to_layer = tied_to_layer\n",
    "            self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            # The weights are conceptually transposed.\n",
    "            # So, the output of a neuron is sum(w_ji * x_i), which means summing over the neurons of the previous layer.\n",
    "            # This is hard to do cleanly with the current structure.\n",
    "            # The simpler approach is to loop manually.\n",
    "            out = []\n",
    "            for j in range(len(self.neurons)):\n",
    "                # The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\n",
    "                # For each output neuron (j), sum the weighted inputs.\n",
    "                # The weight connecting input `i` to output `j` is the same as the weight connecting\n",
    "                # input `j` of the encoder layer to output `i`.\n",
    "                act = sum(self.tied_to_layer.neurons[i].w[j] * x[i] for i in range(len(x))) + self.neurons[j].b if isinstance(x, list) else self.tied_to_layer.neurons[0].w[j] * x + self.neurons[j].b\n",
    "                \n",
    "                # Apply activation\n",
    "                if self.neurons[j].activate and self.neurons[j].nonlin is False:\n",
    "                    act = self.neurons[j].activate(act)\n",
    "                else:\n",
    "                    act = act.relu() if self.neurons[j].nonlin else act\n",
    "                out.append(act)\n",
    "            return out[0] if len(out) == 1 else out\n",
    "        else:\n",
    "            # Standard layer behavior\n",
    "            out = [n(x) for n in self.neurons]\n",
    "            return out\n",
    "\n",
    "    def parameters(self):\n",
    "        # In a tied layer, the weights are shared, but the biases are not.\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            return [n.b for n in self.neurons]\n",
    "        else:\n",
    "            return [p for n in self.neurons for p in n.parameters()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts, tied_weights_from=None, **kwargs):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = []\n",
    "        if tied_weights_from is None:\n",
    "            # Standard MLP initialization\n",
    "            self.layers = [Layer(sz[i], sz[i+1], nonlin=i != len(nouts)-1, **kwargs) for i in range(len(nouts))]\n",
    "        else:\n",
    "            # Tied-weight MLP initialization\n",
    "            tied_layers = list(reversed(tied_weights_from))\n",
    "            for i in range(len(nouts)):\n",
    "                # Pass the encoder's layer directly to the decoder's layer.\n",
    "                # The decoder layer will use the encoder's weights.\n",
    "                self.layers.append(Layer(sz[i], sz[i+1], tied_to_layer=tied_layers[i], nonlin=i != len(nouts)-1, **kwargs))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "\n",
    "class AutoEncoder(Module):\n",
    "    def __init__(self, in_embeds=1, hidden_layers=[], latent_dim=1, act_func=None, tied=False):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.act_func = act_func\n",
    "        self.encoder = MLP(in_embeds, hidden_layers + [latent_dim])\n",
    "        \n",
    "        # Create decoder, passing encoder layers for tied weights\n",
    "        if tied:\n",
    "            self.decoder = MLP(latent_dim, list(reversed(hidden_layers)) + [in_embeds], tied_weights_from=self.encoder.layers, activate=act_func)\n",
    "        else:\n",
    "            self.decoder = MLP(latent_dim, list(reversed(hidden_layers)) + [in_embeds], activate=act_func)\n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        compressed = self.encoder(x)\n",
    "        out = self.decoder(compressed)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.encoder.parameters() + self.decoder.parameters()\n",
    "\n",
    "    def layers(self):\n",
    "        return self.encoder.layers + self.decoder.layers\n",
    "\n",
    "    def pretty(self):\n",
    "        if self.act_func != None:\n",
    "            hey = str(self.act_func)\n",
    "            return hey.split()[1][6:]\n",
    "        else:\n",
    "            return \"no function\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"encoder has {len(self.encoder.layers)}, decoder has {len(self.decoder.layers)}, latent dim is {self.latent_dim} activated with {self.pretty()}\"\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(Module):\n",
    "    \"\"\"\n",
    "    Simple Variational Autoencoder implementation.\n",
    "    The encoder outputs mean and log-variance for each latent dimension.\n",
    "    Uses reparameterization trick to sample from the latent distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_embeds=1, hidden_layers=[], latent_dim=1, act_func=None, tied=False):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.act_func = act_func\n",
    "        \n",
    "        # Encoder outputs 2 * latent_dim: mean and log-variance for each dimension\n",
    "        # Last layer outputs 2*latent_dim (no activation on this layer)\n",
    "        self.encoder = MLP(in_embeds, hidden_layers + [2 * latent_dim])\n",
    "        \n",
    "        # Decoder takes latent_dim as input\n",
    "        if tied:\n",
    "            # For tied weights, we'd need to handle the 2*latent_dim -> latent_dim transition\n",
    "            # For simplicity, we'll skip tied weights in VAE for now\n",
    "            self.decoder = MLP(latent_dim, list(reversed(hidden_layers)) + [in_embeds], activate=act_func)\n",
    "        else:\n",
    "            self.decoder = MLP(latent_dim, list(reversed(hidden_layers)) + [in_embeds], activate=act_func)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to mean and log-variance\"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        # Ensure encoded is a list\n",
    "        if not isinstance(encoded, list):\n",
    "            encoded = [encoded]\n",
    "        \n",
    "        # Split the output into mean and log_var\n",
    "        # encoded should be a list of 2*latent_dim values\n",
    "        if len(encoded) != 2 * self.latent_dim:\n",
    "            raise ValueError(f\"Encoder output dimension {len(encoded)} doesn't match expected 2*latent_dim={2*self.latent_dim}\")\n",
    "        \n",
    "        mu = encoded[:self.latent_dim]\n",
    "        log_var = encoded[self.latent_dim:]\n",
    "        \n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + sigma * epsilon\n",
    "        where epsilon ~ N(0,1) and sigma = exp(0.5 * log_var)\n",
    "        \n",
    "        Note: epsilon is sampled and treated as a constant during backprop\n",
    "        \"\"\"\n",
    "        # Sample epsilon from standard normal (treated as constant in backprop)\n",
    "        epsilon = [Value(random.gauss(0, 1)) for _ in range(len(mu))]\n",
    "        \n",
    "        # Compute sigma = exp(0.5 * log_var) more efficiently\n",
    "        # sigma = exp(0.5 * log_var) = sqrt(exp(log_var))\n",
    "        sigma = [(log_var_i * 0.5).exp() for log_var_i in log_var]\n",
    "        \n",
    "        # z = mu + sigma * epsilon\n",
    "        z = [mu_i + sigma_i * eps_i for mu_i, sigma_i, eps_i in zip(mu, sigma, epsilon)]\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent sample to reconstruction\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass: encode, sample, decode\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mu, log_var\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.encoder.parameters() + self.decoder.parameters()\n",
    "    \n",
    "    \n",
    "    def layers(self):\n",
    "        return self.encoder.layers + self.decoder.layers\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"VAE(encoder: {len(self.encoder.layers)} layers, decoder: {len(self.decoder.layers)} layers, latent_dim: {self.latent_dim})\"\n",
    "\n",
    "\n",
    "truth = AutoEncoder(in_embeds=784, latent_dim=8)\n",
    "ban = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "print(len(truth.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers\"\"\"\n",
    "\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = 0\n",
    "\n",
    "  \n",
    "    def step(self):\n",
    "        \"\"\"Take a step of gradient descent\"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "\n",
    "    def __init__(self, parameters, learning_rate = 0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.count = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def increment(self):\n",
    "        self.count += 1\n",
    "\n",
    "    def step(self):\n",
    "        self.increment()\n",
    "        # Calculate the base LR\n",
    "        new_lr = 1.0 - 0.9 * self.count / 100\n",
    "        \n",
    "        # Ensure the learning rate is never negative (or zero, to stop training)\n",
    "        self.learning_rate = max(0.00001, new_lr) \n",
    "        \n",
    "        \"\"\"Update model parameters in the opposite direction of their gradient\"\"\"\n",
    "\n",
    "        if self.learning_rate > 0: # Only update if the LR is positive\n",
    "            for p in self.parameters:\n",
    "                p.data -= self.learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class BatchIterator:\n",
    "    \"\"\"Iterates on data by batches\"\"\"\n",
    "\n",
    "    def __init__(self,inputs,targets,batch_size=32):\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self):\n",
    "        starts = list(range(0, len(self.inputs), self.batch_size))\n",
    "            \n",
    "        for start in starts:\n",
    "            end = start + self.batch_size\n",
    "            batch_inputs = self.inputs[start:end]\n",
    "            batch_targets = self.targets[start:end]\n",
    "            yield (batch_inputs, batch_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    total_loss = sum([(true - pred)**2 for true, pred in zip(y_true, y_pred)])\n",
    "    mean_loss = total_loss / len(y_true)\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def vae_loss(reconstruction, target, mu, log_var, beta=1.0):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder loss = Reconstruction Loss + beta * KL Divergence\n",
    "    \n",
    "    Args:\n",
    "        reconstruction: Value or List of Value objects (reconstructed output)\n",
    "        target: List of Value objects (original input)\n",
    "        mu: List of Value objects (mean of latent distribution)\n",
    "        log_var: List of Value objects (log variance of latent distribution)\n",
    "        beta: Weight for KL divergence term (default 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Total VAE loss as a Value object\n",
    "    \"\"\"\n",
    "    # Ensure reconstruction is a list\n",
    "    if not isinstance(reconstruction, list):\n",
    "        reconstruction = [reconstruction]\n",
    "    if not isinstance(target, list):\n",
    "        target = [target]\n",
    "    \n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = mean_squared_error(target, reconstruction)\n",
    "    \n",
    "    # KL divergence: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "    # This encourages the latent distribution to be close to N(0,1)\n",
    "    kl_terms = []\n",
    "    for mu_i, log_var_i in zip(mu, log_var):\n",
    "        # KL term for one dimension: -0.5 * (1 + log_var - mu^2 - exp(log_var))\n",
    "        kl_term = -0.5 * (Value(1.0) + log_var_i - mu_i**2 - log_var_i.exp())\n",
    "        kl_terms.append(kl_term)\n",
    "    \n",
    "    kl_loss = sum(kl_terms) / len(kl_terms) if kl_terms else Value(0.0)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " # from pyfit.engine import Vector, Scalar\n",
    "# from pyfit.nn import Module\n",
    "# from pyfit.optim import Optimizer\n",
    "# from pyfit.data import BatchIterator\n",
    "# from pyfit.metrics import binary_accuracy\n",
    "\n",
    "# Used to record training history for metrics\n",
    "History = {}\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Encapsulates the model training loop\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, data_iterator, num_epochs=500, verbose=False):\n",
    "        \"\"\"Fits the model to the data\"\"\"\n",
    "\n",
    "        history = {\"loss\": []}\n",
    "        epoch_loss = 0\n",
    "        epoch_y_true = []\n",
    "        epoch_y_pred = []\n",
    "        for epoch in range(num_epochs):\n",
    "            # Reset the gradients of model parameters\n",
    "            self.optimizer.zero_grad()\n",
    "            # Reset epoch data\n",
    "            epoch_loss = 0\n",
    "            epoch_y_true = []\n",
    "            epoch_y_pred = []\n",
    "\n",
    "            for batch in data_iterator():\n",
    "                # Forward pass\n",
    "                outputs = list(map(self.model, batch[0]))\n",
    "                \n",
    "                batch_y_true = [Value(val) for sublist in batch[1] for val in sublist]\n",
    "                batch_y_pred = [val for sublist in outputs for val in sublist]\n",
    "                # Loss computation\n",
    "                # [item for sublist in outputs[0] for item in sublist]\n",
    "                batch_loss = self.loss(batch_y_true, batch_y_pred)\n",
    "                epoch_loss += batch_loss.data\n",
    "\n",
    "                # Store batch predictions and ground truth for computing epoch metrics\n",
    "                epoch_y_pred.extend(batch_y_pred)\n",
    "                epoch_y_true.extend(batch[1])\n",
    "\n",
    "                # Backprop and gradient descent\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Accuracy computation for epoch\n",
    "            \n",
    "            \n",
    "\n",
    "            # Record training history\n",
    "            history[\"loss\"].append(epoch_loss)\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                    f\"loss: {epoch_loss:.6f}, \"\n",
    "                )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(\n",
    "    in_embeds=10,        # Input dimension\n",
    "    hidden_layers=[6],   # Number of hidden layers\n",
    "    latent_dim=3,        # Latent space dimension\n",
    "    act_func=None,       # Activation function (None uses default leaky_relu)\n",
    "    tied=False           # Whether to use tied weights\n",
    ")\n",
    "\n",
    "x = [Value(random.uniform(0, 1)) for _ in range(10)]\n",
    "target = [Value(val.data) for val in x] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Loss: 0.000071819046\n",
      "VAE Loss: 0.000150542662\n",
      "VAE Loss: 0.000005891627\n",
      "VAE Loss: 0.000161628487\n",
      "VAE Loss: 0.000126828517\n",
      "VAE Loss: 0.000344047718\n",
      "VAE Loss: 0.000296484026\n",
      "VAE Loss: 0.000135095677\n",
      "VAE Loss: 0.000639330880\n",
      "VAE Loss: 0.000050342075\n",
      "VAE Loss: 0.000175332547\n",
      "VAE Loss: 0.000305858295\n",
      "VAE Loss: 0.000013133684\n",
      "VAE Loss: 0.000032099661\n",
      "VAE Loss: 0.000342029251\n",
      "VAE Loss: 0.000053625854\n",
      "VAE Loss: 0.000116598499\n",
      "VAE Loss: 0.000289217698\n",
      "VAE Loss: 0.000432347814\n",
      "VAE Loss: 0.000067622514\n",
      "VAE Loss: 0.000886883132\n",
      "VAE Loss: 0.000442482466\n",
      "VAE Loss: 0.000402963422\n",
      "VAE Loss: 0.000065284827\n",
      "VAE Loss: 0.000222492916\n",
      "VAE Loss: 0.000117668962\n",
      "VAE Loss: 0.000126229884\n",
      "VAE Loss: 0.000072999911\n",
      "VAE Loss: 0.000043667026\n",
      "VAE Loss: 0.000395761845\n",
      "VAE Loss: 0.000411684193\n",
      "VAE Loss: 0.000145923414\n",
      "VAE Loss: 0.000353336974\n",
      "VAE Loss: 0.000194288813\n",
      "VAE Loss: 0.000280931034\n",
      "VAE Loss: 0.000211145247\n",
      "VAE Loss: 0.000024468288\n",
      "VAE Loss: 0.000237040616\n",
      "VAE Loss: 0.000164150973\n",
      "VAE Loss: 0.000641300493\n",
      "VAE Loss: 0.000010561193\n",
      "VAE Loss: 0.000186238987\n",
      "VAE Loss: 0.000104333774\n",
      "VAE Loss: 0.000031181344\n",
      "VAE Loss: 0.000008965007\n",
      "VAE Loss: 0.000165521556\n",
      "VAE Loss: 0.000356398078\n",
      "VAE Loss: 0.000115231561\n",
      "VAE Loss: 0.000291788165\n",
      "VAE Loss: 0.000010353729\n",
      "VAE Loss: 0.000041884473\n",
      "VAE Loss: 0.000044744592\n",
      "VAE Loss: 0.000016027211\n",
      "VAE Loss: 0.000184442037\n",
      "VAE Loss: 0.000219832638\n",
      "VAE Loss: 0.000029750156\n",
      "VAE Loss: 0.000053004700\n",
      "VAE Loss: 0.000016389176\n",
      "VAE Loss: 0.000306604243\n",
      "VAE Loss: 0.000074617743\n",
      "VAE Loss: 0.000397220516\n",
      "VAE Loss: 0.001330820368\n",
      "VAE Loss: 0.000971367169\n",
      "VAE Loss: 0.000311612357\n",
      "VAE Loss: 0.000070250434\n",
      "VAE Loss: 0.000187834877\n",
      "VAE Loss: 0.000023076090\n",
      "VAE Loss: 0.000061136348\n",
      "VAE Loss: 0.000062816511\n",
      "VAE Loss: 0.000093338236\n",
      "VAE Loss: 0.000896039894\n",
      "VAE Loss: 0.000047438967\n",
      "VAE Loss: 0.000038174933\n",
      "VAE Loss: 0.000129307726\n",
      "VAE Loss: 0.000059592139\n",
      "VAE Loss: 0.000130901527\n",
      "VAE Loss: 0.000014767984\n",
      "VAE Loss: 0.000021933946\n",
      "VAE Loss: 0.000119744636\n",
      "VAE Loss: 0.000043618215\n",
      "VAE Loss: 0.000031514694\n",
      "VAE Loss: 0.000022357312\n",
      "VAE Loss: 0.000018860208\n",
      "VAE Loss: 0.000152144621\n",
      "VAE Loss: 0.000526929723\n",
      "VAE Loss: 0.000082595075\n",
      "VAE Loss: 0.000030118504\n",
      "VAE Loss: 0.000233547179\n",
      "VAE Loss: 0.000244973866\n",
      "VAE Loss: 0.000010617024\n",
      "VAE Loss: 0.000155317724\n",
      "VAE Loss: 0.000231084385\n",
      "VAE Loss: 0.000115537327\n",
      "VAE Loss: 0.000053103267\n",
      "VAE Loss: 0.000443303818\n",
      "VAE Loss: 0.000057602508\n",
      "VAE Loss: 0.000108174091\n",
      "VAE Loss: 0.000109287655\n",
      "VAE Loss: 0.000026684840\n",
      "VAE Loss: 0.000021913813\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(vae.parameters(), learning_rate=0.01)\n",
    "\n",
    "# Example input (list of Values)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass returns: (reconstruction, mu, log_var)\n",
    "    reconstruction, mu, log_var = vae(x)\n",
    "\n",
    "    # Ensure reconstruction is a list for loss computation\n",
    "    if not isinstance(reconstruction, list):\n",
    "        reconstruction = [reconstruction]\n",
    "\n",
    "    # Compute VAE loss\n",
    "     # Target is the input itself (autoencoder)\n",
    "    loss = vae_loss(reconstruction, target, mu, log_var, beta=1.0)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"VAE Loss: {loss.data:.12f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=-0.382192549612819, grad=0), Value(data=-0.9871766826472484, grad=0), Value(data=-1.3902898698504944, grad=0)]\n"
     ]
    }
   ],
   "source": [
    "compressed_rep = vae.reparameterize(mu=mu, log_var=log_var)\n",
    "\n",
    "print(compressed_rep)\n",
    "\n",
    "noise = np.random.normal(size=(10, len(compressed_rep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.34900593  1.64399474  0.73666641]\n",
      "[Value(data=0.985134910069068, grad=0), Value(data=0.8662643339110586, grad=0), Value(data=0.4134853652903019, grad=0), Value(data=0.7724076315383772, grad=0), Value(data=0.14152003318699363, grad=0), Value(data=0.962483158418475, grad=0), Value(data=0.9102564669814398, grad=0), Value(data=0.04960142846822449, grad=0), Value(data=0.9348712405861429, grad=0), Value(data=0.6578060778406193, grad=0)]\n",
      "\n",
      "[-0.37028047  1.65658418 -2.52636698]\n",
      "[Value(data=0.9802453679124944, grad=0), Value(data=0.8816975526759046, grad=0), Value(data=0.42204078890800867, grad=0), Value(data=0.8135362624646293, grad=0), Value(data=0.11790604489505518, grad=0), Value(data=0.9509466466917663, grad=0), Value(data=0.8906696096317596, grad=0), Value(data=0.022550282712508654, grad=0), Value(data=0.9449829281661893, grad=0), Value(data=0.6930531435532512, grad=0)]\n",
      "\n",
      "[-0.10328233  1.52524772 -2.14889216]\n",
      "[Value(data=0.976550135067789, grad=0), Value(data=0.8808769883850853, grad=0), Value(data=0.4254692475968874, grad=0), Value(data=0.8024383067411307, grad=0), Value(data=0.12699101113164285, grad=0), Value(data=0.9580433308490768, grad=0), Value(data=0.8935070468338175, grad=0), Value(data=0.02549179432679572, grad=0), Value(data=0.9419035486931474, grad=0), Value(data=0.6870192951711013, grad=0)]\n",
      "\n",
      "[ 0.17604668 -0.20215454 -1.82288062]\n",
      "[Value(data=0.9625592898169918, grad=0), Value(data=0.8767300050090365, grad=0), Value(data=0.438555330498905, grad=0), Value(data=0.7633257848010426, grad=0), Value(data=0.15547291385373838, grad=0), Value(data=0.9747419916733366, grad=0), Value(data=0.8899204285389432, grad=0), Value(data=0.02900453469458531, grad=0), Value(data=0.9336348188577964, grad=0), Value(data=0.6653907931401059, grad=0)]\n",
      "\n",
      "[-0.66378372 -0.40987059 -0.8993979 ]\n",
      "[Value(data=0.9677058486685561, grad=0), Value(data=0.8700500445987948, grad=0), Value(data=0.43196606031419243, grad=0), Value(data=0.7539447568049127, grad=0), Value(data=0.15767040764135534, grad=0), Value(data=0.9719171257231946, grad=0), Value(data=0.8931011968764588, grad=0), Value(data=0.03718567565698476, grad=0), Value(data=0.9322685970768921, grad=0), Value(data=0.6542993261663677, grad=0)]\n",
      "\n",
      "[-1.70539147 -0.44946472 -0.0789878 ]\n",
      "[Value(data=0.9758719997794056, grad=0), Value(data=0.8634639083459574, grad=0), Value(data=0.4224150926037996, grad=0), Value(data=0.7513292770330098, grad=0), Value(data=0.1541388626262061, grad=0), Value(data=0.9646583692516097, grad=0), Value(data=0.8956509951164239, grad=0), Value(data=0.04457223490643361, grad=0), Value(data=0.9326868132343341, grad=0), Value(data=0.6464116059455244, grad=0)]\n",
      "\n",
      "[ 1.30498119 -1.19714278  0.90382157]\n",
      "[Value(data=0.942778140032137, grad=0), Value(data=0.8680609735386217, grad=0), Value(data=0.45593966292691, grad=0), Value(data=0.690751923331709, grad=0), Value(data=0.21169951805740536, grad=0), Value(data=1.015851521165984, grad=0), Value(data=0.9079924332456935, grad=0), Value(data=0.05084696686919493, grad=0), Value(data=0.9143345964069387, grad=0), Value(data=0.6226868243896215, grad=0)]\n",
      "\n",
      "[ 0.99382126  0.46297868 -0.14344092]\n",
      "[Value(data=0.9574753493818193, grad=0), Value(data=0.8746442897705259, grad=0), Value(data=0.44278751844078446, grad=0), Value(data=0.740310059198325, grad=0), Value(data=0.17577269972008877, grad=0), Value(data=0.9937949441513187, grad=0), Value(data=0.9062842510812547, grad=0), Value(data=0.0415740932533051, grad=0), Value(data=0.9255647899460677, grad=0), Value(data=0.6518160619193454, grad=0)]\n",
      "\n",
      "[0.5348716  0.67064212 0.30695209]\n",
      "[Value(data=0.9624135355018315, grad=0), Value(data=0.8719125505554077, grad=0), Value(data=0.4372522966067415, grad=0), Value(data=0.7418090474362641, grad=0), Value(data=0.172394953545749, grad=0), Value(data=0.9898714320654984, grad=0), Value(data=0.9088788004029114, grad=0), Value(data=0.04540630535366443, grad=0), Value(data=0.9261273660760343, grad=0), Value(data=0.6496170225354103, grad=0)]\n",
      "\n",
      "[ 0.19501844  0.67287608 -0.84466035]\n",
      "[Value(data=0.9668504980519953, grad=0), Value(data=0.875093592116805, grad=0), Value(data=0.4337228734472464, grad=0), Value(data=0.763503443528717, grad=0), Value(data=0.15554260288774893, grad=0), Value(data=0.9768690873716418, grad=0), Value(data=0.9000319710884495, grad=0), Value(data=0.036345131495136185, grad=0), Value(data=0.932344244463406, grad=0), Value(data=0.6631911569147804, grad=0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in noise:\n",
    "    print(i)\n",
    "    print(vae.decode(i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to construct a training loop, I need a loss function\n",
    "# forward pass\n",
    "# backward pass\n",
    "# batches if im feeling cheeky\n",
    "# update (Stochastic gradient descent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder Loss: 0.084794596709\n",
      "AutoEncoder Loss: 0.074151194465\n",
      "AutoEncoder Loss: 0.071656995611\n",
      "AutoEncoder Loss: 0.068956169866\n",
      "AutoEncoder Loss: 0.062914463212\n",
      "AutoEncoder Loss: 0.061188982058\n",
      "AutoEncoder Loss: 0.056724166151\n",
      "AutoEncoder Loss: 0.055546029427\n",
      "AutoEncoder Loss: 0.051616357157\n",
      "AutoEncoder Loss: 0.053331085476\n",
      "AutoEncoder Loss: 0.047542098555\n",
      "AutoEncoder Loss: 0.045820759242\n",
      "AutoEncoder Loss: 0.043959753769\n",
      "AutoEncoder Loss: 0.042424076590\n",
      "AutoEncoder Loss: 0.040957481030\n",
      "AutoEncoder Loss: 0.039549070416\n",
      "AutoEncoder Loss: 0.038194264760\n",
      "AutoEncoder Loss: 0.036889289857\n",
      "AutoEncoder Loss: 0.035630951472\n",
      "AutoEncoder Loss: 0.034416540016\n",
      "AutoEncoder Loss: 0.033243751528\n",
      "AutoEncoder Loss: 0.032110619659\n",
      "AutoEncoder Loss: 0.031015457334\n",
      "AutoEncoder Loss: 0.030036833850\n",
      "AutoEncoder Loss: 0.030268856344\n",
      "AutoEncoder Loss: 0.028138089311\n",
      "AutoEncoder Loss: 0.027056161712\n",
      "AutoEncoder Loss: 0.026132461550\n",
      "AutoEncoder Loss: 0.025240351394\n",
      "AutoEncoder Loss: 0.024378528345\n",
      "AutoEncoder Loss: 0.023546294053\n",
      "AutoEncoder Loss: 0.022742976432\n",
      "AutoEncoder Loss: 0.022032091586\n",
      "AutoEncoder Loss: 0.021949050763\n",
      "AutoEncoder Loss: 0.020623068861\n",
      "AutoEncoder Loss: 0.019856706397\n",
      "AutoEncoder Loss: 0.019186135349\n",
      "AutoEncoder Loss: 0.018540759804\n",
      "AutoEncoder Loss: 0.017919744166\n",
      "AutoEncoder Loss: 0.017322443042\n",
      "AutoEncoder Loss: 0.016748206498\n",
      "AutoEncoder Loss: 0.016234549664\n",
      "AutoEncoder Loss: 0.016012907577\n",
      "AutoEncoder Loss: 0.015226079319\n",
      "AutoEncoder Loss: 0.014725816089\n",
      "AutoEncoder Loss: 0.014253522903\n",
      "AutoEncoder Loss: 0.013800741560\n",
      "AutoEncoder Loss: 0.013366552164\n",
      "AutoEncoder Loss: 0.012950303399\n",
      "AutoEncoder Loss: 0.012551362540\n",
      "AutoEncoder Loss: 0.012169106719\n",
      "AutoEncoder Loss: 0.011802923796\n",
      "AutoEncoder Loss: 0.011452213553\n",
      "AutoEncoder Loss: 0.011116388774\n",
      "AutoEncoder Loss: 0.010794876191\n",
      "AutoEncoder Loss: 0.010487117280\n",
      "AutoEncoder Loss: 0.010192568927\n",
      "AutoEncoder Loss: 0.009910703957\n",
      "AutoEncoder Loss: 0.009641011552\n",
      "AutoEncoder Loss: 0.009382997533\n",
      "AutoEncoder Loss: 0.009136184561\n",
      "AutoEncoder Loss: 0.008900112214\n",
      "AutoEncoder Loss: 0.008693098330\n",
      "AutoEncoder Loss: 0.008562886445\n",
      "AutoEncoder Loss: 0.008288958891\n",
      "AutoEncoder Loss: 0.008075360881\n",
      "AutoEncoder Loss: 0.007881950949\n",
      "AutoEncoder Loss: 0.007699619259\n",
      "AutoEncoder Loss: 0.007525994447\n",
      "AutoEncoder Loss: 0.007360251173\n",
      "AutoEncoder Loss: 0.007201945213\n",
      "AutoEncoder Loss: 0.007050738795\n",
      "AutoEncoder Loss: 0.006906332414\n",
      "AutoEncoder Loss: 0.006768446864\n",
      "AutoEncoder Loss: 0.006636818038\n",
      "AutoEncoder Loss: 0.006511195134\n",
      "AutoEncoder Loss: 0.006391339807\n",
      "AutoEncoder Loss: 0.006277025591\n",
      "AutoEncoder Loss: 0.006168037420\n",
      "AutoEncoder Loss: 0.006064171175\n",
      "AutoEncoder Loss: 0.005965233251\n",
      "AutoEncoder Loss: 0.005871040139\n",
      "AutoEncoder Loss: 0.005781418020\n",
      "AutoEncoder Loss: 0.005696202376\n",
      "AutoEncoder Loss: 0.005615237609\n",
      "AutoEncoder Loss: 0.005538376679\n",
      "AutoEncoder Loss: 0.005465480754\n",
      "AutoEncoder Loss: 0.005396418880\n",
      "AutoEncoder Loss: 0.005331067657\n",
      "AutoEncoder Loss: 0.005269310941\n",
      "AutoEncoder Loss: 0.005211039552\n",
      "AutoEncoder Loss: 0.005156151003\n",
      "AutoEncoder Loss: 0.005104549241\n",
      "AutoEncoder Loss: 0.005056144403\n",
      "AutoEncoder Loss: 0.005010852587\n",
      "AutoEncoder Loss: 0.004968595636\n",
      "AutoEncoder Loss: 0.004929300938\n",
      "AutoEncoder Loss: 0.004892901236\n",
      "AutoEncoder Loss: 0.004859334458\n",
      "AutoEncoder Loss: 0.004828543548\n",
      "AutoEncoder Loss: 0.004800476325\n",
      "AutoEncoder Loss: 0.004775085339\n",
      "AutoEncoder Loss: 0.004752327751\n",
      "AutoEncoder Loss: 0.004732165220\n",
      "AutoEncoder Loss: 0.004714563800\n",
      "AutoEncoder Loss: 0.004699493851\n",
      "AutoEncoder Loss: 0.004686929958\n",
      "AutoEncoder Loss: 0.004676850868\n",
      "AutoEncoder Loss: 0.004669239429\n",
      "AutoEncoder Loss: 0.004664082544\n",
      "AutoEncoder Loss: 0.004661371135\n",
      "AutoEncoder Loss: 0.004661100121\n",
      "AutoEncoder Loss: 0.004661097411\n",
      "AutoEncoder Loss: 0.004661094701\n",
      "AutoEncoder Loss: 0.004661091991\n",
      "AutoEncoder Loss: 0.004661089281\n",
      "AutoEncoder Loss: 0.004661086571\n",
      "AutoEncoder Loss: 0.004661083861\n",
      "AutoEncoder Loss: 0.004661081151\n",
      "AutoEncoder Loss: 0.004661078441\n",
      "AutoEncoder Loss: 0.004661075731\n",
      "AutoEncoder Loss: 0.004661073021\n",
      "AutoEncoder Loss: 0.004661070311\n",
      "AutoEncoder Loss: 0.004661067601\n",
      "AutoEncoder Loss: 0.004661064891\n",
      "AutoEncoder Loss: 0.004661062181\n",
      "AutoEncoder Loss: 0.004661059471\n",
      "AutoEncoder Loss: 0.004661056761\n",
      "AutoEncoder Loss: 0.004661054051\n",
      "AutoEncoder Loss: 0.004661051341\n",
      "AutoEncoder Loss: 0.004661048631\n",
      "AutoEncoder Loss: 0.004661045921\n",
      "AutoEncoder Loss: 0.004661043211\n",
      "AutoEncoder Loss: 0.004661040501\n",
      "AutoEncoder Loss: 0.004661037791\n",
      "AutoEncoder Loss: 0.004661035081\n",
      "AutoEncoder Loss: 0.004661032371\n",
      "AutoEncoder Loss: 0.004661029661\n",
      "AutoEncoder Loss: 0.004661026951\n",
      "AutoEncoder Loss: 0.004661024241\n",
      "AutoEncoder Loss: 0.004661021531\n",
      "AutoEncoder Loss: 0.004661018822\n",
      "AutoEncoder Loss: 0.004661016112\n",
      "AutoEncoder Loss: 0.004661013402\n",
      "AutoEncoder Loss: 0.004661010692\n",
      "AutoEncoder Loss: 0.004661007982\n",
      "AutoEncoder Loss: 0.004661005272\n",
      "AutoEncoder Loss: 0.004661002562\n",
      "AutoEncoder Loss: 0.004660999852\n",
      "AutoEncoder Loss: 0.004660997142\n",
      "AutoEncoder Loss: 0.004660994432\n",
      "AutoEncoder Loss: 0.004660991722\n",
      "AutoEncoder Loss: 0.004660989012\n",
      "AutoEncoder Loss: 0.004660986302\n",
      "AutoEncoder Loss: 0.004660983592\n",
      "AutoEncoder Loss: 0.004660980882\n",
      "AutoEncoder Loss: 0.004660978172\n",
      "AutoEncoder Loss: 0.004660975462\n",
      "AutoEncoder Loss: 0.004660972752\n",
      "AutoEncoder Loss: 0.004660970042\n",
      "AutoEncoder Loss: 0.004660967332\n",
      "AutoEncoder Loss: 0.004660964622\n",
      "AutoEncoder Loss: 0.004660961913\n",
      "AutoEncoder Loss: 0.004660959203\n",
      "AutoEncoder Loss: 0.004660956493\n",
      "AutoEncoder Loss: 0.004660953783\n",
      "AutoEncoder Loss: 0.004660951073\n",
      "AutoEncoder Loss: 0.004660948363\n",
      "AutoEncoder Loss: 0.004660945653\n",
      "AutoEncoder Loss: 0.004660942943\n",
      "AutoEncoder Loss: 0.004660940233\n",
      "AutoEncoder Loss: 0.004660937523\n",
      "AutoEncoder Loss: 0.004660934813\n",
      "AutoEncoder Loss: 0.004660932103\n",
      "AutoEncoder Loss: 0.004660929393\n",
      "AutoEncoder Loss: 0.004660926684\n",
      "AutoEncoder Loss: 0.004660923974\n",
      "AutoEncoder Loss: 0.004660921264\n",
      "AutoEncoder Loss: 0.004660918554\n",
      "AutoEncoder Loss: 0.004660915844\n",
      "AutoEncoder Loss: 0.004660913134\n",
      "AutoEncoder Loss: 0.004660910424\n",
      "AutoEncoder Loss: 0.004660907714\n",
      "AutoEncoder Loss: 0.004660905004\n",
      "AutoEncoder Loss: 0.004660902294\n",
      "AutoEncoder Loss: 0.004660899584\n",
      "AutoEncoder Loss: 0.004660896875\n",
      "AutoEncoder Loss: 0.004660894165\n",
      "AutoEncoder Loss: 0.004660891455\n",
      "AutoEncoder Loss: 0.004660888745\n",
      "AutoEncoder Loss: 0.004660886035\n",
      "AutoEncoder Loss: 0.004660883325\n",
      "AutoEncoder Loss: 0.004660880615\n",
      "AutoEncoder Loss: 0.004660877905\n",
      "AutoEncoder Loss: 0.004660875195\n",
      "AutoEncoder Loss: 0.004660872486\n",
      "AutoEncoder Loss: 0.004660869776\n",
      "AutoEncoder Loss: 0.004660867066\n",
      "AutoEncoder Loss: 0.004660864356\n",
      "AutoEncoder Loss: 0.004660861646\n",
      "AutoEncoder Loss: 0.004660858936\n",
      "AutoEncoder Loss: 0.004660856226\n",
      "AutoEncoder Loss: 0.004660853516\n",
      "AutoEncoder Loss: 0.004660850807\n",
      "AutoEncoder Loss: 0.004660848097\n",
      "AutoEncoder Loss: 0.004660845387\n",
      "AutoEncoder Loss: 0.004660842677\n",
      "AutoEncoder Loss: 0.004660839967\n",
      "AutoEncoder Loss: 0.004660837257\n",
      "AutoEncoder Loss: 0.004660834547\n",
      "AutoEncoder Loss: 0.004660831838\n",
      "AutoEncoder Loss: 0.004660829128\n",
      "AutoEncoder Loss: 0.004660826418\n",
      "AutoEncoder Loss: 0.004660823708\n",
      "AutoEncoder Loss: 0.004660820998\n",
      "AutoEncoder Loss: 0.004660818288\n",
      "AutoEncoder Loss: 0.004660815578\n",
      "AutoEncoder Loss: 0.004660812869\n",
      "AutoEncoder Loss: 0.004660810159\n",
      "AutoEncoder Loss: 0.004660807449\n",
      "AutoEncoder Loss: 0.004660804739\n",
      "AutoEncoder Loss: 0.004660802029\n",
      "AutoEncoder Loss: 0.004660799319\n",
      "AutoEncoder Loss: 0.004660796610\n",
      "AutoEncoder Loss: 0.004660793900\n",
      "AutoEncoder Loss: 0.004660791190\n",
      "AutoEncoder Loss: 0.004660788480\n",
      "AutoEncoder Loss: 0.004660785770\n",
      "AutoEncoder Loss: 0.004660783060\n",
      "AutoEncoder Loss: 0.004660780351\n",
      "AutoEncoder Loss: 0.004660777641\n",
      "AutoEncoder Loss: 0.004660774931\n",
      "AutoEncoder Loss: 0.004660772221\n",
      "AutoEncoder Loss: 0.004660769511\n",
      "AutoEncoder Loss: 0.004660766801\n",
      "AutoEncoder Loss: 0.004660764092\n",
      "AutoEncoder Loss: 0.004660761382\n",
      "AutoEncoder Loss: 0.004660758672\n",
      "AutoEncoder Loss: 0.004660755962\n",
      "AutoEncoder Loss: 0.004660753252\n",
      "AutoEncoder Loss: 0.004660750543\n",
      "AutoEncoder Loss: 0.004660747833\n",
      "AutoEncoder Loss: 0.004660745123\n",
      "AutoEncoder Loss: 0.004660742413\n",
      "AutoEncoder Loss: 0.004660739703\n",
      "AutoEncoder Loss: 0.004660736994\n",
      "AutoEncoder Loss: 0.004660734284\n",
      "AutoEncoder Loss: 0.004660731574\n",
      "AutoEncoder Loss: 0.004660728864\n",
      "AutoEncoder Loss: 0.004660726154\n",
      "AutoEncoder Loss: 0.004660723445\n",
      "AutoEncoder Loss: 0.004660720735\n",
      "AutoEncoder Loss: 0.004660718025\n",
      "AutoEncoder Loss: 0.004660715315\n",
      "AutoEncoder Loss: 0.004660712605\n",
      "AutoEncoder Loss: 0.004660709896\n",
      "AutoEncoder Loss: 0.004660707186\n",
      "AutoEncoder Loss: 0.004660704476\n",
      "AutoEncoder Loss: 0.004660701766\n",
      "AutoEncoder Loss: 0.004660699057\n",
      "AutoEncoder Loss: 0.004660696347\n",
      "AutoEncoder Loss: 0.004660693637\n",
      "AutoEncoder Loss: 0.004660690927\n",
      "AutoEncoder Loss: 0.004660688217\n",
      "AutoEncoder Loss: 0.004660685508\n",
      "AutoEncoder Loss: 0.004660682798\n",
      "AutoEncoder Loss: 0.004660680088\n",
      "AutoEncoder Loss: 0.004660677378\n",
      "AutoEncoder Loss: 0.004660674669\n",
      "AutoEncoder Loss: 0.004660671959\n",
      "AutoEncoder Loss: 0.004660669249\n",
      "AutoEncoder Loss: 0.004660666539\n",
      "AutoEncoder Loss: 0.004660663830\n",
      "AutoEncoder Loss: 0.004660661120\n",
      "AutoEncoder Loss: 0.004660658410\n",
      "AutoEncoder Loss: 0.004660655700\n",
      "AutoEncoder Loss: 0.004660652991\n",
      "AutoEncoder Loss: 0.004660650281\n",
      "AutoEncoder Loss: 0.004660647571\n",
      "AutoEncoder Loss: 0.004660644861\n",
      "AutoEncoder Loss: 0.004660642152\n",
      "AutoEncoder Loss: 0.004660639442\n",
      "AutoEncoder Loss: 0.004660636732\n",
      "AutoEncoder Loss: 0.004660634022\n",
      "AutoEncoder Loss: 0.004660631313\n",
      "AutoEncoder Loss: 0.004660628603\n",
      "AutoEncoder Loss: 0.004660625893\n",
      "AutoEncoder Loss: 0.004660623183\n",
      "AutoEncoder Loss: 0.004660620474\n",
      "AutoEncoder Loss: 0.004660617764\n",
      "AutoEncoder Loss: 0.004660615054\n",
      "AutoEncoder Loss: 0.004660612345\n",
      "AutoEncoder Loss: 0.004660609635\n",
      "AutoEncoder Loss: 0.004660606925\n",
      "AutoEncoder Loss: 0.004660604215\n",
      "AutoEncoder Loss: 0.004660601506\n",
      "AutoEncoder Loss: 0.004660598796\n",
      "AutoEncoder Loss: 0.004660596086\n",
      "AutoEncoder Loss: 0.004660593376\n",
      "AutoEncoder Loss: 0.004660590667\n",
      "AutoEncoder Loss: 0.004660587957\n",
      "AutoEncoder Loss: 0.004660585247\n",
      "AutoEncoder Loss: 0.004660582538\n",
      "AutoEncoder Loss: 0.004660579828\n",
      "AutoEncoder Loss: 0.004660577118\n",
      "AutoEncoder Loss: 0.004660574409\n",
      "AutoEncoder Loss: 0.004660571699\n",
      "AutoEncoder Loss: 0.004660568989\n",
      "AutoEncoder Loss: 0.004660566279\n",
      "AutoEncoder Loss: 0.004660563570\n",
      "AutoEncoder Loss: 0.004660560860\n",
      "AutoEncoder Loss: 0.004660558150\n",
      "AutoEncoder Loss: 0.004660555441\n",
      "AutoEncoder Loss: 0.004660552731\n",
      "AutoEncoder Loss: 0.004660550021\n",
      "AutoEncoder Loss: 0.004660547312\n",
      "AutoEncoder Loss: 0.004660544602\n",
      "AutoEncoder Loss: 0.004660541892\n",
      "AutoEncoder Loss: 0.004660539183\n",
      "AutoEncoder Loss: 0.004660536473\n",
      "AutoEncoder Loss: 0.004660533763\n",
      "AutoEncoder Loss: 0.004660531054\n",
      "AutoEncoder Loss: 0.004660528344\n",
      "AutoEncoder Loss: 0.004660525634\n",
      "AutoEncoder Loss: 0.004660522925\n",
      "AutoEncoder Loss: 0.004660520215\n",
      "AutoEncoder Loss: 0.004660517505\n",
      "AutoEncoder Loss: 0.004660514796\n",
      "AutoEncoder Loss: 0.004660512086\n",
      "AutoEncoder Loss: 0.004660509376\n",
      "AutoEncoder Loss: 0.004660506667\n",
      "AutoEncoder Loss: 0.004660503957\n",
      "AutoEncoder Loss: 0.004660501247\n",
      "AutoEncoder Loss: 0.004660498538\n",
      "AutoEncoder Loss: 0.004660495828\n",
      "AutoEncoder Loss: 0.004660493118\n",
      "AutoEncoder Loss: 0.004660490409\n",
      "AutoEncoder Loss: 0.004660487699\n",
      "AutoEncoder Loss: 0.004660484989\n",
      "AutoEncoder Loss: 0.004660482280\n",
      "AutoEncoder Loss: 0.004660479570\n",
      "AutoEncoder Loss: 0.004660476860\n",
      "AutoEncoder Loss: 0.004660474151\n",
      "AutoEncoder Loss: 0.004660471441\n",
      "AutoEncoder Loss: 0.004660468731\n",
      "AutoEncoder Loss: 0.004660466022\n",
      "AutoEncoder Loss: 0.004660463312\n",
      "AutoEncoder Loss: 0.004660460603\n",
      "AutoEncoder Loss: 0.004660457893\n",
      "AutoEncoder Loss: 0.004660455183\n",
      "AutoEncoder Loss: 0.004660452474\n",
      "AutoEncoder Loss: 0.004660449764\n",
      "AutoEncoder Loss: 0.004660447054\n",
      "AutoEncoder Loss: 0.004660444345\n",
      "AutoEncoder Loss: 0.004660441635\n",
      "AutoEncoder Loss: 0.004660438926\n",
      "AutoEncoder Loss: 0.004660436216\n",
      "AutoEncoder Loss: 0.004660433506\n",
      "AutoEncoder Loss: 0.004660430797\n",
      "AutoEncoder Loss: 0.004660428087\n",
      "AutoEncoder Loss: 0.004660425377\n",
      "AutoEncoder Loss: 0.004660422668\n",
      "AutoEncoder Loss: 0.004660419958\n",
      "AutoEncoder Loss: 0.004660417249\n",
      "AutoEncoder Loss: 0.004660414539\n",
      "AutoEncoder Loss: 0.004660411829\n",
      "AutoEncoder Loss: 0.004660409120\n",
      "AutoEncoder Loss: 0.004660406410\n",
      "AutoEncoder Loss: 0.004660403701\n",
      "AutoEncoder Loss: 0.004660400991\n",
      "AutoEncoder Loss: 0.004660398281\n",
      "AutoEncoder Loss: 0.004660395572\n",
      "AutoEncoder Loss: 0.004660392862\n",
      "AutoEncoder Loss: 0.004660390153\n",
      "AutoEncoder Loss: 0.004660387443\n",
      "AutoEncoder Loss: 0.004660384733\n",
      "AutoEncoder Loss: 0.004660382024\n",
      "AutoEncoder Loss: 0.004660379314\n",
      "AutoEncoder Loss: 0.004660376605\n",
      "AutoEncoder Loss: 0.004660373895\n",
      "AutoEncoder Loss: 0.004660371186\n",
      "AutoEncoder Loss: 0.004660368476\n",
      "AutoEncoder Loss: 0.004660365766\n",
      "AutoEncoder Loss: 0.004660363057\n",
      "AutoEncoder Loss: 0.004660360347\n",
      "AutoEncoder Loss: 0.004660357638\n",
      "AutoEncoder Loss: 0.004660354928\n",
      "AutoEncoder Loss: 0.004660352219\n",
      "AutoEncoder Loss: 0.004660349509\n",
      "AutoEncoder Loss: 0.004660346799\n",
      "AutoEncoder Loss: 0.004660344090\n",
      "AutoEncoder Loss: 0.004660341380\n",
      "AutoEncoder Loss: 0.004660338671\n",
      "AutoEncoder Loss: 0.004660335961\n",
      "AutoEncoder Loss: 0.004660333252\n",
      "AutoEncoder Loss: 0.004660330542\n",
      "AutoEncoder Loss: 0.004660327832\n",
      "AutoEncoder Loss: 0.004660325123\n",
      "AutoEncoder Loss: 0.004660322413\n",
      "AutoEncoder Loss: 0.004660319704\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage of VariationalAutoEncoder\n",
    "\n",
    "# Create a simple VAE\n",
    "auto = AutoEncoder(\n",
    "    in_embeds=10,        # Input dimension\n",
    "    hidden_layers=[6],   # Number of hidden layers\n",
    "    latent_dim=3,        # Latent space dimension\n",
    "    act_func=Value.sigmoid,       # Activation function (None uses default leaky_relu)\n",
    "    tied=True           # Whether to use tied weights\n",
    ")\n",
    "\n",
    "optimizer = SGD(auto.parameters())\n",
    "\n",
    "# Example input (list of Values)\n",
    "x = [Value(random.uniform(0, 1)) for _ in range(10)]\n",
    "target = [Value(val.data) for val in x]  # Target is the input itself (autoencoder)\n",
    "for i in range(400):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass returns: (reconstruction, mu, log_var)\n",
    "    reconstruction = auto(x)\n",
    "\n",
    "    # Ensure reconstruction is a list for loss computation\n",
    "\n",
    "    # Compute VAE loss\n",
    "    \n",
    "    loss = mean_squared_error(reconstruction, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"AutoEncoder Loss: {loss.data:.12f}\")\n",
    "\n",
    "\n",
    "# Note: To use VAE with the Trainer class, you'll need to create a wrapper\n",
    "# or modify the training loop to handle the (reconstruction, mu, log_var) output\n",
    "# and use vae_loss instead of the standard MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46287292963696824, 0.792207447624853, 0.5478360993824788, 0.19592486435858503, 0.263906575657317, 0.4867360316982996, 0.07709150764517192, 0.22548110069400118, 0.25902858125933703, 0.5027308595138207]\n",
      "[0.44718868813872376, 0.7675428203120515, 0.5354794785389686, 0.3138074764386894, 0.21515935256280197, 0.46354223697132085, 0.19886951685005566, 0.34112288809038765, 0.2540205142411105, 0.5263450571596118]\n"
     ]
    }
   ],
   "source": [
    "print([j.data for j in x])\n",
    "print([i.data for i in reconstruction])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "t = MinMaxScaler()\n",
    "t.fit(X)\n",
    "X_train = t.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "Iteration : 0, AutoEncoder Loss: 0.243101277896\n",
      "(64,)\n",
      "Iteration : 1, AutoEncoder Loss: 0.209554474847\n",
      "(64,)\n",
      "Iteration : 2, AutoEncoder Loss: 0.195153729115\n",
      "(64,)\n",
      "Iteration : 3, AutoEncoder Loss: 0.191140189781\n",
      "(64,)\n",
      "Iteration : 4, AutoEncoder Loss: 0.188718272333\n",
      "(64,)\n",
      "Iteration : 5, AutoEncoder Loss: 0.193006403770\n",
      "(64,)\n",
      "Iteration : 6, AutoEncoder Loss: 0.178557645838\n",
      "(64,)\n",
      "Iteration : 7, AutoEncoder Loss: 0.166324824406\n",
      "(64,)\n",
      "Iteration : 8, AutoEncoder Loss: 0.157395371741\n",
      "(64,)\n",
      "Iteration : 9, AutoEncoder Loss: 0.155887547776\n",
      "(64,)\n",
      "Iteration : 10, AutoEncoder Loss: 0.155665978985\n",
      "(64,)\n",
      "Iteration : 11, AutoEncoder Loss: 0.155607102259\n",
      "(64,)\n",
      "Iteration : 12, AutoEncoder Loss: 0.156550397051\n",
      "(64,)\n",
      "Iteration : 13, AutoEncoder Loss: 0.155992145061\n",
      "(64,)\n",
      "Iteration : 14, AutoEncoder Loss: 0.155478549465\n",
      "(64,)\n",
      "Iteration : 15, AutoEncoder Loss: 0.153316993607\n",
      "(64,)\n",
      "Iteration : 16, AutoEncoder Loss: 0.151037202368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass returns: (reconstruction, mu, log_var)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# reconstruction = [list(map(Value, xrow)) for xrow in X_train]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(Value, X_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mauto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(scores)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure reconstruction is a list for loss computation\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute VAE loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 151\u001b[0m, in \u001b[0;36mAutoEncoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    150\u001b[0m     compressed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m--> 151\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 119\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# For each output neuron (j), sum the weighted inputs.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# The weight connecting input `i` to output `j` is the same as the weight connecting\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# input `j` of the encoder layer to output `i`.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_to_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_to_layer\u001b[38;5;241m.\u001b[39mneurons[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mw[j] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Apply activation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mactivate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mnonlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# For each output neuron (j), sum the weighted inputs.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# The weight connecting input `i` to output `j` is the same as the weight connecting\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# input `j` of the encoder layer to output `i`.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_to_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_to_layer\u001b[38;5;241m.\u001b[39mneurons[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mw[j] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Apply activation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mactivate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mnonlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mValue.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     23\u001b[0m     out\u001b[38;5;241m.\u001b[39m_backward \u001b[38;5;241m=\u001b[39m _backward\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     28\u001b[0m     other \u001b[38;5;241m=\u001b[39m other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Value) \u001b[38;5;28;01melse\u001b[39;00m Value(other)\n\u001b[1;32m     29\u001b[0m     out \u001b[38;5;241m=\u001b[39m Value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m other\u001b[38;5;241m.\u001b[39mdata, (\u001b[38;5;28mself\u001b[39m, other), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage of VariationalAutoEncoder\n",
    "\n",
    "# Create a simple VAE\n",
    "auto = AutoEncoder(\n",
    "    in_embeds=64,        # Input dimension\n",
    "    n_hidden_layers=1,   # Number of hidden layers\n",
    "    compressed=16,        # Latent space dimension\n",
    "    act_func=Value.sigmoid,       # Activation function (None uses default leaky_relu)\n",
    "    tied=True           # Whether to use tied weights\n",
    ")\n",
    "\n",
    "optimizer = SGD(auto.parameters())\n",
    "\n",
    "# Example input (list of Values)\n",
    "target = X_train[0]\n",
    "target = list(map(Value, target)) # Target is the input itself (autoencoder)\n",
    "for i in range(150):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass returns: (reconstruction, mu, log_var)\n",
    "    \n",
    "    # reconstruction = [list(map(Value, xrow)) for xrow in X_train]\n",
    "    \n",
    "    reconstruction = map(Value, X_train[0])\n",
    "    \n",
    "    scores = auto(reconstruction)\n",
    "    # Ensure reconstruction is a list for loss computation\n",
    "\n",
    "    # Compute VAE loss\n",
    "    \n",
    "    loss = mean_squared_error(target, scores)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Iteration : {i}, AutoEncoder Loss: {total_loss.data:.12f}\")\n",
    "    print(auto.parameters()[:30])\n",
    "\n",
    "\n",
    "# Note: To use VAE with the Trainer class, you'll need to create a wrapper\n",
    "# or modify the training loop to handle the (reconstruction, mu, log_var) output\n",
    "# and use vae_loss instead of the standard MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_digits()\n",
    "\n",
    "mean = sum(sum(X.data)) / (len(X.data) * 64)\n",
    "\n",
    "std = (sum(sum((X.data - mean)**2)) / (len(X.data) * 64))**0.5\n",
    "\n",
    "def standardize_transform(x):\n",
    "    \"\"\"\n",
    "    Standardizes a given tensor using the precomputed MNIST mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor to standardize.\n",
    "\n",
    "    Returns:\n",
    "        Standardized tensor.\n",
    "    \"\"\"\n",
    "    return (x - mean) / std\n",
    "\n",
    "def standardize_inverse(x):\n",
    "    \"\"\"\n",
    "    Reverses the standardization on a tensor using the precomputed MNIST mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor to de-standardize.\n",
    "\n",
    "    Returns:\n",
    "        De-standardized tensor.\n",
    "    \"\"\"\n",
    "    return x * std + mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = AutoEncoder(in_embeds=64, hidden_layers=[32], latent_dim=8, act_func=Value.sigmoid, tied=False) \n",
    "optimizer = SGD(auto.parameters(), learning_rate=0.5)\n",
    "\n",
    "train = X_train_values[:10]\n",
    "target = target_values[:10]\n",
    "historical_loss = []\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    # reconstruction = map(Value, single_test)\n",
    "    \n",
    "    # score = auto(reconstruction)\n",
    "    \n",
    "    scores_values = [auto(x) for x in train]\n",
    "    \n",
    "    # loss = mean_squared_error(target, score)\n",
    "    \n",
    "    loss = sum([mean_squared_error(targ, pred) for targ, pred in zip(target, scores_values)])\n",
    "    historical_loss.append(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Iteration : {i}, AutoEncoder Loss: {loss.data:.12f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_micrograd_latent_space(model, data, targets):\n",
    "    \"\"\"\n",
    "    Displays the distribution of data in the Autoencoder's latent space \n",
    "    using the first two dimensions of Z, colored by target labels.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained micrograd autoencoder model with an .encode() method.\n",
    "        data: The input data (numpy array) to process.\n",
    "        targets: The labels/targets for coloring the points.\n",
    "    \"\"\"\n",
    "    # Note: micrograd models don't typically have a formal .eval() mode, \n",
    "    # but the forward pass for inference is the same.\n",
    "    \n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "    COLORS = ['red', 'blue', 'green', 'yellow', 'orange', 'cyan', 'gray', 'black', 'purple', 'pink']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        img_data = data[i]\n",
    "        label = targets[i]\n",
    "        \n",
    "        # Convert numpy array row to a list of micrograd Value objects\n",
    "        img_values = list(map(Value, img_data))\n",
    "        \n",
    "        # Get the latent representation (requires model.encode() method)\n",
    "        z_values = model.encode(img_values)\n",
    "        \n",
    "        # Convert micrograd Values to a numpy array of floats\n",
    "        z_numpy = np.array([v.data for v in z_values])\n",
    "        \n",
    "        latent_vectors.append(z_numpy)\n",
    "        labels.append(label)\n",
    "\n",
    "    latent_vectors = np.array(latent_vectors)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    unique_labels = sorted(list(set(labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        indices = np.where(labels == label)\n",
    "        z_points = latent_vectors[indices]\n",
    "        \n",
    "        # Plot the first two dimensions (Z1 vs Z2)\n",
    "        if z_points.shape[1] >= 2:\n",
    "            plt.scatter(\n",
    "                z_points[:, 0], \n",
    "                z_points[:, 1], \n",
    "                color=COLORS[i % len(COLORS)], \n",
    "                label=str(label), \n",
    "                alpha=0.7\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Cannot visualize latent space: latent_dim is {z_points.shape[1]}, need at least 2 dimensions to plot Z1 vs Z2.\")\n",
    "            return\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title('Micrograd AutoEncoder Latent Space Distribution (Z1 vs Z2)')\n",
    "    plt.xlabel('Z1')\n",
    "    plt.ylabel('Z2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_micrograd_latent_space(\n",
    "    model=auto,\n",
    "    data=multi_test,       # Use the scaled data\n",
    "    targets=X.images[:10]    # Use the actual digit labels for coloring\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
