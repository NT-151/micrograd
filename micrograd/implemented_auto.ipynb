{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op  # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)\n",
    "                          ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # fix dead neuron problem\n",
    "    def leaky_relu(self):\n",
    "        out = Value(self.data * 0.01 if self.data <\n",
    "                    0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            local_grad = 1.0 if self.data > 0 else 0.01\n",
    "            self.grad += local_grad * out.grad \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "\n",
    "        out = Value(math.log(self.data), (self, ), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        t = 1 / (1 + (np.exp(-x)))\n",
    "\n",
    "        out = Value(t, (self, ), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data * (1 - out.data)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other.data\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self.data <= other.data\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.data > other.data\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.data < other.data\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):  # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):  # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def layers(self):\n",
    "        return []\n",
    "\n",
    "    def summary(self):\n",
    "        return f\"{len(self.layers())} layers, {len(self.parameters())} parameters\"\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    # I want to introduce weight sharing, which means I need to be able to\n",
    "    # initialise a neuron with pre defined weights, but leave the bias?\n",
    "\n",
    "    def __init__(self, nin, nonlin=True, **kwargs):\n",
    "        tied_weights = kwargs.get('tied_weights', None)\n",
    "        self.w = tied_weights if tied_weights is not None else [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "        self.nonlin = nonlin\n",
    "        self.activate = kwargs.get('activate', None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if isinstance(x, (Value, float, int)):\n",
    "            # This is for a single input, likely at the start of a layer\n",
    "            act = (self.w[0] * x) + self.b\n",
    "        else:\n",
    "            act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        if self.activate and self.nonlin == False:\n",
    "            return self.activate(act)\n",
    "        else:\n",
    "            return act.leaky_relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b] if isinstance(self.w[0], Value) else [p for w_list in self.w for p in w_list] + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else '{self.activate}'}Neuron({len(self.w)})\"\n",
    "\n",
    "\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, tied_to_layer=None, **kwargs):\n",
    "        if tied_to_layer is None:\n",
    "            # Standard Layer initialization\n",
    "            self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "        else:\n",
    "            # Tied Layer initialization\n",
    "            # The weights for this layer are the transpose of the tied_to_layer's weights\n",
    "            # This requires careful construction.\n",
    "            # Number of inputs for this layer = number of outputs of the tied layer\n",
    "            # Number of outputs for this layer = number of inputs of the tied layer\n",
    "            self.tied_to_layer = tied_to_layer\n",
    "            self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            # The weights are conceptually transposed.\n",
    "            # So, the output of a neuron is sum(w_ji * x_i), which means summing over the neurons of the previous layer.\n",
    "            # This is hard to do cleanly with the current structure.\n",
    "            # The simpler approach is to loop manually.\n",
    "            out = []\n",
    "            for j in range(len(self.neurons)):\n",
    "                # The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\n",
    "                # For each output neuron (j), sum the weighted inputs.\n",
    "                # The weight connecting input `i` to output `j` is the same as the weight connecting\n",
    "                # input `j` of the encoder layer to output `i`.\n",
    "                act = sum(self.tied_to_layer.neurons[i].w[j] * x[i] for i in range(len(x))) + self.neurons[j].b if isinstance(x, list) else self.tied_to_layer.neurons[0].w[j] * x + self.neurons[j].b\n",
    "                \n",
    "                # Apply activation\n",
    "                if self.neurons[j].activate and self.neurons[j].nonlin is False:\n",
    "                    act = self.neurons[j].activate(act)\n",
    "                else:\n",
    "                    act = act.relu() if self.neurons[j].nonlin else act\n",
    "                out.append(act)\n",
    "            return out[0] if len(out) == 1 else out\n",
    "        else:\n",
    "            # Standard layer behavior\n",
    "            out = [n(x) for n in self.neurons]\n",
    "            return out\n",
    "\n",
    "    def parameters(self):\n",
    "        # In a tied layer, the weights are shared, but the biases are not.\n",
    "        if hasattr(self, 'tied_to_layer'):\n",
    "            return [n.b for n in self.neurons]\n",
    "        else:\n",
    "            return [p for n in self.neurons for p in n.parameters()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts, tied_weights_from=None, **kwargs):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = []\n",
    "        if tied_weights_from is None:\n",
    "            # Standard MLP initialization\n",
    "            self.layers = [Layer(sz[i], sz[i+1], nonlin=i != len(nouts)-1, **kwargs) for i in range(len(nouts))]\n",
    "        else:\n",
    "            # Tied-weight MLP initialization\n",
    "            tied_layers = list(reversed(tied_weights_from))\n",
    "            for i in range(len(nouts)):\n",
    "                # Pass the encoder's layer directly to the decoder's layer.\n",
    "                # The decoder layer will use the encoder's weights.\n",
    "                self.layers.append(Layer(sz[i], sz[i+1], tied_to_layer=tied_layers[i], nonlin=i != len(nouts)-1, **kwargs))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "\n",
    "class AutoEncoder(Module):\n",
    "    def __init__(self, in_embeds, n_hidden_layers, compressed, act_func=None, tied=False):\n",
    "        n_hidden_layers = [math.ceil(in_embeds / i)\n",
    "                           for i in range(2, n_hidden_layers + 2)]\n",
    "        \n",
    "        \n",
    "        self.act_func = act_func\n",
    "        \n",
    "        # Create encoder\n",
    "        self.encoder = MLP(in_embeds, n_hidden_layers + [compressed])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Create decoder, passing encoder layers for tied weights\n",
    "        if tied:\n",
    "            self.decoder = MLP(compressed, list(reversed(n_hidden_layers)) + [in_embeds], tied_weights_from=self.encoder.layers, activate=act_func)\n",
    "        else:\n",
    "            self.decoder = MLP(compressed, list(reversed(n_hidden_layers)) + [in_embeds], activate=act_func)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        compressed = self.encoder(x)\n",
    "        out = self.decoder(compressed)\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.encoder.parameters() + self.decoder.parameters()\n",
    "\n",
    "    def layers(self):\n",
    "        return self.encoder.layers + self.decoder.layers\n",
    "\n",
    "    def pretty(self):\n",
    "        if self.act_func != None:\n",
    "            hey = str(self.act_func)\n",
    "            return hey.split()[1][6:]\n",
    "        else:\n",
    "            return \"no function\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"encoder has {self.summary()}, decoder has {self.summary()} activated with {self.pretty()}\"\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(Module):\n",
    "    \"\"\"\n",
    "    Simple Variational Autoencoder implementation.\n",
    "    The encoder outputs mean and log-variance for each latent dimension.\n",
    "    Uses reparameterization trick to sample from the latent distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_embeds, n_hidden_layers, latent_dim, act_func=None, tied=False):\n",
    "        n_hidden_layers = [math.ceil(in_embeds / i)\n",
    "                           for i in range(2, n_hidden_layers + 2)]\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.act_func = act_func\n",
    "        \n",
    "        # Encoder outputs 2 * latent_dim: mean and log-variance for each dimension\n",
    "        # Last layer outputs 2*latent_dim (no activation on this layer)\n",
    "        self.encoder = MLP(in_embeds, n_hidden_layers + [2 * latent_dim])\n",
    "        \n",
    "        # Decoder takes latent_dim as input\n",
    "        if tied:\n",
    "            # For tied weights, we'd need to handle the 2*latent_dim -> latent_dim transition\n",
    "            # For simplicity, we'll skip tied weights in VAE for now\n",
    "            self.decoder = MLP(latent_dim, list(reversed(n_hidden_layers)) + [in_embeds], activate=act_func)\n",
    "        else:\n",
    "            self.decoder = MLP(latent_dim, list(reversed(n_hidden_layers)) + [in_embeds], activate=act_func)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to mean and log-variance\"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        # Ensure encoded is a list\n",
    "        if not isinstance(encoded, list):\n",
    "            encoded = [encoded]\n",
    "        \n",
    "        # Split the output into mean and log_var\n",
    "        # encoded should be a list of 2*latent_dim values\n",
    "        if len(encoded) != 2 * self.latent_dim:\n",
    "            raise ValueError(f\"Encoder output dimension {len(encoded)} doesn't match expected 2*latent_dim={2*self.latent_dim}\")\n",
    "        \n",
    "        mu = encoded[:self.latent_dim]\n",
    "        log_var = encoded[self.latent_dim:]\n",
    "        \n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + sigma * epsilon\n",
    "        where epsilon ~ N(0,1) and sigma = exp(0.5 * log_var)\n",
    "        \n",
    "        Note: epsilon is sampled and treated as a constant during backprop\n",
    "        \"\"\"\n",
    "        # Sample epsilon from standard normal (treated as constant in backprop)\n",
    "        epsilon = [Value(random.gauss(0, 1)) for _ in range(len(mu))]\n",
    "        \n",
    "        # Compute sigma = exp(0.5 * log_var) more efficiently\n",
    "        # sigma = exp(0.5 * log_var) = sqrt(exp(log_var))\n",
    "        sigma = [(log_var_i * 0.5).exp() for log_var_i in log_var]\n",
    "        \n",
    "        # z = mu + sigma * epsilon\n",
    "        z = [mu_i + sigma_i * eps_i for mu_i, sigma_i, eps_i in zip(mu, sigma, epsilon)]\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent sample to reconstruction\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Forward pass: encode, sample, decode\"\"\"\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mu, log_var\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.encoder.parameters() + self.decoder.parameters()\n",
    "    \n",
    "    \n",
    "    def layers(self):\n",
    "        return self.encoder.layers + self.decoder.layers\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"VAE(encoder: {len(self.encoder.layers)} layers, decoder: {len(self.decoder.layers)} layers, latent_dim: {self.latent_dim})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers\"\"\"\n",
    "\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad = 0\n",
    "\n",
    "  \n",
    "    def step(self):\n",
    "        \"\"\"Take a step of gradient descent\"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "\n",
    "    def __init__(self, parameters, learning_rate = 0.01):\n",
    "        super().__init__(parameters)\n",
    "        self.count = 0\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def increment(self):\n",
    "        self.count += 1\n",
    "\n",
    "    def step(self):\n",
    "        self.increment()\n",
    "        # Calculate the base LR\n",
    "        new_lr = 1.0 - 0.9 * self.count / 100\n",
    "        \n",
    "        # Ensure the learning rate is never negative (or zero, to stop training)\n",
    "        self.learning_rate = max(0.00001, new_lr) \n",
    "        \n",
    "        \"\"\"Update model parameters in the opposite direction of their gradient\"\"\"\n",
    "\n",
    "        if self.learning_rate > 0: # Only update if the LR is positive\n",
    "            for p in self.parameters:\n",
    "                p.data -= self.learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Heavily inspired by https://github.com/joelgrus/joelnet/blob/master/joelnet/data.py\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "\n",
    "# Batch = NamedTuple(\"Batch\", [(\"inputs\", List[Vector]), (\"targets\", Vector)])\n",
    "\n",
    "class BatchIterator:\n",
    "    \"\"\"Iterates on data by batches\"\"\"\n",
    "\n",
    "    def __init__(self,inputs,targets,batch_size=32):\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __call__(self):\n",
    "        starts = list(range(0, len(self.inputs), self.batch_size))\n",
    "            \n",
    "        for start in starts:\n",
    "            end = start + self.batch_size\n",
    "            batch_inputs = self.inputs[start:end]\n",
    "            batch_targets = self.targets[start:end]\n",
    "            yield (batch_inputs, batch_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    total_loss = sum([(true - pred)**2 for true, pred in zip(y_true, y_pred)])\n",
    "    mean_loss = total_loss / len(y_true)\n",
    "    \n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def vae_loss(reconstruction, target, mu, log_var, beta=1.0):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder loss = Reconstruction Loss + beta * KL Divergence\n",
    "    \n",
    "    Args:\n",
    "        reconstruction: Value or List of Value objects (reconstructed output)\n",
    "        target: List of Value objects (original input)\n",
    "        mu: List of Value objects (mean of latent distribution)\n",
    "        log_var: List of Value objects (log variance of latent distribution)\n",
    "        beta: Weight for KL divergence term (default 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Total VAE loss as a Value object\n",
    "    \"\"\"\n",
    "    # Ensure reconstruction is a list\n",
    "    if not isinstance(reconstruction, list):\n",
    "        reconstruction = [reconstruction]\n",
    "    if not isinstance(target, list):\n",
    "        target = [target]\n",
    "    \n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = mean_squared_error(target, reconstruction)\n",
    "    \n",
    "    # KL divergence: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "    # This encourages the latent distribution to be close to N(0,1)\n",
    "    kl_terms = []\n",
    "    for mu_i, log_var_i in zip(mu, log_var):\n",
    "        # KL term for one dimension: -0.5 * (1 + log_var - mu^2 - exp(log_var))\n",
    "        kl_term = -0.5 * (Value(1.0) + log_var_i - mu_i**2 - log_var_i.exp())\n",
    "        kl_terms.append(kl_term)\n",
    "    \n",
    "    kl_loss = sum(kl_terms) / len(kl_terms) if kl_terms else Value(0.0)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = recon_loss + beta * kl_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # from pyfit.engine import Vector, Scalar\n",
    "# from pyfit.nn import Module\n",
    "# from pyfit.optim import Optimizer\n",
    "# from pyfit.data import BatchIterator\n",
    "# from pyfit.metrics import binary_accuracy\n",
    "\n",
    "# Used to record training history for metrics\n",
    "History = {}\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Encapsulates the model training loop\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, data_iterator, num_epochs=500, verbose=False):\n",
    "        \"\"\"Fits the model to the data\"\"\"\n",
    "\n",
    "        history = {\"loss\": []}\n",
    "        epoch_loss = 0\n",
    "        epoch_y_true = []\n",
    "        epoch_y_pred = []\n",
    "        for epoch in range(num_epochs):\n",
    "            # Reset the gradients of model parameters\n",
    "            self.optimizer.zero_grad()\n",
    "            # Reset epoch data\n",
    "            epoch_loss = 0\n",
    "            epoch_y_true = []\n",
    "            epoch_y_pred = []\n",
    "\n",
    "            for batch in data_iterator():\n",
    "                # Forward pass\n",
    "                outputs = list(map(self.model, batch[0]))\n",
    "                \n",
    "                batch_y_true = [Value(val) for sublist in batch[1] for val in sublist]\n",
    "                batch_y_pred = [val for sublist in outputs for val in sublist]\n",
    "                # Loss computation\n",
    "                # [item for sublist in outputs[0] for item in sublist]\n",
    "                batch_loss = self.loss(batch_y_true, batch_y_pred)\n",
    "                epoch_loss += batch_loss.data\n",
    "\n",
    "                # Store batch predictions and ground truth for computing epoch metrics\n",
    "                epoch_y_pred.extend(batch_y_pred)\n",
    "                epoch_y_true.extend(batch[1])\n",
    "\n",
    "                # Backprop and gradient descent\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Accuracy computation for epoch\n",
    "            \n",
    "            \n",
    "\n",
    "            # Record training history\n",
    "            history[\"loss\"].append(epoch_loss)\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                    f\"loss: {epoch_loss:.6f}, \"\n",
    "                )\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE Loss: 1.885692220844\n",
      "VAE Loss: 0.775253382145\n",
      "VAE Loss: 0.388295866018\n",
      "VAE Loss: 0.243936953579\n",
      "VAE Loss: 0.159898954791\n",
      "VAE Loss: 0.113929933732\n",
      "VAE Loss: 0.743840775263\n",
      "VAE Loss: 0.441804987907\n",
      "VAE Loss: 0.230553340726\n",
      "VAE Loss: 0.125320135904\n",
      "VAE Loss: 0.071456777775\n",
      "VAE Loss: 0.042591899699\n",
      "VAE Loss: 0.026575101850\n",
      "VAE Loss: 0.017439395863\n",
      "VAE Loss: 0.011621689478\n",
      "VAE Loss: 0.008075181871\n",
      "VAE Loss: 0.005686644723\n",
      "VAE Loss: 0.004081616450\n",
      "VAE Loss: 0.049005339617\n",
      "VAE Loss: 0.007548395786\n",
      "VAE Loss: 0.004662716209\n",
      "VAE Loss: 0.002952894746\n",
      "VAE Loss: 0.001952988214\n",
      "VAE Loss: 0.001342187352\n",
      "VAE Loss: 0.000926823349\n",
      "VAE Loss: 0.000671867040\n",
      "VAE Loss: 0.000486797917\n",
      "VAE Loss: 0.000359053988\n",
      "VAE Loss: 0.000267787015\n",
      "VAE Loss: 0.000201657980\n",
      "VAE Loss: 0.000152519990\n",
      "VAE Loss: 0.000116479037\n",
      "VAE Loss: 0.000089446124\n",
      "VAE Loss: 0.000069512354\n",
      "VAE Loss: 0.000053671708\n",
      "VAE Loss: 0.000042077098\n",
      "VAE Loss: 0.000032660843\n",
      "VAE Loss: 0.014389295911\n",
      "VAE Loss: 0.001375019987\n",
      "VAE Loss: 0.000910263630\n",
      "VAE Loss: 0.000653112667\n",
      "VAE Loss: 0.000451886090\n",
      "VAE Loss: 0.000506110214\n",
      "VAE Loss: 0.000385625332\n",
      "VAE Loss: 0.000255035100\n",
      "VAE Loss: 0.000150540349\n",
      "VAE Loss: 0.000110275337\n",
      "VAE Loss: 0.000083878742\n",
      "VAE Loss: 0.000066721590\n",
      "VAE Loss: 0.000044947046\n",
      "VAE Loss: 0.000044954374\n",
      "VAE Loss: 0.000032655635\n",
      "VAE Loss: 0.000032839422\n",
      "VAE Loss: 0.000037389219\n",
      "VAE Loss: 0.000025890110\n",
      "VAE Loss: 0.000045320445\n",
      "VAE Loss: 0.000022629383\n",
      "VAE Loss: 0.000018048739\n",
      "VAE Loss: 0.000014618228\n",
      "VAE Loss: 0.000012058502\n",
      "VAE Loss: 0.000006543551\n",
      "VAE Loss: 0.000008788164\n",
      "VAE Loss: 0.000007484130\n",
      "VAE Loss: 0.000048765703\n",
      "VAE Loss: 0.000008047601\n",
      "VAE Loss: 0.000006862508\n",
      "VAE Loss: 0.000007215154\n",
      "VAE Loss: 0.000003516682\n",
      "VAE Loss: 0.032671052769\n",
      "VAE Loss: 0.000487131242\n",
      "VAE Loss: 0.000407175719\n",
      "VAE Loss: 0.000342815565\n",
      "VAE Loss: 0.000332406976\n",
      "VAE Loss: 0.000247359758\n",
      "VAE Loss: 0.000213723929\n",
      "VAE Loss: 0.000183460654\n",
      "VAE Loss: 0.000247944433\n",
      "VAE Loss: 0.000168149692\n",
      "VAE Loss: 0.000121870918\n",
      "VAE Loss: 0.000107975110\n",
      "VAE Loss: 0.000194045757\n",
      "VAE Loss: 0.000084648834\n",
      "VAE Loss: 0.000076372840\n",
      "VAE Loss: 0.000069510049\n",
      "VAE Loss: 0.000061987458\n",
      "VAE Loss: 0.000057106822\n",
      "VAE Loss: 0.000089324396\n",
      "VAE Loss: 0.000048448490\n",
      "VAE Loss: 0.000042821574\n",
      "VAE Loss: 0.000041466882\n",
      "VAE Loss: 0.000038512811\n",
      "VAE Loss: 0.000036260947\n",
      "VAE Loss: 0.000033501189\n",
      "VAE Loss: 0.000031475281\n",
      "VAE Loss: 0.000029036591\n",
      "VAE Loss: 0.000093178208\n",
      "VAE Loss: 0.000027476045\n",
      "VAE Loss: 0.000025701460\n",
      "VAE Loss: 0.000024697230\n",
      "VAE Loss: 0.000023173988\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage of VariationalAutoEncoder\n",
    "\n",
    "# Create a simple VAE\n",
    "vae = VariationalAutoEncoder(\n",
    "    in_embeds=10,        # Input dimension\n",
    "    n_hidden_layers=2,   # Number of hidden layers\n",
    "    latent_dim=3,        # Latent space dimension\n",
    "    act_func=None,       # Activation function (None uses default leaky_relu)\n",
    "    tied=False           # Whether to use tied weights\n",
    ")\n",
    "\n",
    "optimizer = SGD(vae.parameters(), learning_rate=0.01)\n",
    "\n",
    "# Example input (list of Values)\n",
    "x = [Value(random.uniform(0, 1)) for _ in range(10)]\n",
    "target = [Value(val.data) for val in x] \n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass returns: (reconstruction, mu, log_var)\n",
    "    reconstruction, mu, log_var = vae(x)\n",
    "\n",
    "    # Ensure reconstruction is a list for loss computation\n",
    "    if not isinstance(reconstruction, list):\n",
    "        reconstruction = [reconstruction]\n",
    "\n",
    "    # Compute VAE loss\n",
    "     # Target is the input itself (autoencoder)\n",
    "    loss = vae_loss(reconstruction, target, mu, log_var, beta=1.0)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"VAE Loss: {loss.data:.12f}\")\n",
    "\n",
    "\n",
    "# Note: To use VAE with the Trainer class, you'll need to create a wrapper\n",
    "# or modify the training loop to handle the (reconstruction, mu, log_var) output\n",
    "# and use vae_loss instead of the standard MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to construct a training loop, I need a loss function\n",
    "# forward pass\n",
    "# backward pass\n",
    "# batches if im feeling cheeky\n",
    "# update (Stochastic gradient descent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder Loss: 0.154100578898\n",
      "AutoEncoder Loss: 0.121172180946\n",
      "AutoEncoder Loss: 0.108503458121\n",
      "AutoEncoder Loss: 0.098586663214\n",
      "AutoEncoder Loss: 0.092146694514\n",
      "AutoEncoder Loss: 0.088537622307\n",
      "AutoEncoder Loss: 0.084915297342\n",
      "AutoEncoder Loss: 0.083621397654\n",
      "AutoEncoder Loss: 0.080006730797\n",
      "AutoEncoder Loss: 0.077217073003\n",
      "AutoEncoder Loss: 0.076242318730\n",
      "AutoEncoder Loss: 0.080102734315\n",
      "AutoEncoder Loss: 0.072887421172\n",
      "AutoEncoder Loss: 0.069148290593\n",
      "AutoEncoder Loss: 0.066283086624\n",
      "AutoEncoder Loss: 0.063735622323\n",
      "AutoEncoder Loss: 0.062401797382\n",
      "AutoEncoder Loss: 0.061932117944\n",
      "AutoEncoder Loss: 0.058127429064\n",
      "AutoEncoder Loss: 0.055364448549\n",
      "AutoEncoder Loss: 0.053791074059\n",
      "AutoEncoder Loss: 0.053383041223\n",
      "AutoEncoder Loss: 0.049881004690\n",
      "AutoEncoder Loss: 0.047505002879\n",
      "AutoEncoder Loss: 0.047413136493\n",
      "AutoEncoder Loss: 0.044312554137\n",
      "AutoEncoder Loss: 0.042458256759\n",
      "AutoEncoder Loss: 0.042094161924\n",
      "AutoEncoder Loss: 0.039283335822\n",
      "AutoEncoder Loss: 0.037694878208\n",
      "AutoEncoder Loss: 0.037233357732\n",
      "AutoEncoder Loss: 0.034757532524\n",
      "AutoEncoder Loss: 0.033368489854\n",
      "AutoEncoder Loss: 0.032885359788\n",
      "AutoEncoder Loss: 0.030756444927\n",
      "AutoEncoder Loss: 0.029500224505\n",
      "AutoEncoder Loss: 0.029057774697\n",
      "AutoEncoder Loss: 0.027267815298\n",
      "AutoEncoder Loss: 0.026101904998\n",
      "AutoEncoder Loss: 0.025738216497\n",
      "AutoEncoder Loss: 0.024259535865\n",
      "AutoEncoder Loss: 0.023176757857\n",
      "AutoEncoder Loss: 0.022726372876\n",
      "AutoEncoder Loss: 0.022331907750\n",
      "AutoEncoder Loss: 0.020999846441\n",
      "AutoEncoder Loss: 0.020089199088\n",
      "AutoEncoder Loss: 0.019403222374\n",
      "AutoEncoder Loss: 0.019023704692\n",
      "AutoEncoder Loss: 0.018204613826\n",
      "AutoEncoder Loss: 0.017531496977\n",
      "AutoEncoder Loss: 0.017001300701\n",
      "AutoEncoder Loss: 0.016733340247\n",
      "AutoEncoder Loss: 0.016058689214\n",
      "AutoEncoder Loss: 0.015508562072\n",
      "AutoEncoder Loss: 0.015045090570\n",
      "AutoEncoder Loss: 0.014607397442\n",
      "AutoEncoder Loss: 0.014195214571\n",
      "AutoEncoder Loss: 0.013840831794\n",
      "AutoEncoder Loss: 0.013591098267\n",
      "AutoEncoder Loss: 0.013169446587\n",
      "AutoEncoder Loss: 0.012804855370\n",
      "AutoEncoder Loss: 0.012484256909\n",
      "AutoEncoder Loss: 0.012206260612\n",
      "AutoEncoder Loss: 0.012005632599\n",
      "AutoEncoder Loss: 0.011686237757\n",
      "AutoEncoder Loss: 0.011405235573\n",
      "AutoEncoder Loss: 0.011154501581\n",
      "AutoEncoder Loss: 0.010920160320\n",
      "AutoEncoder Loss: 0.010718875977\n",
      "AutoEncoder Loss: 0.010546291467\n",
      "AutoEncoder Loss: 0.010321388305\n",
      "AutoEncoder Loss: 0.010119075250\n",
      "AutoEncoder Loss: 0.009933557530\n",
      "AutoEncoder Loss: 0.009757753457\n",
      "AutoEncoder Loss: 0.009598781790\n",
      "AutoEncoder Loss: 0.009469105156\n",
      "AutoEncoder Loss: 0.009307725023\n",
      "AutoEncoder Loss: 0.009160038179\n",
      "AutoEncoder Loss: 0.009020584062\n",
      "AutoEncoder Loss: 0.008888594316\n",
      "AutoEncoder Loss: 0.008766234232\n",
      "AutoEncoder Loss: 0.008669402247\n",
      "AutoEncoder Loss: 0.008552761598\n",
      "AutoEncoder Loss: 0.008442984484\n",
      "AutoEncoder Loss: 0.008339459030\n",
      "AutoEncoder Loss: 0.008241715737\n",
      "AutoEncoder Loss: 0.008151676865\n",
      "AutoEncoder Loss: 0.008079196384\n",
      "AutoEncoder Loss: 0.007994594532\n",
      "AutoEncoder Loss: 0.007915181917\n",
      "AutoEncoder Loss: 0.007840631694\n",
      "AutoEncoder Loss: 0.007770679885\n",
      "AutoEncoder Loss: 0.007708583007\n",
      "AutoEncoder Loss: 0.007655097835\n",
      "AutoEncoder Loss: 0.007596647745\n",
      "AutoEncoder Loss: 0.007542335257\n",
      "AutoEncoder Loss: 0.007491991097\n",
      "AutoEncoder Loss: 0.007445473022\n",
      "AutoEncoder Loss: 0.007405892457\n",
      "AutoEncoder Loss: 0.007370420750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage of VariationalAutoEncoder\n",
    "\n",
    "# Create a simple VAE\n",
    "auto = AutoEncoder(\n",
    "    in_embeds=10,        # Input dimension\n",
    "    n_hidden_layers=2,   # Number of hidden layers\n",
    "    compressed=3,        # Latent space dimension\n",
    "    act_func=Value.sigmoid,       # Activation function (None uses default leaky_relu)\n",
    "    tied=True           # Whether to use tied weights\n",
    ")\n",
    "\n",
    "optimizer = SGD(auto.parameters())\n",
    "\n",
    "# Example input (list of Values)\n",
    "x = [Value(random.uniform(0, 1)) for _ in range(10)]\n",
    "target = [Value(val.data) for val in x]  # Target is the input itself (autoencoder)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass returns: (reconstruction, mu, log_var)\n",
    "    reconstruction = auto(x)\n",
    "\n",
    "    # Ensure reconstruction is a list for loss computation\n",
    "\n",
    "    # Compute VAE loss\n",
    "    \n",
    "    loss = mean_squared_error(reconstruction, target)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"AutoEncoder Loss: {loss.data:.12f}\")\n",
    "\n",
    "\n",
    "# Note: To use VAE with the Trainer class, you'll need to create a wrapper\n",
    "# or modify the training loop to handle the (reconstruction, mu, log_var) output\n",
    "# and use vae_loss instead of the standard MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "t = MinMaxScaler()\n",
    "t.fit(X)\n",
    "X_train = t.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "Iteration : 0, AutoEncoder Loss: 0.243101277896\n",
      "(64,)\n",
      "Iteration : 1, AutoEncoder Loss: 0.209554474847\n",
      "(64,)\n",
      "Iteration : 2, AutoEncoder Loss: 0.195153729115\n",
      "(64,)\n",
      "Iteration : 3, AutoEncoder Loss: 0.191140189781\n",
      "(64,)\n",
      "Iteration : 4, AutoEncoder Loss: 0.188718272333\n",
      "(64,)\n",
      "Iteration : 5, AutoEncoder Loss: 0.193006403770\n",
      "(64,)\n",
      "Iteration : 6, AutoEncoder Loss: 0.178557645838\n",
      "(64,)\n",
      "Iteration : 7, AutoEncoder Loss: 0.166324824406\n",
      "(64,)\n",
      "Iteration : 8, AutoEncoder Loss: 0.157395371741\n",
      "(64,)\n",
      "Iteration : 9, AutoEncoder Loss: 0.155887547776\n",
      "(64,)\n",
      "Iteration : 10, AutoEncoder Loss: 0.155665978985\n",
      "(64,)\n",
      "Iteration : 11, AutoEncoder Loss: 0.155607102259\n",
      "(64,)\n",
      "Iteration : 12, AutoEncoder Loss: 0.156550397051\n",
      "(64,)\n",
      "Iteration : 13, AutoEncoder Loss: 0.155992145061\n",
      "(64,)\n",
      "Iteration : 14, AutoEncoder Loss: 0.155478549465\n",
      "(64,)\n",
      "Iteration : 15, AutoEncoder Loss: 0.153316993607\n",
      "(64,)\n",
      "Iteration : 16, AutoEncoder Loss: 0.151037202368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass returns: (reconstruction, mu, log_var)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# reconstruction = [list(map(Value, xrow)) for xrow in X_train]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m reconstruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(Value, X_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 25\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mauto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(scores)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure reconstruction is a list for loss computation\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Compute VAE loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 151\u001b[0m, in \u001b[0;36mAutoEncoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    150\u001b[0m     compressed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m--> 151\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[2], line 119\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 119\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# For each output neuron (j), sum the weighted inputs.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# The weight connecting input `i` to output `j` is the same as the weight connecting\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# input `j` of the encoder layer to output `i`.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_to_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_to_layer\u001b[38;5;241m.\u001b[39mneurons[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mw[j] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Apply activation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mactivate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mnonlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons)):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# The j-th neuron of this layer uses the j-th weight of every neuron in the tied layer.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# For each output neuron (j), sum the weighted inputs.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# The weight connecting input `i` to output `j` is the same as the weight connecting\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# input `j` of the encoder layer to output `i`.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_to_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_to_layer\u001b[38;5;241m.\u001b[39mneurons[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mw[j] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Apply activation\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mactivate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons[j]\u001b[38;5;241m.\u001b[39mnonlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mValue.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     23\u001b[0m     out\u001b[38;5;241m.\u001b[39m_backward \u001b[38;5;241m=\u001b[39m _backward\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m     28\u001b[0m     other \u001b[38;5;241m=\u001b[39m other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Value) \u001b[38;5;28;01melse\u001b[39;00m Value(other)\n\u001b[1;32m     29\u001b[0m     out \u001b[38;5;241m=\u001b[39m Value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m other\u001b[38;5;241m.\u001b[39mdata, (\u001b[38;5;28mself\u001b[39m, other), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage of VariationalAutoEncoder\n",
    "\n",
    "# Create a simple VAE\n",
    "auto = AutoEncoder(\n",
    "    in_embeds=64,        # Input dimension\n",
    "    n_hidden_layers=1,   # Number of hidden layers\n",
    "    compressed=16,        # Latent space dimension\n",
    "    act_func=Value.sigmoid,       # Activation function (None uses default leaky_relu)\n",
    "    tied=True           # Whether to use tied weights\n",
    ")\n",
    "\n",
    "optimizer = SGD(auto.parameters())\n",
    "\n",
    "# Example input (list of Values)\n",
    "target = X_train[0]\n",
    "target = list(map(Value, target)) # Target is the input itself (autoencoder)\n",
    "for i in range(150):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass returns: (reconstruction, mu, log_var)\n",
    "    \n",
    "    # reconstruction = [list(map(Value, xrow)) for xrow in X_train]\n",
    "    \n",
    "    reconstruction = map(Value, X_train[0])\n",
    "    \n",
    "    scores = auto(reconstruction)\n",
    "    # Ensure reconstruction is a list for loss computation\n",
    "\n",
    "    # Compute VAE loss\n",
    "    \n",
    "    loss = mean_squared_error(target, scores)\n",
    "\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in auto.parameters()))\n",
    "    total_loss = loss + reg_loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Iteration : {i}, AutoEncoder Loss: {total_loss.data:.12f}\")\n",
    "    print(auto.parameters()[:30])\n",
    "\n",
    "\n",
    "# Note: To use VAE with the Trainer class, you'll need to create a wrapper\n",
    "# or modify the training loop to handle the (reconstruction, mu, log_var) output\n",
    "# and use vae_loss instead of the standard MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
